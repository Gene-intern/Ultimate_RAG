# -*- coding: utf-8 -*-
import os, re, json, time, pickle
from typing import List, Dict, Any, Tuple, Optional, Iterable, Set, Callable
from datetime import datetime, timedelta

import gradio as gr
import numpy as np
import torch
import requests
from functools import lru_cache
from concurrent.futures import ThreadPoolExecutor, Future, as_completed
import threading
try:
    from rapidfuzz import fuzz  # å¯ç”¨å‰‡ä½¿ç”¨ï¼›ä¸å¯ç”¨æ™‚ä»¥ None è¡¨ç¤º
except Exception:
    fuzz = None
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from dateutil import tz
from dateutil.parser import parse as dtparse
from dateutil.relativedelta import relativedelta

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_groq import ChatGroq
from langchain_core.prompts import PromptTemplate

# ===== å…±ç”¨ï¼šè«‹æ±‚å™¨ + è¿·ä½ å¿«å– =====
_REQ_TIMEOUT = 3
def _req_get(
    url: str,
    params: dict = None,
    headers: dict = None,
    timeout: float = _REQ_TIMEOUT,
    retries: int = 1,
    backoff: float = 0.6,
):
    base_headers = {
        "User-Agent": "Mozilla/5.0",
        "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.8",
        "Connection": "keep-alive",
    }
    if headers:
        base_headers.update(headers)
    for i in range(retries + 1):
        try:
            r = requests.get(url, params=params, headers=base_headers, timeout=timeout)
            if r.status_code == 200 and r.text:
                return r
        except Exception:
            pass
        time.sleep(backoff * (2 ** i))
    return None

# è¶…è¼•é‡è¨˜æ†¶é«”å¿«å–ï¼ˆTTL ç§’ï¼‰
_CACHE: Dict[str, Dict[str, Any]] = {}
_RESOLVE_CACHE_TTL = 1800  # seconds
_RECENT_RESOLVE_TTL = 1800  # seconds

_RECENT_RESOLVED: Dict[str, Tuple[str, float]] = {}
_BG_EXECUTOR_LOCK = threading.Lock()
_BG_EXECUTOR: Optional[ThreadPoolExecutor] = None
_TW_LOAD_LOCK = threading.Lock()
_TW_LOAD_FUTURE: Optional[Future] = None

def _cache_get(key: str) -> Optional[Any]:
    it = _CACHE.get(key)
    if not it:
        return None
    if it["exp"] < time.time():
        _CACHE.pop(key, None)
        return None
    return it["val"]

def _cache_set(key: str, val: Any, ttl: int = 300) -> None:
    _CACHE[key] = {"val": val, "exp": time.time() + ttl}


def _get_bg_executor() -> ThreadPoolExecutor:
    global _BG_EXECUTOR
    with _BG_EXECUTOR_LOCK:
        if _BG_EXECUTOR is None:
            _BG_EXECUTOR = ThreadPoolExecutor(max_workers=3, thread_name_prefix="rag-bg")
        return _BG_EXECUTOR


def _recent_token_key(token: Optional[str]) -> Optional[str]:
    if not token:
        return None
    norm = _normalize_name(token)
    norm = norm.strip() if isinstance(norm, str) else ""
    if norm:
        return norm.lower()
    t = (token or "").strip().lower()
    return t or None


def _remember_resolved_symbol(token: Optional[str], symbol: Optional[str]) -> None:
    if not token or not symbol:
        return
    key = _recent_token_key(token)
    if not key:
        return
    _RECENT_RESOLVED[key] = (symbol, time.time())
    # ä¹Ÿè¨˜ä½ ticker è‡ªèº«èˆ‡å»æ‰å°æ•¸é»çš„å½¢å¼
    sym_key = _recent_token_key(symbol)
    if sym_key:
        _RECENT_RESOLVED[sym_key] = (symbol, time.time())
    digits = re.sub(r"\D", "", symbol)
    if digits:
        digits_key = digits.lower()
        _RECENT_RESOLVED[digits_key] = (symbol, time.time())


def _lookup_recent_symbol(token: Optional[str]) -> Optional[str]:
    key = _recent_token_key(token)
    if not key:
        return None
    item = _RECENT_RESOLVED.get(key)
    if not item:
        return None
    sym, ts = item
    if (time.time() - ts) > _RECENT_RESOLVE_TTL:
        _RECENT_RESOLVED.pop(key, None)
        return None
    return sym


def _warm_tw_lists_async(force: bool = False) -> None:
    global _TW_LOAD_FUTURE
    fresh = _is_tw_cache_fresh()
    if fresh and not force:
        return
    with _TW_LOAD_LOCK:
        if _TW_LOAD_FUTURE and not _TW_LOAD_FUTURE.done():
            return
        executor = _get_bg_executor()
        _TW_LOAD_FUTURE = executor.submit(_load_tw_lists, True)

# =========================
# FAISS å…¼å®¹è¼‰å…¥
# =========================
def load_faiss_compat(folder_path: str, embeddings, index_name: str = "index"):
    """
    å…¼å®¹ä¸åŒå¹´ä»£çš„ LangChain FAISS å„²å­˜æ ¼å¼ã€‚
    """
    import faiss

    from langchain_community.vectorstores.faiss import FAISS as _FAISS
    try:
        return _FAISS.load_local(
            folder_path, embeddings, index_name=index_name,
            allow_dangerous_deserialization=True
        )
    except Exception:
        pass

    faiss_path = os.path.join(folder_path, f"{index_name}.faiss")
    pkl_path   = os.path.join(folder_path, f"{index_name}.pkl")
    if not (os.path.exists(faiss_path) and os.path.exists(pkl_path)):
        raise FileNotFoundError(f"Missing {faiss_path} or {pkl_path}")

    faiss_index = faiss.read_index(faiss_path)
    with open(pkl_path, "rb") as f:
        obj = pickle.load(f)

    InMemoryDocstore = None
    for modpath, clsname in [
        ("langchain_community.docstore.in_memory", "InMemoryDocstore"),
        ("langchain_community.docstores", "InMemoryDocstore"),
    ]:
        try:
            mod = __import__(modpath, fromlist=[clsname])
            InMemoryDocstore = getattr(mod, clsname, None)
            if InMemoryDocstore: break
        except Exception:
            continue
    if InMemoryDocstore is None:
        try:
            mod = __import__("langchain.docstore.in_memory", fromlist=["InMemoryDocstore"])
            InMemoryDocstore = getattr(mod, "InMemoryDocstore")
        except Exception:
            pass

    try:
        from langchain_core.documents import Document
    except Exception:
        from langchain.schema import Document  # å…¼å®¹èˆŠç‰ˆ

    def is_doclike(x):
        if hasattr(x, "page_content") and hasattr(x, "metadata"): return True
        if isinstance(x, dict) and ("page_content" in x or "metadata" in x): return True
        return False

    def to_document(x):
        if hasattr(x, "page_content") and hasattr(x, "metadata"):
            return x
        if isinstance(x, dict):
            if "page_content" in x and "metadata" not in x:
                meta = {k: v for k, v in x.items() if k != "page_content"}
                return Document(page_content=x.get("page_content", ""), metadata=meta)
            if "metadata" in x:
                return Document(page_content=x.get("page_content", ""), metadata=x.get("metadata") or {})
            return Document(page_content="", metadata=x)
        return Document(page_content=str(x), metadata={})

    def coerce_docstore(x):
        if hasattr(x, "search") or hasattr(x, "_dict"): return x
        if isinstance(x, dict) and x:
            conv = {k: to_document(v) for k, v in x.items()}
            return InMemoryDocstore(conv) if InMemoryDocstore else conv
        return None

    def coerce_index_map(x):
        if isinstance(x, list):
            return {i: v for i, v in enumerate(x)}
        if isinstance(x, dict):
            out = {}
            for k, v in x.items():
                try:
                    ki = int(k) if not isinstance(k, int) else k
                    out[ki] = v
                except Exception:
                    pass
            if out: return out
            rev = {}
            for k, v in x.items():
                try:
                    vi = int(v) if not isinstance(v, int) else v
                    rev[vi] = k
                except Exception:
                    continue
            if rev: return rev
        return None

    docstore = None
    index_to_docstore_id = None

    if isinstance(obj, dict):
        for k in ["docstore", "_docstore"]:
            if k in obj and docstore is None:
                ds = coerce_docstore(obj[k])
                if ds is not None: docstore = ds
        for k in ["index_to_docstore_id", "_index_to_docstore_id", "index_map"]:
            if k in obj and index_to_docstore_id is None:
                m = coerce_index_map(obj[k])
                if m is not None: index_to_docstore_id = m

    elif isinstance(obj, tuple) or (isinstance(obj, list) and len(obj) <= 3):
        items = list(obj)
        if len(items) == 3:
            items = items[1:]
        if len(items) == 2:
            a, b = items
            a_ds = coerce_docstore(a); b_ds = coerce_docstore(b)
            a_map = coerce_index_map(a); b_map = coerce_index_map(b)
            if a_ds is not None and b_map is not None:
                docstore, index_to_docstore_id = a_ds, b_map
            elif b_ds is not None and a_map is not None:
                docstore, index_to_docstore_id = b_ds, a_map

    elif isinstance(obj, list) and len(obj) > 3:
        docs, ids = [], []
        for i, e in enumerate(obj):
            if isinstance(e, tuple) and len(e) == 2 and is_doclike(e[1]):
                did, dd = e[0], e[1]
                ids.append(str(did))
                docs.append(to_document(dd))
            elif is_doclike(e):
                ids.append(str(i))
                docs.append(to_document(e))
            else:
                ids.append(str(i))
                docs.append(to_document(e))

        if InMemoryDocstore is None:
            raise RuntimeError("InMemoryDocstore unavailable in this environment.")
        doc_map = {did: doc for did, doc in zip(ids, docs)}
        n_vec = faiss_index.ntotal
        n_doc = len(doc_map)
        n = min(n_vec, n_doc)
        ordered_ids = ids[:n]
        docstore = InMemoryDocstore({did: doc_map[did] for did in ordered_ids})
        index_to_docstore_id = {i: did for i, did in enumerate(ordered_ids)}

    else:
        raise RuntimeError(f"Unsupported pickle type: {type(obj)}")

    from langchain_community.vectorstores.faiss import FAISS as _FAISS
    try:
        return _FAISS(embeddings, faiss_index, docstore, index_to_docstore_id)
    except TypeError:
        return _FAISS(getattr(embeddings, "embed_query", embeddings), faiss_index, docstore, index_to_docstore_id)
# =========================
# åŸºæœ¬è¨­å®š
# =========================
load_dotenv()
GROQ_API_KEY = os.getenv("GROQ_API_KEY", "")
os.environ["GROQ_API_KEY"] = GROQ_API_KEY

# å„ªå…ˆä½¿ç”¨ CUDAï¼Œå…¶æ¬¡ä½¿ç”¨ Apple Silicon çš„ MPSï¼Œæœ€å¾Œé€€å› CPU
try:
    _HAS_MPS = bool(getattr(torch.backends, "mps", None)) and torch.backends.mps.is_available()
except Exception:
    _HAS_MPS = False
DEVICE = "cuda" if torch.cuda.is_available() else ("mps" if _HAS_MPS else "cpu")
STORE_ZH = "indices/store_zh"
STORE_EN = "indices/store_en"
TZ_TW = tz.gettz("Asia/Taipei")
TW_TICKER_RE = re.compile(r"^\d{4}\.(TW|TWO)$")

SPECIAL_OVERVIEW_LABEL = "ğŸ—“ï¸ ç”¢å‡ºè©²æœˆæ•´é«”æ¦‚è¦½ï¼ˆé»æˆ‘ç”Ÿæˆï¼‰"

# ==== BGE reranker ====
try:
    from FlagEmbedding import FlagReranker
    _HAS_BGE_RERANK = True
except Exception:
    FlagReranker = None
    _HAS_BGE_RERANK = False

BGE_RERANK_MODEL = os.getenv("BGE_RERANK_MODEL", "BAAI/bge-reranker-large")
bge_reranker = None
if _HAS_BGE_RERANK:
    try:
        # ä¾è£ç½®è¨­å®šï¼šCUDA ç”¨ FP16ï¼›MPS/CPU ç”¨ FP32ï¼Œä¸¦å˜—è©¦é¡¯å¼æŒ‡å®š device
        rerank_kwargs = {"use_fp16": (DEVICE == "cuda")}
        try:
            # æ–°ç‰ˆ FlagReranker æ”¯æ´ device åƒæ•¸
            rerank_kwargs["device"] = DEVICE
            bge_reranker = FlagReranker(BGE_RERANK_MODEL, **rerank_kwargs)
        except TypeError:
            # èˆŠç‰ˆä¸æ”¯æ´ device åƒæ•¸ï¼Œé€€å›ä¸å¸¶ device
            rerank_kwargs.pop("device", None)
            bge_reranker = FlagReranker(BGE_RERANK_MODEL, **rerank_kwargs)
    except Exception as e:
        print(f"[WARN] init bge reranker failed (device={DEVICE}): {e}")
        _HAS_BGE_RERANK = False

# ===== LLM è«‹æ±‚å¤§å°ä¿è­·ï¼ˆé¿å… 413 / éé•· promptï¼‰=====
# å­—å…ƒç²—ä¼°ï¼šè‹±æ–‡ç´„ 4 chars/tokenï¼Œä¸­æ–‡ç´„ 1â€“2 chars/tokenï¼Œé€™è£¡ä¿å®ˆæŠ“ä¸Šé™
LLM_CTX_TOTAL_CHAR_SOFT = 7000   # å–®æ¬¡å›ç­” context ç¸½é•·åº¦ä¸Šé™ï¼ˆè»Ÿæ€§ï¼‰
LLM_CTX_PER_DOC_CHAR    = 1000   # æ¯æ®µæœ€å¤šä¿ç•™å­—å…ƒ
LLM_CTX_MAX_DOCS        = 8      # æœ€å¤šå¸¶å…¥çš„æ®µè½æ•¸

YAHOO_FIRST = True  # True=Yahoo-first + åå–®å†·å‚™æ´
SYMBOL_MAP_PATH = os.getenv("SYMBOL_MAP_PATH", "cache/symbol_map.json")
SYMBOL_MAP_TTL_DAYS = int(os.getenv("SYMBOL_MAP_TTL_DAYS", "1"))

_SYM_KV = {"ts": 0, "map": {}}  # å…§å­˜é¡åƒ

def _sym_norm_key(s: str) -> str:
    # å°ä¸­æ–‡åç”¨ä½ ç¾æˆçš„ _normalize_nameï¼Œå°å…¶å®ƒç”¨ lower+strip
    if re.search(r"[\u4e00-\u9fff]", s or ""):
        return _normalize_name(s or "")
    return (s or "").strip().lower()

def _symcache_load():
    try:
        if os.path.exists(SYMBOL_MAP_PATH):
            with open(SYMBOL_MAP_PATH, "r", encoding="utf-8") as f:
                obj = json.load(f)
                if isinstance(obj, dict) and "map" in obj:
                    _SYM_KV.update(obj)
    except Exception:
        pass

def _symcache_flush():
    try:
        os.makedirs(os.path.dirname(SYMBOL_MAP_PATH), exist_ok=True)
        with open(SYMBOL_MAP_PATH, "w", encoding="utf-8") as f:
            json.dump(_SYM_KV, f, ensure_ascii=False)
    except Exception:
        pass

def _symcache_get_record(name: str) -> Optional[Dict[str, Any]]:
    if not name:
        return None
    if not _SYM_KV["map"]:
        _symcache_load()
    key = _sym_norm_key(name)
    rec = _SYM_KV["map"].get(key)
    if not rec:
        return None
    try:
        ts = float(rec.get("ts", 0))
        if time.time() - ts > SYMBOL_MAP_TTL_DAYS * 86400:
            _SYM_KV["map"].pop(key, None)
            _symcache_flush()
            return None
    except Exception:
        return None
    return rec


def _symcache_get(name: str) -> Optional[str]:
    rec = _symcache_get_record(name)
    if not rec:
        return None
    return rec.get("symbol") or None

def _symcache_put(name: str, symbol: str) -> None:
    if not (name and symbol): return
    if not _SYM_KV["map"]:
        _symcache_load()
    key = _sym_norm_key(name)
    _SYM_KV["map"][key] = {"symbol": symbol, "ts": time.time()}
    _symcache_flush()
# =========================
# Embeddings & Vectorstores
# =========================
embed_zh = HuggingFaceEmbeddings(
    model_name="BAAI/bge-large-zh-v1.5",
    model_kwargs={"device": DEVICE},
    encode_kwargs={"normalize_embeddings": True},
)
embed_en = HuggingFaceEmbeddings(
    model_name="BAAI/bge-large-en-v1.5",
    model_kwargs={"device": DEVICE},
    encode_kwargs={"normalize_embeddings": True},
)

vectorstore_zh = load_faiss_compat(STORE_ZH, embed_zh, index_name="index")
vectorstore_en = load_faiss_compat(STORE_EN, embed_en, index_name="index")

def _set_thread_env_if_unset(n_threads: int | None = None):
    n = n_threads or (os.cpu_count() or 4)
    # åªåœ¨æœªè¨­å®šæ™‚è³¦å€¼ï¼Œé¿å…è¦†å¯«ä½ å·²ç¶“åœ¨ shell è¨­çš„åƒæ•¸
    os.environ.setdefault("OMP_NUM_THREADS", str(n))
    os.environ.setdefault("VECLIB_MAXIMUM_THREADS", str(n))  # macOS Accelerate/vecLib

_set_thread_env_if_unset()

# ---- ä¹‹å¾Œå† import faiss ----
try:
    import faiss
    # æŸäº› macOS/arm64 çš„ wheel æ²’æœ‰é€™äº› APIï¼›æ‰€ä»¥è¦å…ˆåˆ¤æ–·
    if hasattr(faiss, "omp_set_num_threads"):
        faiss.omp_set_num_threads(int(os.environ.get("OMP_NUM_THREADS", os.cpu_count() or 4)))
    if hasattr(faiss, "omp_get_max_threads"):
        print(f"[FAISS] max threads = {faiss.omp_get_max_threads()}")
    else:
        print("[FAISS] OpenMP control API not exposed; using env vars only.")
except Exception as e:
    print(f"[WARN] faiss thread setup failed: {e}")
# =========================
# LLMs
# =========================
# === ä½¿ç”¨è€…å¯é¸çš„ã€Œç”Ÿæˆæ¨¡å‹ã€ç™½åå–®ï¼ˆåªå½±éŸ¿æœ€çµ‚å›ç­”ï¼‰===
GEN_MODEL_WHITELIST = [
    "llama-3.1-8b-instant",
    "llama-3.3-70b-versatile",
    #"openai/gpt-oss-120b",
    "openai/gpt-oss-20b",
]
GEN_MODEL_DEFAULT = "llama-3.3-70b-versatile"

# === å…§éƒ¨å›ºå®šæ¨¡å‹ï¼ˆä¸å—å‰ç«¯æ§åˆ¶ï¼‰===
CLS_MODEL    = "llama-3.1-8b-instant"  # åˆ†é¡ / æŠ½å–
PARA_MODEL   = "llama-3.1-8b-instant"  # å¤šæŸ¥è©¢æ”¹å¯«
RERANK_MODEL = "llama-3.1-8b-instant"  # LLM-based rerankï¼ˆè‹¥ä½ æœ‰ BGEï¼Œå°±ç•¶å‚™æ´ï¼‰

# å…¨åŸŸå¥æŸ„ï¼ˆå…ˆçµ¦ Noneï¼Œç¨å¾Œåˆå§‹åŒ–ï¼‰
llm = None             # ç”Ÿæˆç”¨ï¼ˆå‰ç«¯å¯é¸ï¼‰
classifier_llm = None  # å›ºå®š
paraphrase_llm = None  # å›ºå®š
rerank_llm = None      # å›ºå®š

def init_internal_llms():
    """å•Ÿå‹•æ™‚å‘¼å«ä¸€æ¬¡ï¼šæŠŠå…§éƒ¨ä¸‰ä½å›ºå®šåˆ°è¼•æ¨¡å‹ã€‚"""
    global classifier_llm, paraphrase_llm, rerank_llm
    classifier_llm = ChatGroq(temperature=0, model=CLS_MODEL)
    paraphrase_llm = ChatGroq(temperature=0, model=PARA_MODEL)
    rerank_llm     = ChatGroq(temperature=0, model=RERANK_MODEL)

def set_gen_llm(user_choice: str) -> str:
    """
    æ¯æ¬¡å›ç­”å‰å‘¼å«ï¼šåªåˆ‡æ›ã€ç”Ÿæˆç”¨ã€æ¨¡å‹ã€‚
    è‹¥å‰ç«¯å‚³ä¾†çš„å‹è™Ÿä¸åœ¨ç™½åå–®ï¼Œå‰‡å›é€€æˆé è¨­ã€‚
    å›å‚³ï¼šå¯¦éš›ç”Ÿæ•ˆçš„å‹è™Ÿï¼ˆå¯æ”¾åˆ° Debug é¡¯ç¤ºï¼‰ã€‚
    """
    global llm
    chosen = (user_choice or "").strip()
    model = chosen if chosen in GEN_MODEL_WHITELIST else GEN_MODEL_DEFAULT
    llm = ChatGroq(temperature=0, model=model)
    return model

# =========================
# å¸¸æ•¸ / å·¥å…·
# =========================
FOLLOWUP_PLACEHOLDER = "â€” é»é¸å»¶ä¼¸å•é¡Œï¼ˆæœƒè‡ªå‹•å¡«å…¥ä¸Šæ–¹è¼¸å…¥æ¡†ï¼‰ â€”"
_ZH_MAP = str.maketrans({"è‡º": "å°"})
_FULL2HALF = str.maketrans({
    "ï¼Œ": ",", "ã€‚": ".", "ï¼": "!", "ï¼Ÿ": "?",
    "ã€": "[", "ã€‘": "]", "ï¼ˆ": "(", "ï¼‰": ")",
    "ï¼…": "%", "ï¼‹": "+", "ï¼": "-", "ï¼": "=",
    "ï¼š": ":", "ï¼›": ";", "ã€": ",", "ã€€": " ",
})
_ZWS = re.compile(r"[\u200B-\u200D\uFEFF\u00A0\s]+")
NEWS_KWS = ["æ–°è","å¿«è¨Š","æ¶ˆæ¯","å ±å°","å ±é“","æ›´æ–°","é ­æ¢","press release","headline","breaking","news"]
STOCK_KWS = ["è‚¡ç¥¨","è‚¡åƒ¹","å ±åƒ¹","è¡Œæƒ…","èµ°å‹¢","æ”¶ç›¤","é–‹ç›¤","ç›®æ¨™åƒ¹","ä¼°å€¼","æœ¬ç›Šæ¯”","price","quote","stock","target price"]
FINREP_KWS = ["è²¡å ±","ç‡Ÿæ”¶","EPS","æ¯›åˆ©ç‡","ç²åˆ©","æŒ‡å¼•","guidance","å¹´å ±","å­£å ±",
              "è³‡ç”¢è² å‚µè¡¨","ç¾é‡‘æµé‡è¡¨","income statement","balance sheet","cash flow"]

_QWORDS_RE = re.compile(
    r"^(?:æ˜¯)?(?:å“ªæ”¯|å“ªæª”|å“ªå®¶|å“ªé–“|å“ªå€‹|å“ªä¸€(?:æ”¯|æª”|å®¶)|ä»€éº¼|ç”šéº¼|æ˜¯ä»€éº¼)(?:å‘¢|å—)?$"
)

DATE_KEYS_PRIMARY   = ("published_date", "event_date", "published_at", "pub_date")
DATE_KEYS_FALLBACK  = ("created_at", "create_at", "date", "ingested_at")

CHI_NUM = {"é›¶":0,"ã€‡":0,"â—‹":0,"ï¼¯":0,"ä¸€":1,"äºŒ":2,"ä¸‰":3,"å››":4,"äº”":5,"å…­":6,"ä¸ƒ":7,"å…«":8,"ä¹":9,"å":10,"åä¸€":11,"åäºŒ":12}

ALIAS_TICKER_MAP = {
    "è˜‹æœ": ("AAPL", ["Apple", "Apple Inc", "è˜‹æœå…¬å¸"]),
    "å¾®è»Ÿ": ("MSFT", ["Microsoft", "å¾®è»Ÿå…¬å¸"]),
    "è¼é”": ("NVDA", ["NVIDIA", "Nvidia", "è¼é”"]),
    "è°·æ­Œ": ("GOOGL", ["Alphabet", "Google", "è°·æ­Œ"]),
    "äºé¦¬éœ": ("AMZN", ["Amazon", "äºé¦¬éœ"]),
    "ç‰¹æ–¯æ‹‰": ("TSLA", ["Tesla", "ç‰¹æ–¯æ‹‰"]),
    "ç¾å…‰": ("MU", ["Micron", "Micron Technology", "ç¾å…‰"]),
}

def _tw_find_name_for_code(sym: str) -> Optional[str]:
    _load_tw_lists()
    return (_TW_CACHE.get("by_code") or {}).get((sym or "").upper())

def _cjk_overlap_score(a: str, b: str) -> int:
    import re
    A = set(re.findall(r"[\u4e00-\u9fff]", _normalize_name(a or "")))
    B = set(re.findall(r"[\u4e00-\u9fff]", _normalize_name(b or "")))
    return len(A & B)

def _norm_for_kw(s: str) -> str:
    s = (s or "").translate(_FULL2HALF).translate(_ZH_MAP).lower()
    return _ZWS.sub("", s)

def contains_any(text: str, kws: Iterable[str]) -> bool:
    t = _norm_for_kw(text)
    return any(_norm_for_kw(kw) in t for kw in kws)

def is_zh(text: str) -> bool:
    return bool(re.search(r"[\u4e00-\u9fff]", text or ""))

def extract_json_from_text(text: str) -> Optional[dict]:
    try:
        return json.loads(text)
    except Exception:
        pass
    for m in re.findall(r"(\{[\s\S]*?\})", text or ""):
        try:
            return json.loads(m)
        except Exception:
            continue
    return None

def parse_any_date(s: str) -> Optional[datetime]:
    try:
        d = dtparse(s)
        return d if d.tzinfo else d.replace(tzinfo=tz.tzutc())
    except Exception:
        return None

_FNAME_DATE_PAT = re.compile(
    r"(20\d{2})[-/._\s]?(0[1-9]|1[0-2])[-/._\s]?(0[1-9]|[12]\d|3[01])"
)

def _infer_date_from_name(md: Dict[str, Any]) -> Optional[datetime]:
    for key in ("filename", "title"):
        s = (md or {}).get(key) or ""
        m = _FNAME_DATE_PAT.search(s)
        if m:
            y, mo, d = map(int, m.groups())
            return datetime(y, mo, d).replace(tzinfo=tz.tzutc())
    return None

def get_doc_date_dt(md: Dict[str, Any]) -> Optional[datetime]:
    # å…ˆæŒ‰å„ªå…ˆåºæ‰¾
    for k in (*DATE_KEYS_PRIMARY, *DATE_KEYS_FALLBACK):
        v = (md or {}).get(k)
        if v:
            dt = parse_any_date(v)
            if dt:
                return dt
    # éƒ½æ²’æœ‰æ™‚ï¼Œå˜—è©¦æª”å/æ¨™é¡Œ
    return _infer_date_from_name(md)

# --- A) åš´æ ¼çš„å…¬å¸ token æ¸…æ´—ï¼ˆä¸é  mapï¼‰ ---
def clean_company_token(raw: str, *, original_text: str = "") -> Optional[str]:
    s = (raw or "").strip()

    # ğŸ”§ æ–°å¢ï¼šå»å£èªå‰ç¶´ï¼ˆé¿å…ã€Œè«‹è·Ÿæˆ‘èªªèç¨‹é›»ã€è¢«ç•¶æˆå…¬å¸ï¼‰
    s = re.sub(
        r"^(?:è«‹å•|è«‹çµ¦æˆ‘|è«‹æä¾›|è«‹å¹«æˆ‘|å¹«æˆ‘|å¹«å¿™|éº»ç…©|å¯ä¸å¯ä»¥|å¯ä»¥çµ¦æˆ‘|å¯ä»¥|èƒ½ä¸èƒ½|æ˜¯å¦|"
        r"å¹«æˆ‘æ•´ç†|è·Ÿæˆ‘èªª|è«‹è·Ÿæˆ‘èªª|å‘Šè¨´æˆ‘)\s*",
        "", s
    )

    # æ¨™é»èˆ‡ç©ºç™½
    s = re.sub(r"[ï¼Œ,ã€‚ï¼.ã€ï¼›;ï¼š:ï¼!ï¼Ÿ?\s\u3000ã€Œã€ã€ã€ï¼ˆï¼‰()ï¼»ï¼½\[\]ã€ã€‘<>ã€ˆã€‰â€¦]+", "", s)
    s = re.sub(r"(å—|å‘¢|å§|å‘€|å”·|å–”|å“¦|å•¦|è€¶)+$", "", s)  # å¥å°¾èªæ°£è©
    s = re.sub(r"(?:æ˜¯ä»€éº¼|ä»€éº¼æ˜¯)$", "", s)

    # å…ˆæŠŠå¸¸è¦‹ã€ŒæŸ¥è©¢å°¾ç¢¼ã€å»æ‰ï¼šçš„è²¡å ±/å¹´å ±/æ³•èªª/æ–°è/è‚¡åƒ¹/é‡é»/æ‘˜è¦/èµ°å‹¢â€¦ï¼ˆå«å¯é¸ã€Œçš„ã€ï¼‰
    s = re.sub(
        r"(?:çš„)?(?:è²¡å ±|å¹´å ±|å­£å ±|æ³•èªª(?:æœƒ)?|æ–°è|å¿«è¨Š|å ±å°|æ›´æ–°|å…¬å‘Š|é‡è¨Š|ç‡Ÿæ”¶|EPS|æ¯›åˆ©ç‡|å±•æœ›|guidance|"
        r"é‡é»|æ‘˜è¦|æ¦‚æ³|ä»‹ç´¹|å ±å‘Š|overview|è‚¡åƒ¹|å ±åƒ¹|ç›®æ¨™åƒ¹|èµ°å‹¢|ä»Šå¤©|ä»Šæ—¥|ç¾åœ¨|ç›®å‰)+$",
        "", s, flags=re.I
    )
    # ğŸ”§ æ–°å¢ï¼šå»æ‰ã€Œçš„è‚¡ / è‚¡ç¥¨ / å€‹è‚¡ / ä»£è™Ÿ / ä»£ç¢¼ã€é€™é¡å°¾ç¶´
    s = re.sub(r"(?:çš„|ä¹‹)?(?:è‚¡ç¥¨ä»£è™Ÿ|è‚¡ç¥¨|å€‹è‚¡|æœ¬è‚¡|è‚¡|ä»£è™Ÿ|ä»£ç¢¼)+$", "", s, flags=re.I)

    # å»æ‰å°¾ç«¯åŠ©è©ï¼ˆé¿å…ã€Œå¾®è»Ÿçš„ã€ã€Œå°ç©é›»ä¹‹ã€ï¼‰
    s = re.sub(r"(?:çš„|ä¹‹)+$", "", s)
    
    s = re.sub(r"(?:çš„)?(?:ä»‹ç´¹|ç°¡ä»‹|æ¦‚è¿°|overview|æ˜¯ä»€éº¼|ä»€éº¼æ˜¯|å®šç¾©)+$", "", s, flags=re.I)
    if re.search(r"(?:ç”¢æ¥­|è¡Œæ¥­|å¸‚å ´)$", s):  
        return None

    if _QWORDS_RE.fullmatch(s):
        return None
    
    if re.fullmatch(r"(?:æ˜¯|å—|å‘¢|å˜›|å•¥|ä½•|ä»€éº¼|ç”šéº¼)+", s):
        return None
    
    if not s:
        return None
    # éé•·/éçŸ­æˆ–æ®˜æ¸£
    if len(s) < 2 or len(s) > 20:
        return None
    if re.search(r"(å¯ä»¥çµ¦æˆ‘|è«‹å•|å¤šå°‘|åƒ¹æ ¼|æ–°è|æœ€æ–°|é‡é»|æ‘˜è¦|è²¡å ±|æ³•èªª|åœ¨å“ª|æ€éº¼|ç‚ºä½•|ç‚ºä»€éº¼|ä»Šå¤©|ä»Šæ—¥|ç¾åœ¨|ç›®å‰)", s):
        return None
    if re.fullmatch(r"\d{4}", s) and looks_like_year(s, original_text or ""):
        return None

    # åˆæ³•å‹æ…‹ï¼šticker / è‹±æ–‡å / ä¸­æ–‡å
    is_ticker = bool(re.fullmatch(r"[A-Za-z]{1,6}(?:\.[A-Za-z]{2,4})?|\d{4}(?:\.[A-Za-z]{2,4})?", s))
    is_en     = bool(re.fullmatch(r"[A-Za-z][A-Za-z0-9&\-.]{1,19}", s))
    is_cjk    = bool(re.fullmatch(r"[\u4e00-\u9fff]{2,10}", s))
    if not (is_ticker or is_en or is_cjk):
        return None
    return s

# --- B) ä»¥ Yahoo Autocomplete / resolve_symbol åšã€Œå¯æ”œå¼å…¸ç¯„åŒ–ã€ ---
def canonicalize_company_needles(
    token: str,
    *,
    fast_mode: bool = False,
    autoc_timeout: Optional[float] = None,
    autoc_max_langs: Optional[int] = None,
) -> Tuple[Optional[str], Set[str]]:
    """
    è¼¸å…¥ï¼šä¸€å€‹ä¹¾æ·¨ tokenï¼ˆå¯èƒ½æ˜¯ä¸­æ–‡å/è‹±æ–‡å/ä»£è™Ÿï¼‰
    è¼¸å‡ºï¼š(ticker or None, éœ€è¦åŒ¹é…çš„ needles set[å°å¯«])
    - å®Œå…¨ä¸ä½¿ç”¨æ‰‹å¯« map
    - å…ˆç”¨ resolve_symbol() ç¢ºèªæœ€çµ‚æœ‰æ•ˆ ticker
    - å†ç”¨ yahoo_autoc_all() æŠ“åˆ°è©² ticker çš„ display nameï¼ˆä¸­è‹±æ··ï¼‰ï¼Œè£œé€² needles
    """
    needles: Set[str] = set()
    if not token:
        return None, needles

    # âœ… å…ˆèµ°æ¥µå°åˆ¥åä¿éšªçµ²ï¼ˆé«˜é » Top Nï¼‰
    alias = ALIAS_TICKER_MAP.get(token)
    if alias:
        sym, extra = alias
        needles.update({token.lower(), sym.lower(), *[x.lower() for x in extra]})
        return sym, needles

    # 1) å…ˆå˜—è©¦è§£æå‡ºæœ‰æ•ˆ ticker
    sym = resolve_symbol(token, fast_mode=fast_mode, autoc_timeout=autoc_timeout, autoc_max_langs=autoc_max_langs)
    if sym:
        needles.add(sym.lower())
        import re
        digits = re.sub(r"\D", "", sym)
        if digits:
            needles.add(digits)
        # 2) å¾ autocomplete æŠ“é¡¯ç¤ºåç¨±ï¼ˆè·¨èªç³»/è·¨å€ï¼‰
        cands = yahoo_autoc_all(
            token,
            regions=("US","TW","HK"),
            langs=("zh-TW","zh-Hant-TW","en-US"),
            timeout=autoc_timeout or 5.0,
            max_langs=autoc_max_langs,
        )
        for c in cands:
            name = (c.get("name") or "").strip()
            if name:
                # å»é™¤å…¬å¸å°¾ç¶´ï¼ˆInc., Corp., Co., Ltd.ç­‰ï¼‰
                name_norm = re.sub(r"\b(inc|inc\.|corp|corp\.|co|co\.|ltd|ltd\.|plc|sa|ag|nv|kk)\b\.?", "", name, flags=re.I)
                needles.add(name_norm.lower())
        # å†è£œä¸€æ¬¡ç”¨ ticker åæŸ¥ï¼ˆé¿å… token ä¸æ˜¯åŸå§‹æŸ¥è©¢èªè¨€ï¼‰
        cands2 = yahoo_autoc_all(
            sym,
            regions=("US","TW","HK"),
            langs=("zh-TW","en-US"),
            timeout=autoc_timeout or 5.0,
            max_langs=autoc_max_langs,
        )
        for c in cands2:
            if c.get("symbol") and c.get("symbol").lower() == sym.lower():
                name = (c.get("name") or "").strip()
                if name:
                    name_norm = re.sub(r"\b(inc|inc\.|corp|corp\.|co|co\.|ltd|ltd\.|plc|sa|ag|nv|kk)\b\.?", "", name, flags=re.I)
                    needles.add(name_norm.lower())
        return sym, {n for n in needles if n}

    # 3) é‚„åŸå¤±æ•—æ™‚ï¼Œè©¦è‘—è®“ LLM ç¿»æˆè‹±æ–‡å…¬å¸åå†èµ°ä¸€æ¬¡
    en_name = to_english_company_name(token) or token
    if fast_mode:
        return None, {token.lower()}
    cands = yahoo_autoc_all(
        en_name,
        regions=("US","TW","HK"),
        langs=("en-US","zh-TW"),
        timeout=autoc_timeout or 5.0,
        max_langs=autoc_max_langs,
    )
    best = pick_best_yahoo_candidate(en_name, cands) if cands else None
    if best and is_valid_symbol(best):
        needles.add(best.lower())
        for c in cands:
            if (c.get("symbol") or "").lower() == best.lower():
                name = (c.get("name") or "").strip()
                if name:
                    name_norm = re.sub(r"\b(inc|inc\.|corp|corp\.|co|co\.|ltd|ltd\.|plc|sa|ag|nv|kk)\b\.?", "", name, flags=re.I)
                    needles.add(name_norm.lower())
        return best, needles

    # 4) å¯¦åœ¨æ‰¾ä¸åˆ°ï¼Œå°±è‡³å°‘æŠŠ token æœ¬èº«ç•¶ needleï¼ˆä½éšä¿éšœï¼‰
    needles.add(token.lower())
    return None, needles

# --- C) æ–‡æª”æ˜¯å¦æåŠå…¬å¸ï¼ˆä¸é  mapï¼Œåƒ…åšå°å¯«åŒ…å«ï¼‰ ---
def doc_mentions_any_company(md: Dict[str, Any], text: str, needles: Set[str]) -> bool:
    title = (md or {}).get("title", "") or ""
    source = (md or {}).get("source", "") or ""
    url = ((md or {}).get("doc_url") or (md or {}).get("url") or "") or ""
    fname = (md or {}).get("filename", "") or ""
    hay = " ".join([title, source, url, text or ""]).lower()

    # âœ… Apple çš„ SEC CIKï¼ˆé¿å…ã€Œapple payã€ä¹‹é¡èª¤å‘½ä¸­ï¼‰
    if "sec.gov" in url.lower() and ("data/320193" in url.lower() or "cik=0000320193" in url.lower()):
        return True

    for k in needles or []:
        k = (k or "").lower().strip()
        if not k:
            continue
        # è‹±æ–‡è©ï¼šç”¨å­—æ¯æ•¸å­—é‚Šç•Œï¼›ä¸­æ–‡è©ï¼šç›´æ¥åŒ…å«å³å¯
        if re.search(r"[a-z]", k):
            if re.search(rf"(?<![a-z0-9]){re.escape(k)}(?![a-z0-9])", hay):
                return True
        else:
            if k in hay:
                return True
    return False

# ===== å¹´ä»½è§£æ / æ–‡ä»¶å¹´åˆ¤å®š =====
# æ”¯æ´ FY24 / FY'24 / FYâ€™24 / FY2024
_FY_PAT = re.compile(r"\bFY\s*['â€™]?\s*(?:20)?(\d{2})\b", re.I)

# æ”¯æ´ "Fiscal 2024" èˆ‡ "Fiscal Year 2024"
_FISCAL_PAT = re.compile(r"\bFISCAL(?:\s+YEAR)?\s+(20\d{2})\b", re.I)

# 1Q24 / 3Q FY2024
_Q_PAT = re.compile(r"\b([1-4])Q(?:\s*|\s*FY\s*)?(?:20)?(\d{2})\b", re.I)

# å››ä½æ•¸å¹´ä»½ï¼ˆè‹¥ä½ å·²æœ‰ _Y4_PAT å°±ä¿ç•™åŸåå³å¯ï¼‰
_Y4_PAT = re.compile(r"\b((?:19|20)\d{2})\b")

def _fy_to_year(s: str) -> int:
    # å…©ä½æ•¸ â†’ 2000+yyï¼›å››ä½æ•¸å°±ç›´å›
    return int(s) if len(s) == 4 else 2000 + int(s)

def detect_doc_report_year_from_md_or_text(md: dict, text: str = "") -> int | None:
    hay = " ".join(str(x) for x in [
        md.get("title",""), md.get("filename",""), md.get("doc_url",""),
        md.get("url",""), md.get("source",""), " ".join((md.get("tags") or [])), text or ""
    ] if x)

    m = _FY_PAT.search(hay)
    if m:
        return _fy_to_year(m.group(1))

    m2 = _FISCAL_PAT.search(hay)
    if m2:
        return int(m2.group(1))

    m_q = _Q_PAT.search(hay)
    if m_q:
        return 2000 + int(m_q.group(2))

    m4 = _Y4_PAT.search(hay)
    if m4:
        return int(m4.group(1))

    dt = get_doc_date_dt(md)
    return dt.year if dt else None

def extract_report_year_targets(user_q: str) -> set[int]:
    """
    å¾ä½¿ç”¨è€…å•é¡ŒæŠ½å–å¯èƒ½çš„è²¡å ±å¹´ä»½ï¼ˆæœƒè¨ˆå¹´åº¦ï¼‰ã€‚æ”¯æ´ï¼š
    - 2023 / FY2023 / 1Q23 / 4Q2023 ç­‰
    å›å‚³å¯èƒ½å¹´ä»½é›†åˆï¼ˆæ•´æ•¸ï¼‰ï¼Œè‹¥æŠ“ä¸åˆ°å‰‡å›ç©ºé›†åˆã€‚
    """
    text = (user_q or "").strip()
    yrs: set[int] = set()

    # FY2023
    for m in _FY_PAT.finditer(text):
        yrs.add(int("20" + m.group(1)))

    # 1Q23 / 3Q FY2023
    for m in _Q_PAT.finditer(text):
        yy = int(m.group(2))
        yrs.add(2000 + yy)

    # ç›´æ¥å¯« 2023
    for m in _Y4_PAT.finditer(text):
        yrs.add(int(m.group(1)))

    return yrs

def detect_doc_report_year(md: dict) -> int | None:
    md = md or {}
    hay = " ".join(
        str(x) for x in [
            md.get("title",""), md.get("filename",""), md.get("doc_url",""),
            md.get("url",""), md.get("source",""), " ".join((md.get("tags") or []))
        ] if x
    )

    # å…ˆå˜—è©¦ FYï¼ˆæ”¯æ´ FY25/FY 2025ï¼‰
    m = _FY_PAT.search(hay)
    if m:
        yy = int(m.group(1))  # å…©ä½æ•¸
        return 2000 + yy

    # å†ä¾†å˜—è©¦ Qx FYyyï¼ˆä¾‹å¦‚ Q4 FY25ï¼‰
    m_q = _Q_PAT.search(hay)
    if m_q:
        yy = int(m_q.group(2))
        return 2000 + yy

    # ä¾‹å¦‚ "Microsoft 2025 Annual Report"
    m2 = _Y4_PAT.search(hay)
    if m2:
        return int(m2.group(1))

    dt = get_doc_date_dt(md)
    return dt.year if dt else None

def get_doc_date_str(md: Dict[str, Any]) -> str:
    dt = get_doc_date_dt(md)
    return dt.strftime("%Y-%m-%d") if dt else "æœªçŸ¥æ—¥æœŸ"

def _is_pdf_md(md: Dict[str, Any]) -> bool:
    s = (md.get("source") or md.get("filename") or md.get("doc_url") or "").lower()
    st = (md.get("source_type") or "").lower()
    tags = md.get("tags") or []
    if isinstance(tags, str):
        tags = [tags]
    return s.endswith(".pdf") or "pdf" in st or any("pdf" in str(t).lower() for t in tags)

def extract_page_number(md: Dict[str, Any]) -> Optional[int]:
    """
    å¾ metadata å–é ç¢¼ï¼›å¸¸è¦‹éµï¼š
      - 'page'ï¼ˆPyPDFLoader/Unstructured å¸¸è¦‹ï¼Œé€šå¸¸ 0-basedï¼‰
      - 'page_number'ï¼ˆæœ‰äº› loader ç”¨ 1-basedï¼‰
      - 'loc': {'page': ...}
    å° PDF é è¨­å°‡ 0-based è½‰ç‚º 1-basedï¼ˆæ›´ç¬¦åˆè®€è€…ç›´è¦ºï¼‰ã€‚
    """
    cand = md.get("page", None)
    if cand is None:
        cand = md.get("page_number", None)
    if cand is None:
        loc = md.get("loc") or md.get("location")
        if isinstance(loc, dict):
            cand = loc.get("page") or loc.get("page_number")

    try:
        p = int(cand)
    except Exception:
        return None

    # å¤šæ•¸ PDF loader çš„ page æ˜¯ 0-basedï¼Œçµ±ä¸€è½‰ 1-based é¡¯ç¤º
    if _is_pdf_md(md) and p >= 0:
        # å¦‚æœä¾†æºæœ¬ä¾†å°±æ˜¯ 1-basedï¼ˆå°‘æ•¸æƒ…æ³ï¼‰ï¼Œæœƒå¤šåŠ  1ï¼›å¦‚éœ€åš´æ ¼åˆ¤æ–·å¯å†åŠ æ——æ¨™æ§åˆ¶
        p = p + 1
    return p if p > 0 else None

def _compress_pages_to_ranges(pages: Iterable[int]) -> List[Tuple[int, int]]:
    """æŠŠé ç¢¼å»é‡ã€æ’åºã€å£“æˆé€£çºŒç¯„åœ [(start,end), ...]"""
    uniq = sorted(set(int(x) for x in pages if isinstance(x, int)))
    if not uniq:
        return []
    ranges = []
    start = prev = uniq[0]
    for p in uniq[1:]:
        if p == prev + 1:
            prev = p
        else:
            ranges.append((start, prev))
            start = prev = p
    ranges.append((start, prev))
    return ranges

def _format_pages_suffix(pages: Iterable[int]) -> str:
    """å›å‚³é©åˆæ”¾åœ¨è¡Œå°¾çš„é ç¢¼å­—ä¸²ï¼Œä¾‹å¦‚ï¼š' ï½œpp. 5â€“7, 12'ï¼›è‹¥ç©ºå‰‡å›ç©ºå­—ä¸²"""
    ranges = _compress_pages_to_ranges(pages)
    if not ranges:
        return ""
    parts = []
    for a, b in ranges:
        parts.append(f"p. {a}" if a == b else f"pp. {a}â€“{b}")
    return " ï½œ" + ", ".join(parts)

def get_bucket(md: Dict[str, Any]) -> str:
    s = ((md or {}).get("source") or (md or {}).get("source_type") or "").lower()
    title = (md or {}).get("title", "").lower()
    url = ((md or {}).get("doc_url") or (md or {}).get("url") or "").lower()
    tags = md.get("tags") or []
    if isinstance(tags, str):
        tags = [tags]
    tags_l = [str(t).lower() for t in tags]

    # filing
    if (
        "sec_filing" in s or re.search(r"\bsec(_|$|\b)", s) or "sec.gov" in url
        or any(k in title for k in _FILING_HINTS)
        or any(k in tags_l for k in ("filing","10-k","10q","8-k","20-f","6-k","s-1","f-1","def 14a","sec"))
    ):
        return "filing"

    # âœ… çµ±ä¸€ç‚º transcriptï¼ˆå« transcript_summary/summaryï¼‰
    transcript_hints = ("transcript", "transcripts", "transcripts_summary",
                        "earnings call", "conference call", "prepared remarks", "q&a", "q & a",
                        "æ³•èªª", "æ³•èªªæœƒ", "é›»è©±æœƒè­°")
    if any(h in s for h in transcript_hints) or any(h in title for h in transcript_hints) \
       or any("transcript" in t for t in tags_l):
        return "transcript"


    if "blog" in s or "blogs" in s or "post" in s: return "blog"
    if "research" in s or "research_report" in s: return "research"
    if "pdf" in s or "report" in s or "pdf_report" in s: return "pdf"
    if any("research" in t for t in tags_l): return "research"
    if any("blog" in t for t in tags_l): return "blog"
    if any(("pdf" in t) or ("report" in t) for t in tags_l): return "pdf"
    return "other"

_FILING_HINTS = (
    "form 10-k","10-k","form 10-q","10-q","8-k",
    "20-f","form 20-f","6-k","form 6-k",
    "s-1","form s-1","f-1","form f-1",
    "def 14a","schedule 14a",
    "annual report"  # ä»ä¿ç•™
)
def is_filing_like(md: Dict[str, Any]) -> bool:
    s = ((md or {}).get("source") or (md or {}).get("source_type") or "").lower()
    title = (md or {}).get("title", "").lower()
    url = ((md or {}).get("doc_url") or (md or {}).get("url") or "").lower()
    tags = (md or {}).get("tags") or []
    if isinstance(tags, str):
        tags = [tags]
    tags_l = [str(t).lower() for t in tags]

    # æ›´ç²¾æº–çš„ SEC ä¾†æºåˆ¤æ–·ï¼šé¿å… "security" èª¤æ“Š
    if "sec.gov" in url or re.search(r"\bsec(_|$|\b)", s):
        return True

    if any(h in title for h in _FILING_HINTS): return True
    if any(h in url   for h in _FILING_HINTS): return True
    if any(h in tags_l for h in _FILING_HINTS): return True

    return get_bucket(md) == "filing"

def _recency_weight(md: Dict[str, Any], half_life_days: int = 45) -> float:
    """
    æŒ‡æ•¸è¡°æ¸›çš„æ–°é®®åº¦åˆ†æ•¸ï¼šè¶Šæ–°è¶Šæ¥è¿‘ 1ï¼Œè¶ŠèˆŠè¶Šæ¥è¿‘ 0ã€‚
    half_life_days å¯ä¾éœ€æ±‚èª¿æ•´ï¼ˆä¾‹å¦‚ 30~90ï¼‰ã€‚
    """
    dt = get_doc_date_dt(md or {})
    if not dt:
        return 1.0
    if not dt.tzinfo:
        dt = dt.replace(tzinfo=tz.tzutc())
    now = datetime.now(tz=tz.tzutc())
    days = max(0.0, (now - dt).total_seconds() / 86400.0)
    return float(np.exp(-days / float(half_life_days)))

def filter_by_recent(docs: List[Any], cos_scores: List[float], days: int = 120, min_keep: int = 6):
    """
    å…ˆå˜—è©¦ä¿ç•™è¿‘ X å¤©çš„å€™é¸ï¼›è‹¥å‰©å¤ªå°‘ï¼ˆ< min_keepï¼‰ï¼Œå‰‡å›é€€ç‚ºåŸä¸²åˆ—ã€‚
    """
    now = datetime.now(tz=tz.tzutc())
    kept_docs, kept_scores = [], []
    for d, sc in zip(docs, cos_scores):
        dt = get_doc_date_dt(d.metadata or {})
        if not dt:
            continue
        if not dt.tzinfo:
            dt = dt.replace(tzinfo=tz.tzutc())
        age_days = (now - dt).total_seconds() / 86400.0
        if age_days <= days:
            kept_docs.append(d); kept_scores.append(sc)
    if len(kept_docs) >= max(1, min_keep):
        return kept_docs, kept_scores
    return docs, cos_scores  # å›é€€

# =========================
# Yahoo / ä»£è™Ÿè§£æ / è‚¡åƒ¹ / æ–°èï¼ˆå¼·åŒ–ï¼‰
# =========================
@lru_cache(maxsize=2048)
def _yahoo_v7_quote(symbol: str) -> dict:
    try:
        r = _req_get("https://query1.finance.yahoo.com/v7/finance/quote",
                     params={"symbols": symbol}, timeout=5)
        items = (r.json().get("quoteResponse", {}) or {}).get("result") or []
        return items[0] if items else {}
    except Exception:
        return {}
    
def _is_tradable_via_quote(symbol: str) -> bool:
    """ç•¶ v8 chart é‚„æ²’æœ‰è³‡æ–™ï¼ˆæ–°è‚¡å¸¸è¦‹ï¼‰æ™‚ï¼Œç”¨ v7 quote æª¢æŸ¥æ˜¯å¦æœ‰å¯ç”¨å ±åƒ¹ã€‚"""
    q = _yahoo_v7_quote(symbol)
    if not q:
        return False
    # ä»»ä½•ä¸€å€‹åƒ¹æ ¼æ¬„ä½æœ‰æ•¸å€¼å³å¯è¦–ç‚ºæœ‰æ•ˆ
    price_fields = [q.get("regularMarketPrice"), q.get("postMarketPrice"), q.get("preMarketPrice")]
    has_price = any(isinstance(x, (int, float)) for x in price_fields)
    # äº¤æ˜“æ‰€ä¹Ÿè¦åˆç†ï¼ˆå°è‚¡å„ªå…ˆï¼‰
    ex_ok = _tw_exchange_ok(symbol, q) or bool(q.get("exchange"))
    return bool(has_price and ex_ok)

def _tw_exchange_ok(symbol: str, quote: Optional[dict] = None) -> bool:
    s = (symbol or "").upper()
    if re.match(r"^\d{4}\.(TW|TWO)$", s):
        return True
    q = quote or _yahoo_v7_quote(s)
    ex = (q.get("fullExchangeName") or q.get("exchange") or "").lower()
    if ("taiwan" in ex) or ("taipei" in ex):
        return True
    if s.endswith(".TW") or s.endswith(".TWO"):
        return True
    return False

@lru_cache(maxsize=2048)
def yahoo_search_symbols_by_keyword(q: str, limit: int = 3) -> list[str]:
    """
    ç”¨ Yahoo è‚¡å¸‚çš„ç«™å…§æœå°‹é ç•¶ fallbackï¼Œç›´æ¥å¾ HTML æŠ“ /quote/<symbol>ã€‚
    å›å‚³å·²é€šé chart é©—è­‰çš„ symbolï¼ˆå„ªå…ˆ TW/TWOï¼‰ã€‚
    """
    q = (q or "").strip()
    if not q:
        return []

    # è©¦å¹¾å€‹å¸¸è¦‹ query åƒæ•¸ï¼ˆYahoo ä¸å®šæœŸæ›´å‹•ï¼‰
    endpoints = [
        ("https://tw.stock.yahoo.com/s",      {"k": q}),
        ("https://tw.stock.yahoo.com/search", {"query": q}),
        ("https://tw.stock.yahoo.com/search", {"p": q}),
    ]

    out, seen = [], set()
    for url, params in endpoints:
        r = _req_get(url, params=params, timeout=6)
        if not r:
            continue
        html = r.text or ""
        # æŠ“æ‰€æœ‰ /quote/<symbol> ç‰‡æ®µï¼ˆå« TW/TWO èˆ‡ç¾è‚¡ï¼‰
        for sym in re.findall(r"/quote/([A-Za-z0-9\.-]{2,15})", html):
            s = sym.upper()
            # å…ˆåå¥½å°è‚¡æ¨£å¼ï¼›å†æ”¾å¯¬åˆ°ä¸€èˆ¬ symbol
            ok = bool(re.match(r"^\d{4}\.(?:TW|TWO)$", s) or re.match(r"^[A-Z]{1,5}(?:\.[A-Z]{2,4})?$", s))
            if not ok or s in seen:
                continue
            seen.add(s)
            if _is_valid_symbol_cached(s, lookback_days=60):
                out.append(s)
                if len(out) >= limit:
                    return out
        if out:
            return out
    return out

def fetch_realtime_quote_batch(symbols: List[str], cache_ttl: int = 30, timeout: Optional[float] = None) -> List[dict]:
    syms = [s for s in (symbols or []) if s]
    if not syms:
        return []
    key = "rtq:" + ",".join(sorted(syms))
    cached = _cache_get(key)
    if cached is not None:
        return cached

    r = _req_get(
        "https://query1.finance.yahoo.com/v7/finance/quote",
        params={"symbols": ",".join(syms)},
        timeout=timeout or _REQ_TIMEOUT,
    )
    out: List[dict] = []
    if not r:
        _cache_set(key, out, cache_ttl)
        return out

    tz_tw = TZ_TW
    try:
        items = r.json().get("quoteResponse", {}).get("result") or []
        for it in items:
            epoch = it.get("regularMarketTime") or it.get("postMarketTime") or it.get("preMarketTime")
            t_local = datetime.fromtimestamp(epoch, tz=tz_tw).strftime("%Y-%m-%d %H:%M:%S") if epoch else None
            price = it.get("regularMarketPrice")
            chg = it.get("regularMarketChange")
            chg_pct = it.get("regularMarketChangePercent")
            pm_price = it.get("postMarketPrice") or it.get("preMarketPrice")
            pm_chg = it.get("postMarketChange") or it.get("preMarketChange")
            pm_chg_pct = it.get("postMarketChangePercent") or it.get("preMarketChangePercent")
            out.append({
                "symbol": it.get("symbol"),
                "name": it.get("shortName") or it.get("longName") or it.get("displayName") or it.get("symbol"),
                "currency": it.get("currency"),
                "exchange": it.get("fullExchangeName") or it.get("exchange"),
                "marketState": it.get("marketState"),
                "time": t_local,
                "price": price,
                "change": chg,
                "changePercent": chg_pct,
                "pm_price": pm_price,
                "pm_change": pm_chg,
                "pm_changePercent": pm_chg_pct,
            })
    except Exception:
        out = []

    _cache_set(key, out, cache_ttl)
    return out

def format_realtime_quotes(quotes: List[dict]) -> str:
    if not quotes:
        return "æŠ±æ­‰ï¼Œæš«æ™‚ç„¡æ³•å–å¾—å³æ™‚å ±åƒ¹ã€‚"

    lines = []
    for q in quotes:
        sym = q.get("symbol") or "?"
        # åç¨±å„ªå…ˆç”¨ v7 å›ä¾†çš„ nameï¼›æ²’æœ‰æ™‚ç”¨æˆ‘å€‘çš„å‚™æ´æŸ¥æ‰¾
        nm = (q.get("name") or "").strip() or _display_name_for_symbol(sym) or ""
        head = f"{sym}ï¼ˆ{nm}ï¼‰" if nm else sym

        # æ²’æœ‰åƒ¹æ ¼ â†’ ç›´æ¥é¡¯ç¤ºã€Œæš«ç„¡å ±åƒ¹ã€
        if q.get("price") is None:
            lines.append(f"{head}: æš«ç„¡å ±åƒ¹")
            continue

        chg_pct = f"{q['changePercent']:+.2f}%" if isinstance(q.get("changePercent"), (int, float)) else "â€”"
        chg = f"{q['change']:+.3f}" if isinstance(q.get("change"), (int, float)) else "â€”"
        tail = f"ï¼ˆ{q.get('currency','')}ï¼Œ{q.get('exchange','')}ï¼Œ{q.get('marketState','')}ï¼Œ{q.get('time','')}ï¼‰"

        line = f"{head} ç¾åƒ¹ {q['price']:.3f}ï¼Œæ¼²è·Œ {chg}ï¼ˆ{chg_pct}ï¼‰{tail}"

        # ç›¤å¾Œ / ç›¤å‰
        if q.get("pm_price") is not None:
            pm_chg_pct = f"{q['pm_changePercent']:+.2f}%" if isinstance(q.get("pm_changePercent"), (int, float)) else "â€”"
            line += f"\n  â†³ ç›¤å¾Œ/ç›¤å‰ {q['pm_price']:.3f}ï¼ˆ{pm_chg_pct}ï¼‰"

        lines.append(line)

    return "\n".join(lines)

@lru_cache(maxsize=512)
def to_english_company_name(name: str) -> Optional[str]:
    """ç”¨ LLM ç”¢ç”Ÿå¸¸è¦‹è‹±æ–‡å®˜æ–¹åç¨±ï¼ˆåªå›åç¨±ï¼‰ï¼Œå†å»æ‰“ en-US çš„è£œå…¨ã€‚"""
    try:
        txt = classifier_llm.invoke(
            f"æŠŠä»¥ä¸‹å…¬å¸åç¨±ç¿»æˆå¸¸è¦‹è‹±æ–‡å®˜æ–¹åï¼Œåƒ…è¼¸å‡ºåç¨±ï¼Œä¸è¦è§£é‡‹ï¼š{name}"
        ).content.strip()
        # åªå…è¨±ç°¡çŸ­ã€å®‰å…¨çš„åå­—
        if 1 <= len(txt) <= 60 and not re.search(r"[^0-9A-Za-z .,&-]", txt):
            return txt
    except Exception:
        pass
    return None

@lru_cache(maxsize=512)
def guess_tickers_via_llm(name: str, max_n: int = 5) -> List[str]:
    """æœ€å¾Œä¸€å±¤ï¼šè«‹ LLM ç›´æ¥çµ¦å¯èƒ½çš„ Yahoo ä»£è™Ÿæ¸…å–®ï¼ˆä¸­/è‹±/æ•¸ï¼‰ï¼Œä¹‹å¾Œé€ä¸€ä»¥ chart é©—è­‰ã€‚"""
    prompt = (
        "åªè¼¸å‡ºä¸€è¡Œåˆæ³• JSONï¼š{\"tickers\":[...]}ã€‚"
        "çµ¦æˆ‘é€™å®¶å…¬å¸æœ€å¯èƒ½çš„ Yahoo Finance ä»£è™Ÿï¼ˆå¯å« .TW/.TWO/.HKï¼‰ï¼Œ"
        "ä¸è¦é™„ä»»ä½•è§£é‡‹æˆ–å…¶å®ƒæ¬„ä½ï¼š\nå…¬å¸ï¼š" + name
    )
    try:
        raw = classifier_llm.invoke(prompt).content.strip()
        obj = extract_json_from_text(raw) or {}
        items = obj.get("tickers", []) if isinstance(obj, dict) else []
        out = []
        for s in items:
            if isinstance(s, str) and 1 <= len(s) <= 15 and re.fullmatch(r"[A-Za-z0-9\.]+", s):
                out.append(s.strip())
        return out[:max_n]
    except Exception:
        return []

def _is_valid_symbol_cached(symbol: str, lookback_days: int = 30) -> bool:
    return is_valid_symbol(symbol, lookback_days=lookback_days)

@lru_cache(maxsize=4096)
def is_valid_symbol(symbol: str, lookback_days: int = 10) -> bool:
    try:
        now = int(datetime.now().timestamp())
        p1 = now - lookback_days * 86400
        r = _req_get(
            f"https://query1.finance.yahoo.com/v8/finance/chart/{symbol}",
            params={"period1": p1, "period2": now, "interval": "1d"},
            headers={"User-Agent": "Mozilla/5.0"}, timeout=5
        )
        res = (r.json().get("chart", {}) if r else {}).get("result")
        ts = (res[0] or {}).get("timestamp") if res else []
        if ts:
            return True
        # å¾Œæ´ï¼šå°è‚¡æ–°è‚¡
        if re.match(r"^\d{4}\.(TW|TWO)$", (symbol or "").upper()):
            return _is_tradable_via_quote(symbol)
        return False
    except Exception:
        if re.match(r"^\d{4}\.(TW|TWO)$", (symbol or "").upper()):
            return _is_tradable_via_quote(symbol)
        return False

def _parse_jsonp(text: str) -> Optional[dict]:
    m = re.search(r"\((\{.*\})\)\s*$", text or "")
    if m:
        try: return json.loads(m.group(1))
        except Exception: return None
    try: return json.loads(text)
    except Exception: return None

def pick_best_yahoo_candidate(q: str, cands: List[dict]) -> Optional[str]:
    # ç›¡é‡ç”¨ä¸­æ–‡é‡ç–Š + æ¨¡ç³Šåº¦æ’å‡ºæœ€å¯èƒ½ï¼›è‹¥å…¨æ˜¯è‹±æ–‡åï¼Œä¹Ÿæœƒå›ç¬¬ä¸€å€‹æœ‰æ•ˆå€™é¸
    qn = _normalize_name(q)
    has_rf = bool(fuzz)
    ranked: List[Tuple[float, dict]] = []
    for c in cands or []:
        name = _normalize_name(c.get("name","") or "")
        base = 0.0
        if has_rf and name:
            base = float(fuzz.token_set_ratio(qn, name))  # 0..100
        bias = 12.0 if c.get("region")=="TW" else (8.0 if c.get("region")=="US" else 0.0)
        ranked.append((base + bias, c))
    if not ranked:
        return None
    ranked.sort(key=lambda x: x[0], reverse=True)
    sym = ranked[0][1].get("symbol")
    return sym if sym and _is_valid_symbol_cached(sym, lookback_days=30) else None


@lru_cache(maxsize=4096)
def _yahoo_autoc_collect(q: str, regions, lang: str, timeout: float, *, legacy: bool) -> List[dict]:
    """Internal helper for Yahoo autocomplete; preserves original behavior."""
    if not (q or "").strip():
        return []
    out: List[dict] = []
    base = "https://s.yimg.com/aq/autoc" if legacy else "https://autoc.finance.yahoo.com/autoc"
    for rg in regions:
        try:
            params = {"query": q, "region": rg, "lang": lang}
            if legacy:
                params["callback"] = ""
            r = _req_get(base, params=params,
                         headers={"User-Agent":"Mozilla/5.0"}, timeout=timeout)
            data = _parse_jsonp(r.text if r else "") or {}
            items = ((data.get("ResultSet") or {}).get("Result") or [])
            for it in items:
                typ = (it.get("type") or it.get("Type") or "").upper()
                if typ not in ("S","E","EQUITY","ETF","FUND"): continue
                out.append({
                    "symbol": it.get("symbol") or it.get("Symbol"),
                    "name": it.get("name") or it.get("Name"),
                    "exch": (it.get("exch") or it.get("Exch") or "").upper(),
                    "region": rg,
                })
        except Exception:
            pass
        finally:
            time.sleep(0.05)
    def _score(x: dict) -> Tuple[int,int,int]:
        rg_score = {"TW":3,"US":2,"HK":1}.get(x.get("region","").upper(),0)
        ex_ok = int(any(k in (x.get("exch") or "") for k in ("TAI","TWO","NYQ","NMS","NQ","NAS")))
        return (rg_score, ex_ok, -len(x.get("symbol") or ""))
    out.sort(key=_score, reverse=True)
    seen, uniq = set(), []
    for c in out:
        s = c.get("symbol")
        if s and s not in seen:
            uniq.append(c); seen.add(s)
    return uniq


@lru_cache(maxsize=4096)
def yahoo_autoc_legacy(q: str, regions=("TW","US","HK"), lang="zh-TW", timeout: float=3.0) -> List[dict]:
    # Reuse shared collector to avoid duplication
    return _yahoo_autoc_collect(q, regions, lang, timeout, legacy=True)

@lru_cache(maxsize=4096)
def yahoo_autoc(q: str, regions=("TW","US","HK"), lang="zh-TW", timeout: float=3.0) -> List[dict]:
    return _yahoo_autoc_collect(q, regions, lang, timeout, legacy=False)

@lru_cache(maxsize=4096)
def yahoo_autoc_all(
    q: str,
    regions=("TW","US","HK"),
    langs=("zh-TW","zh-Hant-TW","zh-HK","en-US"),
    timeout: float = 5.0,
    *,
    max_langs: Optional[int] = None,
) -> List[dict]:
    """åŒæ™‚å˜—è©¦ legacy èˆ‡æ–°ç‰ˆ autocï¼Œä¸¦è¼ªè©¢å¤šèªç³»ï¼›æ•´åˆå»é‡ã€‚

    max_langs: é™åˆ¶æœ€å¤šå˜—è©¦å¹¾å€‹èªç³»ï¼ŒNone è¡¨ç¤ºå…¨éƒ¨ã€‚
    """
    if not (q or "").strip():
        return []
    out: List[dict] = []
    lang_iter = list(langs)
    if max_langs is not None:
        lang_iter = lang_iter[:max_langs]
    for lang in lang_iter:
        try:
            out += yahoo_autoc_legacy(q, regions=regions, lang=lang, timeout=timeout)
        except Exception:
            pass
        try:
            out += yahoo_autoc(q, regions=regions, lang=lang, timeout=timeout)
        except Exception:
            pass
        # è‹¥å·²å–å¾—å€™é¸ï¼Œææ—©åœæ­¢å¾ŒçºŒèªç³»è¼ªè©¢ä»¥é™ä½å»¶é²
        if out:
            break
        time.sleep(0.05)
    # å»é‡
    seen, uniq = set(), []
    for c in out:
        sym = c.get("symbol")
        if sym and sym not in seen:
            uniq.append(c); seen.add(sym)
    return uniq

# --- TWSE/TPEX åç¨±â†’ä»£è™Ÿï¼ˆæ¯æ—¥å¿«å–ï¼‰ ---
_TW_CACHE: Dict[str, Any] = {"ts": 0, "by_name": {}, "by_code": {}}


def _is_tw_cache_fresh(max_age: float = 24 * 3600) -> bool:
    if not _TW_CACHE.get("by_name"):
        return False
    ts = float(_TW_CACHE.get("ts") or 0)
    return (time.time() - ts) < max_age

def _normalize_name(name: str) -> str:
    s = (name or "").translate(_ZH_MAP)
    s = re.sub(r"\s+|ã€€+", "", s)
    s = re.sub(r"[()ï¼ˆï¼‰ã€ã€‘ã€ã€ã€Œã€ã€Šã€‹ã€ˆã€‰Â·ï¼ãƒ»ï½¥ï¼ï¼ï¼Œ,ã€‚ï¼\\.]", "", s)
    s = s.replace("è‚¡ä»½æœ‰é™å…¬å¸", "").replace("æ§è‚¡", "").replace("é›†åœ˜", "").replace("å…¬å¸", "").replace("è‚¡ä»½", "").replace("æœ‰é™", "")
    s = re.sub(r"-?\s*KY$", "", s, flags=re.I)
    s = re.sub(r"-?\s*ï¼«ï¼¹$", "", s, flags=re.I)
    return s

def _ordered_subseq(a: str, b: str) -> bool:
    """åˆ¤æ–· a æ˜¯å¦ç‚º b çš„æœ‰åºå­åºåˆ—ï¼›è™•ç†ã€Œå°ç©é›»ã€ vs ã€Œå°ç£ç©é«”é›»è·¯è£½é€ ã€é€™é¡ç¸®å¯«ã€‚"""
    it = iter(b)
    return all(ch in it for ch in a)

# rapidfuzz å·²æ–¼æª”é ­é›†ä¸­è™•ç†ï¼›æ­¤è™•ä¸å†é‡è¤‡å°å…¥

def _load_tw_lists(force: bool=False) -> None:
    if not force and (time.time() - _TW_CACHE["ts"] < 24*3600) and _TW_CACHE["by_name"]:
        return
    by_name: Dict[str, set] = {}
    sources = [
        ("https://openapi.twse.com.tw/v1/opendata/t187ap03_L", ".TW"),
        ("https://www.tpex.org.tw/openapi/v1/mopsfin_t187ap03_L", ".TWO"),
    ]
    headers = {"User-Agent":"Mozilla/5.0","Accept":"application/json, */*"}
    for url, suf in sources:
        try:
            r = _req_get(url, headers=headers, timeout=8)
            data = r.json() if (r and r.ok) else []
            for row in data:
                code = (row.get("å…¬å¸ä»£è™Ÿ") or row.get("æœ‰åƒ¹è­‰åˆ¸ä»£è™Ÿ") or row.get("Code") or "").strip()
                name = (row.get("å…¬å¸åç¨±") or row.get("æœ‰åƒ¹è­‰åˆ¸åç¨±") or row.get("Name") or "").strip()
                if re.fullmatch(r"\d{4}", code) and name:
                    norm = _normalize_name(name)
                    by_name.setdefault(norm, set()).add(code + suf)
                    norm_no_ky = re.sub(r"-?\s*KY$", "", norm, flags=re.I)
                    if norm_no_ky != norm:
                        by_name.setdefault(norm_no_ky, set()).add(code + suf)

                    _TW_CACHE["by_code"][code + suf] = name  # â† æ–°å¢ï¼šä»£è™Ÿâ†’ä¸­æ–‡ååæŸ¥**
        except Exception:
            continue
    _TW_CACHE["by_name"] = by_name
    _TW_CACHE["ts"] = time.time()

def _resolve_via_yahoo_pipeline(
    q_raw: str,
    *,
    fast_mode: bool = False,
    autoc_timeout: Optional[float] = None,
    autoc_max_langs: Optional[int] = None,
    validate_symbols: bool = True,
) -> Optional[str]:
    q_raw = (q_raw or "").strip()
    if not q_raw:
        return None

    timeout_val = autoc_timeout or (1.2 if fast_mode else 5.0)
    max_langs_val = autoc_max_langs if autoc_max_langs is not None else (2 if fast_mode else None)
    default_langs = ("zh-TW", "zh-Hant-TW", "zh-HK", "en-US") if not fast_mode else ("zh-TW", "en-US")

    def _prefer_tw_first(cands_syms: list[str]) -> list[str]:
        return sorted(set(cands_syms), key=lambda s: (0 if re.match(r"^\d{4}\.(TW|TWO)$", s.upper()) else 1, len(s)))

    def _calc_max_langs(lang_seq: Iterable[str]) -> Optional[int]:
        if max_langs_val is None:
            return None
        seq = list(lang_seq)
        return min(len(seq), max_langs_val)

    def _autoc_candidates(query: str, *, regions=("TW", "US", "HK"), langs: Optional[Iterable[str]] = None) -> List[dict]:
        lang_seq = tuple(langs or default_langs)
        ml = _calc_max_langs(lang_seq)
        return yahoo_autoc_all(query, regions=regions, langs=lang_seq, timeout=timeout_val, max_langs=ml)

    def _is_valid(sym: str) -> bool:
        if not validate_symbols:
            return True
        return _is_valid_symbol_cached(sym, lookback_days=30)

    # 1) ç›´æ¥ç”¨ autocompleteï¼ˆå¤šèªå¤šå€ï¼‰
    syms = []
    for c in _autoc_candidates(q_raw):
        s = (c.get("symbol") or "").upper()
        if s and _is_valid(s):
            syms.append(s)
    for s in _prefer_tw_first(syms):
        return s

    if fast_mode:
        return None

    # 2) è‹±è­¯å†è©¦ä¸€æ¬¡ï¼ˆå¸¸è¦‹æ–¼ä¸­æ–‡å…¬å¸åï¼‰
    en_name = to_english_company_name(q_raw)
    if en_name:
        syms2 = []
        for c in _autoc_candidates(en_name, langs=("en-US", "zh-TW")):
            s = (c.get("symbol") or "").upper()
            if s and _is_valid(s):
                syms2.append(s)
        for s in _prefer_tw_first(syms2):
            return s

    # 3) LLM çŒœ ticker â†’ é€ä¸€é©—è­‰ï¼ˆéœ€é€šéä¸­æ–‡åé‡ç–Šé–€æª»ï¼‰
    for s in _prefer_tw_first(guess_tickers_via_llm(q_raw, max_n=6)):
        if not _is_valid(s):
            continue

        ok = False

        if re.match(r"^\d{4}\.(TW|TWO)$", s.upper()):
            nm = _tw_find_name_for_code(s) or ""
            if _cjk_overlap_score(q_raw, nm) >= 2:
                ok = True

        if not ok:
            for c in _autoc_candidates(s, langs=("zh-TW", "en-US")):
                if (c.get("symbol", "").upper() == s.upper()):
                    nm = c.get("name") or ""
                    if _cjk_overlap_score(q_raw, nm) >= 2:
                        ok = True
                        break

        if ok:
            return s

    # 4) ç«™å…§æœå°‹ï¼ˆHTMLï¼‰â†’ äºŒæ¬¡ä¸­æ–‡åé‡ç–Šé©—è­‰
    for s in yahoo_search_symbols_by_keyword(q_raw, limit=5):
        ok = False
        for c in _autoc_candidates(s, langs=("zh-TW", "en-US")):
            if (c.get("symbol", "").upper() != s.upper()):
                continue
            qn = _normalize_name(q_raw)
            cn = _normalize_name(c.get("name", ""))
            A = set(re.findall(r"[\u4e00-\u9fff]", qn))
            B = set(re.findall(r"[\u4e00-\u9fff]", cn))
            overlap = len(A & B)

            sim_ok = False
            if fuzz:
                try:
                    sim_ok = fuzz.partial_ratio(qn, cn) >= 85
                except Exception:
                    pass

            if overlap >= 2 or sim_ok:
                ok = True
                break
        if ok:
            return s

    return None

def resolve_symbol_by_name(
    name: str,
    *,
    fast_mode: bool = False,
    autoc_timeout: Optional[float] = None,
    autoc_max_langs: Optional[int] = None,
) -> Optional[str]:
    q_raw = (name or "").strip()
    if not q_raw:
        return None
    timeout_val = autoc_timeout or (1.2 if fast_mode else 5.0)
    max_langs_val = autoc_max_langs if autoc_max_langs is not None else (2 if fast_mode else None)
    default_langs = ("zh-TW", "zh-Hant-TW", "en-US") if not fast_mode else ("zh-TW", "en-US")

    def _calc_max_langs(lang_seq: Iterable[str]) -> Optional[int]:
        if max_langs_val is None:
            return None
        return min(len(list(lang_seq)), max_langs_val)

    def _autoc_all(query: str, *, regions=("TW", "US", "HK"), langs: Optional[Iterable[str]] = None) -> List[dict]:
        lang_seq = tuple(langs or default_langs)
        ml = _calc_max_langs(lang_seq)
        return yahoo_autoc_all(query, regions=regions, langs=lang_seq, timeout=timeout_val, max_langs=ml)
    
    # A) å…ˆæŸ¥æŒä¹…å¿«å–ï¼ˆå‘½ä¸­å°±å›ï¼Œé¿å…é‡æ‰“å¤šå€‹ç«¯é»ï¼‰
    hit_rec = _symcache_get_record(q_raw)
    if hit_rec:
        hit = (hit_rec.get("symbol") or "").strip()
        if hit:
            try:
                age = max(0.0, time.time() - float(hit_rec.get("ts", 0)))
            except Exception:
                age = None
            ttl_days = max(1, SYMBOL_MAP_TTL_DAYS)
            revalidate_after = ttl_days * 43200  # 0.5 * 86400
            if age is None or age <= revalidate_after:
                _remember_resolved_symbol(q_raw, hit)
                return hit
            cands = _autoc_all(q_raw, regions=("TW","US","HK"), langs=("zh-TW","en-US"))
            syms = { (c.get("symbol") or "").upper() for c in cands }
            _load_tw_lists()
            norm = _normalize_name(q_raw)
            tw_ok = hit.upper() in set((_TW_CACHE.get("by_name") or {}).get(norm, []))
            if hit.upper() in syms or tw_ok:
                _symcache_put(q_raw, hit)
                _remember_resolved_symbol(q_raw, hit)
                return hit
            _SYM_KV["map"].pop(_sym_norm_key(q_raw), None)
            _symcache_flush()

    # B) Yahoo-first
    sym = _resolve_via_yahoo_pipeline(
        q_raw,
        fast_mode=fast_mode,
        autoc_timeout=autoc_timeout,
        autoc_max_langs=autoc_max_langs,
        validate_symbols=not fast_mode,
    )
    if sym:
        if is_zh(q_raw):
            ok = False
            if re.match(r"^\d{4}\.(TW|TWO)$", sym.upper()):
                nm = _tw_find_name_for_code(sym) or ""
                ok = _cjk_overlap_score(q_raw, nm) >= 2
            if not ok:
                for c in _autoc_all(q_raw, regions=("TW","US","HK"), langs=("zh-TW","en-US")):
                    if (c.get("symbol", "").upper() == sym.upper()):
                        ok = True
                        break
            if ok:
                _symcache_put(q_raw, sym)
                _remember_resolved_symbol(q_raw, sym)
            return sym
        else:
            _symcache_put(q_raw, sym)
            _remember_resolved_symbol(q_raw, sym)
            return sym

    # C) åå–®å†·å‚™æ´ï¼ˆTWSE/TPEXï¼Œåªæœ‰ Yahoo å…¨ç·šå¤±æ•—æ‰ç”¨ï¼‰
    _load_tw_lists()
    by_name: Dict[str, set] = _TW_CACHE["by_name"]
    q = _normalize_name(q_raw)

    def _bias(s: str) -> float:
        return 2.0 if s.endswith(".TW") else (1.0 if s.endswith(".TWO") else 0.0)

    if q in by_name:
        for s in sorted(by_name[q], key=_bias, reverse=True):
            if _is_valid_symbol_cached(s, lookback_days=30):
                _symcache_put(q_raw, s); _remember_resolved_symbol(q_raw, s); return s

    prefix_pool, contain_pool = [], []
    for k, syms in by_name.items():
        if k == q:
            continue
        if k.startswith(q) or q.startswith(k):
            prefix_pool.extend(syms)
        elif q and (q in k or k in q):
            contain_pool.extend(syms)
    for pool in (prefix_pool, contain_pool):
        seen = set()
        for s in sorted((x for x in pool if not (x in seen or seen.add(x))), key=_bias, reverse=True):
            if _is_valid_symbol_cached(s, lookback_days=30):
                _symcache_put(q_raw, s); _remember_resolved_symbol(q_raw, s); return s

    subs = []
    for k, syms in by_name.items():
        if _ordered_subseq(q, k):
            subs.extend(syms)
    for s in sorted(set(subs), key=_bias, reverse=True):
        if _is_valid_symbol_cached(s, lookback_days=30):
            _symcache_put(q_raw, s); _remember_resolved_symbol(q_raw, s); return s

    return None

def resolve_symbol(
    t: str,
    *,
    fast_mode: bool = False,
    autoc_timeout: Optional[float] = None,
    autoc_max_langs: Optional[int] = None,
) -> Optional[str]:
    raw = (t or "").strip()
    if not raw:
        return None
    cache_key = f"symres:{raw.lower()}"
    cached = _cache_get(cache_key)
    if isinstance(cached, str) and cached:
        return cached

    s = raw
    timeout_val = autoc_timeout or (1.2 if fast_mode else 5.0)
    max_langs_val = autoc_max_langs if autoc_max_langs is not None else (2 if fast_mode else None)

    def _cache_and_return(sym: Optional[str]) -> Optional[str]:
        if sym:
            _remember_resolved_symbol(raw, sym)
            _cache_set(cache_key, sym, _RESOLVE_CACHE_TTL)
        return sym

    # å«ä¸­æ–‡ â†’ å…ˆèµ°ä¸­æ–‡åè§£æ
    if re.search(r"[\u4e00-\u9fff]", s):
        hit = resolve_symbol_by_name(
            s,
            fast_mode=fast_mode,
            autoc_timeout=autoc_timeout,
            autoc_max_langs=autoc_max_langs,
        )
        if hit:
            return _cache_and_return(hit)

    # ç´”ä»£ç¢¼æ¨£å¼
    if re.fullmatch(r"[A-Za-z.]+", s):
        ticker_candidate = s.upper()
        is_ticker_like = bool(re.fullmatch(r"[A-Z]{1,6}(?:\.[A-Z]{2,4})?", ticker_candidate))
        if is_ticker_like and fast_mode:
            return _cache_and_return(ticker_candidate)
        if is_ticker_like and not fast_mode:
            if is_valid_symbol(ticker_candidate):
                return _cache_and_return(ticker_candidate)
        autoc_query = ticker_candidate if is_ticker_like else s
        # fast_mode ä¸‹ç›´æ¥ä¾è³´ autocompleteï¼›æ­£å¸¸æ¨¡å¼è£œ chart é©—è­‰
        if is_ticker_like and not fast_mode:
            cands = yahoo_autoc_all(
                autoc_query,
                regions=("US","HK","TW"),
                langs=("en-US","zh-TW","zh-Hant-TW"),
                timeout=timeout_val,
                max_langs=max_langs_val,
            )
            for c in cands:
                sym = c.get("symbol")
                if sym and _is_valid_symbol_cached(sym, lookback_days=30):
                    return _cache_and_return(sym)
        # fast_mode æˆ–å‰é¢æœªå‘½ä¸­ â†’ ç›´æ¥ä¾è³´ Yahoo autocompleteï¼Œå¤šèªå¤šå€åŸŸ
        autoc_query = ticker_candidate if is_ticker_like else s
        cands = yahoo_autoc_all(
            autoc_query,
            regions=("US","HK","TW"),
            langs=("en-US","zh-TW","zh-Hant-TW"),
            timeout=timeout_val,
            max_langs=max_langs_val,
        )
        for c in cands:
            sym = c.get("symbol")
            if sym and _is_valid_symbol_cached(sym, lookback_days=30):
                return _cache_and_return(sym)
        # 3) æœ€å¾Œå†ç”¨ LLM çŒœæ¸¬å¹¾å€‹ ticker ä¸¦é©—è­‰
        if not fast_mode:
            for cand in guess_tickers_via_llm(s, max_n=6):
                if _is_valid_symbol_cached(cand, lookback_days=30):
                    return _cache_and_return(cand)
    # è‹¥ä»¥ä¸Šéƒ½æ²’ä¸­ï¼Œæ‰çœŸçš„å› Noneï¼ˆè½åˆ°å‡½å¼æœ€å¾Œçš„ return Noneï¼‰
    if re.fullmatch(r"\d{4}", s):
        for suf in (".TW",".TWO"):
            cand = s + suf
            if is_valid_symbol(cand):
                return _cache_and_return(cand)
        return None
    if re.fullmatch(r"\d{4}\.[A-Za-z]{2,4}", s):
        return _cache_and_return(s if is_valid_symbol(s) else None)

    # Autocomplete å‚™æ´ï¼ˆæ’åå™¨ï¼‰ï¼šçµ±ä¸€ä½¿ç”¨ yahoo_autoc_allï¼Œä¸¦å¥—ç”¨æ’åå™¨
    for regs in (("TW",), ("US","HK")):
        cands = yahoo_autoc_all(
            s,
            regions=regs,
            langs=("zh-TW","en-US"),
            timeout=timeout_val,
            max_langs=max_langs_val,
        )
        sym = pick_best_yahoo_candidate(s, cands)
        if sym:
            return _cache_and_return(sym)
    return None

def fetch_latest_close_via_chart(symbol: str, lookback_days: int = 7, timeout: Optional[float] = None) -> Optional[dict]:
    """
    ä»¥ Yahoo v8 chart åœ¨éå» lookback_days å¤©å…§å°‹æ‰¾æœ€è¿‘ä¸€ç­†ã€Œæ—¥ç·šæ”¶ç›¤ã€ã€‚
    ç”¨æ–¼ v7 quote æ“‹æ‰æ™‚çš„ fallbackã€‚
    """
    try:
        tz_tw = TZ_TW
        now = datetime.now(tz_tw)
        p1 = int((now - timedelta(days=lookback_days)).replace(hour=0, minute=0, second=0, microsecond=0).timestamp())
        p2 = int(now.timestamp())

        r = _req_get(
            f"https://query1.finance.yahoo.com/v8/finance/chart/{symbol}",
            params={"period1": p1, "period2": p2, "interval": "1d", "includePrePost": "false"},
            timeout=timeout or _REQ_TIMEOUT,
        )
        if not r:
            return None

        res = (r.json().get("chart", {}) or {}).get("result") or []
        if not res:
            return None
        node = res[0] or {}
        ts = node.get("timestamp") or []
        q0 = ((node.get("indicators") or {}).get("quote") or [{}])[0] or {}
        opens  = q0.get("open")  or []
        highs  = q0.get("high")  or []
        lows   = q0.get("low")   or []
        closes = q0.get("close") or []
        vols   = q0.get("volume") or []

        # å¾æœ€è¿‘å¾€å›æ‰¾ç¬¬ä¸€ç­†æœ‰ close çš„ K
        for i in range(len(ts) - 1, -1, -1):
            c = closes[i] if i < len(closes) else None
            if c is None:
                continue
            o = opens[i]  if i < len(opens)  else None
            h = highs[i]  if i < len(highs)  else None
            l = lows[i]   if i < len(lows)   else None
            v = vols[i]   if i < len(vols)   else None
            dstr = datetime.fromtimestamp(ts[i], tz_tw).date().isoformat()
            return {
                "symbol": symbol, "date": dstr,
                "open": o, "high": h, "low": l, "close": c,
                "volume": int(v) if v is not None else None
            }
        return None
    except Exception:
        return None

def fetch_stock_price_on_date(symbol: str, date_str: str) -> Optional[dict]:
    """ç”¨ Yahoo v8 chart å–å¾—ã€æŸæ—¥ã€æ—¥ç·šè³‡æ–™ï¼ˆæ‰¾ä¸åˆ°å°±å› Noneï¼‰ã€‚"""
    try:
        dt = dtparse(date_str)
        tz_tw = TZ_TW
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=tz_tw)
        p1 = int(dt.replace(hour=0, minute=0, second=0, microsecond=0).timestamp())
        p2 = int((dt + timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0).timestamp())
        r = _req_get(
            f"https://query1.finance.yahoo.com/v8/finance/chart/{symbol}",
            params={"period1": p1, "period2": p2, "interval": "1d", "includePrePost": "false"},
            headers={"User-Agent": "Mozilla/5.0"},
            timeout=6,
        )
        res = (r.json().get("chart", {}) if r else {}).get("result")
        if not res:
            return None
        q = (res[0] or {}).get("indicators", {}).get("quote") or []
        if not q or not (q[0] or {}):
            return None
        q0 = q[0]
        def first(a): return a[0] if isinstance(a, list) and a else None
        o, h, l, c, v = map(first, (q0.get("open"), q0.get("high"), q0.get("low"), q0.get("close"), q0.get("volume")))
        if None in (o, h, l, c): return None
        return {"symbol": symbol, "date": date_str, "open": o, "high": h, "low": l, "close": c, "volume": int(v) if v is not None else None}
    except Exception:
        return None

def _display_name_for_symbol(symbol: str) -> Optional[str]:
    """ç›¡é‡çµ¦å‡ºå¯è®€åç¨±ï¼šå…ˆå– Yahoo v7ï¼Œå°è‚¡ç”¨ TW åå–®è£œï¼›æœ€å¾Œç”¨ Yahoo Autocomplete å°ç…§ã€‚"""
    s = (symbol or "").upper()
    if not s:
        return None

    # (A) Yahoo v7 quote
    q = _yahoo_v7_quote(s) or {}
    for k in ("shortName", "longName", "displayName"):
        name = (q.get(k) or "").strip()
        if name:
            return name

    # (B) å°è‚¡ï¼šç”¨ TWSE/TPEX åå–®åæŸ¥
    if re.match(r"^\d{4}\.(TW|TWO)$", s):
        nm = _tw_find_name_for_code(s)
        if nm:
            return nm

    # (C) Yahoo Autocompleteï¼šç”¨ä»£è™Ÿæœ¬èº« & ç´”æ•¸å­—å†æ¯”ä¸€æ¬¡
    for cand in yahoo_autoc_all(
        s,
        regions=("TW","US","HK"),
        langs=("zh-TW","en-US"),
        timeout=1.2,
        max_langs=2,
    ):
        if (cand.get("symbol") or "").upper() == s:
            nm = (cand.get("name") or "").strip()
            if nm:
                return nm
    m = re.match(r"^(\d{4})\.(TW|TWO)$", s)
    if m:
        code_only = m.group(1)
        for cand in yahoo_autoc_all(
            code_only,
            regions=("TW",),
            langs=("zh-TW","en-US"),
            timeout=1.2,
            max_langs=2,
        ):
            if (cand.get("symbol") or "").upper() == s:
                nm = (cand.get("name") or "").strip()
                if nm:
                    return nm
    return None

def format_stock_reply(data: Optional[dict]) -> str:
    if not data:
        return "æŠ±æ­‰ï¼Œç„¡æ³•å–å¾—è©²è‚¡ç¥¨çš„æ—¥ç·šè³‡æ–™ã€‚"
    name = data.get("name") or _display_name_for_symbol(data["symbol"])
    head = f"{data['symbol']}ï¼ˆ{name}ï¼‰" if name else data['symbol']
    vol = data["volume"] if data["volume"] is not None else "â€”"
    return (
        f"{head} æ–¼ {data['date']}ï¼šé–‹ç›¤ {data['open']:.2f}ã€æœ€é«˜ {data['high']:.2f}ã€"
        f"æœ€ä½ {data['low']:.2f}ã€æ”¶ç›¤ {data['close']:.2f}ï¼Œæˆäº¤é‡ {vol} è‚¡ã€‚"
    )

def fetch_yahoo_stock_news(symbol: Optional[str] = None, max_results: int = 5, company_kw: Optional[str] = None) -> List[dict]:
    def _req(url: str) -> Optional[BeautifulSoup]:
        try:
            resp = _req_get(
                url,
                headers={"User-Agent":"Mozilla/5.0","Accept-Language":"zh-TW,zh;q=0.9,en;q=0.8"},
                timeout=3.0,
            )
            return BeautifulSoup(resp.text, "html.parser") if resp else None
        except Exception:
            return None

    def _norm(href: str) -> str:
        return href if (href or "").startswith("http") else ("https://tw.stock.yahoo.com" + (href or ""))

    def _is_article(href: str, title: str) -> bool:
        href = _norm(href or "")
        if not href or not title: return False
        if "/news/" not in href: return False
        if re.search(r"/news($|/)$", href): return False
        if re.search(r"/(mail|weather|quote)($|/)", href): return False
        return len(title.strip()) >= 6

    def _extract_news(soup: Optional[BeautifulSoup]) -> List[dict]:
        if soup is None: return []
        news, seen = [], set()
        selectors = [
            'ul.My\\(0\\).P\\(0\\).Wow\\(bw\\).Ov\\(h\\) li.js-stream-content h3 a',
            'li.js-stream-content h3 a',
            'section a[href*="/news/"]',
            'a[href*="/news/"]',
        ]
        for css in selectors:
            for a in soup.select(css):
                title = a.get_text(strip=True)
                href = a.get("href", "")
                if not _is_article(href, title): continue
                key = (_norm(href), title)
                if key in seen: continue
                seen.add(key)
                news.append({"title": title, "link": _norm(href), "source": "Yahooå¥‡æ‘©"})
                if len(news) >= max_results: return news
            if news: return news
        return news

    def _filter_by_kw(items: List[dict], kws: List[str]) -> List[dict]:
        if not kws: return items
        out = []
        for it in items:
            t = (it.get("title") or "").lower()
            if any(kw for kw in kws if kw and kw.lower() in t):
                out.append(it)
        return out or items

    if symbol:
        url = f"https://tw.stock.yahoo.com/quote/{symbol}/news"
        items = _extract_news(_req(url))
        if items:
            return items
        kws = [company_kw or "", symbol.split(".")[0]]
        items = _extract_news(_req("https://tw.stock.yahoo.com/news"))
        return _filter_by_kw(items, kws)[:max_results]

    items = _extract_news(_req("https://tw.stock.yahoo.com/news"))
    if company_kw:
        items = _filter_by_kw(items, [company_kw])
    return items[:max_results]

def format_news_markdown(news: List[dict]) -> str:
    def fmt_one(n: dict) -> str:
        title = n.get("title") or ""
        link = n.get("link") or "#"
        src = n.get("source") or "Yahooå¥‡æ‘©"
        t = n.get("time")
        suffix = f" â€” {src}" + (f"ï½œ{t}" if t else "")
        return f"- [{title}]({link}){suffix}"
    return "\n".join(fmt_one(x) for x in (news or [])) or "Yahoo æš«ç„¡ç›¸é—œæ–°èã€‚"

# ===== è‚¡ç¥¨ / æ–°è Markdown å€å¡Šçš„å…±ç”¨çµ„è£å™¨ =====
def _prepare_stock_md(
    user_q: str,
    intents: set,
    companies: list,
    *,
    symbol_cache: Optional[Dict[str, Optional[str]]] = None,
    quote_cache: Optional[Dict[str, Optional[List[dict]]]] = None,
    chart_cache: Optional[Dict[str, Optional[dict]]] = None,
) -> str:
    if "stock" not in intents:
        return ""

    targets = (companies or [])[:]
    if not targets:
        rx = re.findall(r"[A-Za-z]{1,6}(?:\.[A-Za-z]{2,4})?|\b\d{4}\b", user_q)
        targets = rx
    if not targets:
        # è®“ guess_companies å…ˆæŠŠå€™é¸è©ä¸Ÿå‡ºä¾†ï¼Œå¾Œé¢å†ç”¨ resolve_symbol() åšæœ€çµ‚åˆ¤æ–·
        targets = guess_companies_from_text(user_q, limit=3)
    # å…ˆåšåš´æ ¼æ¸…æ´—ï¼Œé¿å…æŠŠç–‘å•è©/æè¿°èªç•¶æˆå…¬å¸
    cleaned_targets = []
    seen_ct = set()
    for t in targets:
        ct = clean_company_token(t, original_text=user_q)
        if ct and ct not in seen_ct:
            cleaned_targets.append(ct)
            seen_ct.add(ct)
    targets = cleaned_targets
    if not targets:
        return "ï¼ˆæœªèƒ½è¾¨è­˜æœ‰æ•ˆè‚¡ç¥¨ä»£è™Ÿï¼Œè«‹æä¾›ä»£è™Ÿï¼Œä¾‹å¦‚ï¼š2330.TWã€TSLAã€AAPLï¼‰"

    # ç›®æ¨™å…¬å¸å»é‡ï¼ˆä¿åºï¼‰
    seen_targets = []
    for t in targets:
        if t not in seen_targets:
            seen_targets.append(t)

    symbols = []
    cache = symbol_cache if symbol_cache is not None else {}
    for comp in seen_targets[:6]:
        cache_key = (comp or "").strip().casefold()
        sym = cache.get(cache_key) if cache_key else None
        if sym is None and cache_key in cache:
            continue  # å·²çŸ¥å¤±æ•—ï¼Œä¸é‡è©¦
        if sym is None:
            sym = resolve_symbol(comp, fast_mode=True, autoc_timeout=1.0, autoc_max_langs=2)
        if not sym:
            sym = resolve_symbol(comp, fast_mode=False, autoc_timeout=3.0, autoc_max_langs=3)
        if symbol_cache is not None and cache_key:
            symbol_cache[cache_key] = sym
        if sym:
            _remember_resolved_symbol(comp, sym)
            symbols = [sym]  # åªä¿ç•™ç¬¬ä¸€å€‹è§£ææˆåŠŸçš„ä»£è™Ÿ
            break

    # ç¬¦è™Ÿå»é‡ï¼ˆä¿åºï¼›æ­¤æ™‚æœ€å¤š 1 å€‹ï¼‰
    uniq_syms = []
    seen_syms = set()
    for s in symbols:
        if s and s not in seen_syms:
            uniq_syms.append(s); seen_syms.add(s)
    symbols = uniq_syms

    # è‹¥å·²åŒ…å«å°è‚¡ 2330.TWï¼Œå°±ç§»é™¤é‡è¤‡çš„ç¾è‚¡ ADR TSMï¼ˆé¿å…ä¸å¿…è¦çš„ç¬¬äºŒå®¶å…¬å¸ï¼‰
    if "2330.TW" in symbols and "TSM" in symbols:
        symbols = [s for s in symbols if s != "TSM"]

    # âœ… æ²’æœ‰ä»»ä½•æœ‰æ•ˆä»£è™Ÿ â†’ æ˜ç¢ºå‘ŠçŸ¥ä½¿ç”¨è€…
    if not symbols:
        return "ï¼ˆæœªèƒ½è¾¨è­˜æœ‰æ•ˆè‚¡ç¥¨ä»£è™Ÿï¼Œè«‹æä¾›ä»£è™Ÿï¼Œä¾‹å¦‚ï¼š2330.TWã€TSLAã€AAPLï¼‰"

    # æœ‰æŒ‡å®šæ—¥æœŸ â†’ æŸ¥è©²æ—¥ã€Œæ—¥ç·šã€
    m = re.search(r"(20\d{2}-\d{2}-\d{2})", user_q)
    if m:
        date = m.group(1)
        lines = [format_stock_reply(fetch_stock_price_on_date(sym, date)) for sym in symbols]
    else:
        # æ²’æŒ‡å®šæ—¥æœŸ â†’ å…ˆæŠ“ã€Œå³æ™‚åƒ¹ã€ï¼ŒæŠ“ä¸åˆ°å†é€€å›æœ€è¿‘æ”¶ç›¤
        sym0 = symbols[0]
        cached_qts = None
        if quote_cache is not None:
            cached_qts = quote_cache.get(sym0)
        if cached_qts is None:
            cached_qts = fetch_realtime_quote_batch(symbols, timeout=2.0)
            if quote_cache is not None:
                quote_cache[sym0] = cached_qts
        qts = cached_qts
        if qts:
            lines = [format_realtime_quotes(qts)]
        else:
            lines = []
            for sym in symbols:
                cached_chart = None
                if chart_cache is not None:
                    cached_chart = chart_cache.get(sym)
                if cached_chart is None:
                    cached_chart = fetch_latest_close_via_chart(sym, lookback_days=7, timeout=4.0)
                    if chart_cache is not None:
                        chart_cache[sym] = cached_chart
                data = cached_chart
                lines.append(format_stock_reply(data))

    # è‹¥ä»ç„¡ä»»ä½•å¯ç”¨è³‡è¨Šï¼Œå›è¦†çµ±ä¸€è¨Šæ¯ï¼ˆé¿å…èª¤å°ï¼‰
    if not lines or not any((l or "").strip() for l in lines):
        return "ï¼ˆæŠ±æ­‰ï¼ŒæŸ¥ç„¡å¯ç”¨è‚¡åƒ¹è³‡æ–™ã€‚è«‹ç¢ºèªä»£è™Ÿæˆ–ç¨å¾Œå†è©¦ã€‚ï¼‰"

    return "\n".join(lines).strip()

def _prepare_news_md(
    intents: set,
    companies: list,
    user_q: str = "",
    *,
    symbol_cache: Optional[Dict[str, Optional[str]]] = None,
    news_cache: Optional[Dict[Tuple[Optional[str], Optional[str]], List[dict]]] = None,
) -> str:
    if "news" not in intents:
        return ""
    targets = (companies or [])[:]
    if not targets:
        targets = guess_companies_from_text(user_q, limit=1)
    sym = None
    cache = symbol_cache if symbol_cache is not None else {}
    if targets:
        tok = targets[0]
        cache_key = (tok or "").strip().casefold()
        sym = cache.get(cache_key) if cache_key else None
        if sym is None and cache_key in cache:
            sym = None
        else:
            if sym is None:
                sym = resolve_symbol(tok, fast_mode=True, autoc_timeout=1.0, autoc_max_langs=2)
            if not sym:
                sym = resolve_symbol(tok, fast_mode=False, autoc_timeout=3.0, autoc_max_langs=3)
            if symbol_cache is not None and cache_key:
                symbol_cache[cache_key] = sym
        if sym:
            _remember_resolved_symbol(tok, sym)
    kw_for_filter = (targets[0] if targets and re.search(r"[\u4e00-\u9fff]", targets[0]) else None)
    cache_key = (sym, (kw_for_filter or "").casefold() if kw_for_filter else None)
    cached_news = None
    if news_cache is not None:
        cached_news = news_cache.get(cache_key)
    if cached_news is None:
        cached_news = fetch_yahoo_stock_news(sym, max_results=5, company_kw=kw_for_filter)
        if news_cache is not None:
            news_cache[cache_key] = cached_news
    news = cached_news
    return format_news_markdown(news)


# ========== ä¸­æ–‡â†’è‹±æ–‡ç¿»è­¯ + è‹±æ–‡å¤šå•å¥ ==========

@lru_cache(maxsize=1024)
def zh2en_query(q: str) -> str:
    """
    å°‡ä¸­æ–‡é‡‘è/ç”¢æ¥­å•å¥ç¿»æˆè‹±æ–‡ï¼Œä¿ç•™ï¼šè‚¡ç¥¨ä»£è™Ÿã€æ•¸å­—ã€æ—¥æœŸæ ¼å¼ä¸è®Šã€‚
    åƒ…è¼¸å‡ºè‹±æ–‡ä¸€å¥ï¼Œç„¡æ¨™é»è£é£¾ã€ç„¡å¤šé¤˜èªªæ˜ã€‚
    """
    if not q or not is_zh(q):
        return q or ""
    prompt = (
        "Translate the following Chinese financial/industry query into precise English. "
        "Preserve stock tickers (e.g., 2330.TW, TSLA, AAPL), numbers, and dates as-is. "
        "Return ONLY the English sentence, no quotes or extra text.\n\n"
        f"Query: {q}"
    )
    try:
        txt = classifier_llm.invoke(prompt).content.strip()
        # æ¥µç°¡æ¸…æ´—ï¼šç§»é™¤åŒ…èµ·ä¾†çš„å¼•è™Ÿ
        txt = txt.strip().strip("ã€Œã€\"'")
        return txt
    except Exception:
        return q  # å¤±æ•—æ™‚ç›´æ¥å›åŸæ–‡ï¼Œå¾ŒçºŒæœƒæœ‰å‚™æ´

# =========================
# æª¢ç´¢ï¼ˆç©©å®šå»é‡ / å…ˆéæ¿¾å¾ŒåŠ æ¬Š / æŸ¥è©¢å»å™ª / å›é€€ç¯€æµï¼‰
# =========================
import hashlib
import math
from functools import lru_cache

def _stable_fp(text: str, md: dict) -> str:
    """è·¨ç¨‹åºç©©å®šçš„å…§å®¹æŒ‡ç´‹ï¼Œé¿å… Python å…§å»º hash çš„éš¨æ©ŸåŒ–ã€‚"""
    basis = (
        (text or "")[:1024]
        + "|"
        + str(md.get("doc_id") or md.get("doc_url") or md.get("url") or md.get("source") or md.get("filename") or "")
        + "|"
        + str(md.get("page") or extract_page_number(md) or -1)
    ).encode("utf-8", "ignore")
    return hashlib.sha1(basis).hexdigest()

def _rank_decay(i: int) -> float:
    """å›é€€æ™‚çš„åæ¬¡åˆ†æ•¸ï¼šå¹³æ»‘ã€èˆ‡ k ç„¡é—œã€‚"""
    return 1.0 / (1.0 + i)

def _dedup_norm_queries(qs: list[str]) -> list[str]:
    """æŸ¥è©¢å»å™ªå»é‡ï¼šå»ç©ºç™½ã€å…¨åŠå½¢/å¤§å°å¯«æ¨™æº–åŒ–ã€åˆªé‡ã€‚"""
    seen, out = set(), []
    for q in qs:
        if not q:
            continue
        t = str(q).strip()
        if not t:
            continue
        norm = t.casefold()
        if norm in seen:
            continue
        seen.add(norm)
        out.append(t)
    return out

def _precompute_company_needles(
    companies: List[str] | None,
    user_q: str,
    fetch_info: Optional[Callable[[str], Optional[Dict[str, Any]]]] = None,
) -> set[str]:
    """å…¬å¸éæ¿¾ç”¨çš„ needles é å…ˆç®—ä¸€æ¬¡ï¼Œé¿å…åœ¨ hit è¿´åœˆåè¦†è¨ˆç®—ã€‚"""
    nds = set()
    for tok in (companies or []):
        if fetch_info is not None:
            info = fetch_info(tok)
            if not info:
                continue
            nds.update(info.get("needles") or ())
            continue
        tok_clean = clean_company_token(tok, original_text=user_q)
        if not tok_clean:
            continue
        _sym, needles = canonicalize_company_needles(tok_clean)
        needles = set(needles)
        needles.add(tok_clean.lower())
        nds.update(needles)
    return nds

def _pass_company(md: dict, text: str, needles: set[str]) -> bool:
    return True if not needles else doc_mentions_any_company(md, text or "", needles)

def _year_weight(md: dict, text: str, targets: set[int], strict: bool) -> float | None:
    """å¹´ä»½æ¬Šé‡ï¼šç›¡é‡å°‘ç”¨ç¡¬ç ï¼›strict æ™‚æ‰ä¸Ÿæ‰ä¸ç›¸é„°çš„å¹´ä»½ã€‚"""
    y = detect_doc_report_year_from_md_or_text(md, text or "")
    if y in targets:
        return 1.35
    if (y is not None) and any(abs(y - t) == 1 for t in targets):
        return 0.85
    return (0.0 if strict else 0.55)

def _apply_bucket_weight(score: float, md: dict, type_scales: dict[str, float] | None, epsilon: float, prefilter: bool) -> float | None:
    if type_scales is None:
        return score
    w = float(type_scales.get(get_bucket(md), 1.0))
    if prefilter and w <= 0.0:
        return None
    return score * max(0.0, (epsilon + w))

def _search_with_scores(vs, q: str, k: int):
    # 1) å„ªå…ˆä½¿ç”¨å¸¶åˆ†æ•¸ API
    try:
        return vs.similarity_search_with_relevance_scores(q, k=k)
    except Exception:
        pass
    # 2) å›é€€ï¼šç„¡åˆ†æ•¸æ™‚çµ¦åæ¬¡è¡°æ¸›
    try:
        docs = vs.similarity_search(q, k=k)
        return [(d, _rank_decay(i)) for i, d in enumerate(docs)]
    except Exception:
        return []

@lru_cache(maxsize=512)
def expand_aliases_via_llm(user_q: str, max_terms: int = 12) -> dict:
    """
    è®“ LLM åšè¡“èªåˆ¤åˆ¥èˆ‡ä¸­è‹±åˆ¥åæ“´å±•ã€‚
    åªå› JSONï¼š{topic, zh_aliases, en_aliases, metrics}
    """
    prompt = f"""
    You are a finance term alias expander. Return ONLY valid JSON with keys:
    {{
    "topic": <canonical English term or null>,
    "zh_aliases": [ ... up to {max_terms} ... ],
    "en_aliases": [ ... up to {max_terms} ... ],
    "metrics": [ ... up to 6 ... ]
    }}
    Rules:
    - Include widely used aliases, abbreviations, and the official report/page names if any.
    - Prefer high-precision terms used in reputable sources (e.g., US government statistical releases).
    - Do NOT invent unknown terms. Keep items concise (<= 5 words).
    - Keep language of zh_aliases in Traditional Chinese; en_aliases in English.
    - Examples:
    - Input: "å¯ä»¥çµ¦æˆ‘æœ€æ–°çš„éè¾²æ•¸æ“šå—ï¼Ÿ"
        Output concept includes: topic ~ "nonfarm payrolls",
        zh_aliases contains ["éè¾²", "éè¾²å°±æ¥­", "å°±æ¥­å ±å‘Š"],
        en_aliases contains ["nonfarm payrolls", "NFP", "Employment Situation", "jobs report", "BLS"],
        metrics may contain ["unemployment rate", "average hourly earnings"].

    Input: {user_q}
    """
    try:
        raw = classifier_llm.invoke(prompt).content.strip()
        obj = extract_json_from_text(raw) or {}
        def _clean_list(xs):
            out, seen = [], set()
            for s in xs or []:
                if isinstance(s, str):
                    t = s.strip().strip("ã€Œã€\"'()[]")
                    if 1 <= len(t) <= 40 and t.lower() not in seen:
                        out.append(t); seen.add(t.lower())
            return out[:max_terms]
        return {
            "topic": (obj.get("topic") if isinstance(obj.get("topic"), str) else None),
            "zh_aliases": _clean_list(obj.get("zh_aliases")),
            "en_aliases": _clean_list(obj.get("en_aliases")),
            "metrics": _clean_list(obj.get("metrics")),
        }
    except Exception:
        return {"topic": None, "zh_aliases": [], "en_aliases": [], "metrics": []}

def _merge_cap(a: list[str], b: list[str], cap: int) -> list[str]:
    seen, out = set(), []
    for s in [*a, *b]:
        if s and s not in seen:
            out.append(s); seen.add(s)
        if len(out) >= cap: break
    return out

def is_earnings_call_query(text: str) -> bool:
    t = (text or "").lower()
    keys = ["æ³•èªª", "æ³•èªªæœƒ", "é›»è©±æœƒè­°", "earnings call", "conference call", "prepared remarks", "transcript"]
    return any(k in t for k in keys)

def similarity_search_vectors(
    user_q: str,
    k: int = 12,
    type_scales: Dict[str, float] | None = None,
    prefilter: bool = True,
    epsilon: float = 0.0,
    *,
    intents: set | None = None,
    year_targets: set[int] | None = None,
    strict_year: bool = True,
    companies: List[str] | None = None,
    lap: Optional[Callable[[str], None]] = None,   # â† æ–°å¢ï¼šå¤–éƒ¨å¯å‚³å…¥è¨ˆæ™‚å™¨ï¼ˆä¾‹å¦‚ handle_question çš„ _lapï¼‰
):
    """
    ä¾èªè¨€æŠŠ query åˆ†æµï¼Œä¸¦å¼·åŒ–ã€Œå…¬å¸ï¼‹æ³•èªªæœƒ/è²¡å ±ã€ç¨®å­ã€‚
    å›å‚³ï¼š(docs, scores, zh_queries_used, en_queries_used)
    """
    import re, math

    def _lap(name: str):
        if lap:
            try:
                lap(name)
            except Exception:
                pass

    _lap("retrieval:start")

    # --- ä¸­æ–‡åº« queries (base) ---
    zh_seed = (user_q or "").strip()
    zh_queries = [zh_seed] if zh_seed else []

    # --- è‹±æ–‡åº« queries (base) ---
    if is_zh(user_q):
        base_en = (zh2en_query(user_q) or "").strip()
        en_queries = [base_en] if base_en else []
    else:
        en_seed = zh_seed
        en_queries = [en_seed] if en_seed else []
    _lap("retrieval:mk_base_queries")

    # å…ˆæº–å‚™ pin å®¹å™¨ï¼ˆä¹‹å¾Œæœƒä»¥è¼ƒå¤§ cap åˆä½µï¼Œé¿å…è¢«è£æ‰ï¼‰
    zh_pins: List[str] = []
    en_pins: List[str] = []

    # === è²¡å ±æ„åœ–ï¼šç°¡å–®è£œå¼·é—œéµè© ===
    if intents and ("financial_report" in intents):
        yrs = sorted(year_targets) if year_targets else []
        ticker_candidate = None
        for c in (companies or []):
            c_clean = (c or "").strip()
            if not c_clean:
                continue
            if re.fullmatch(r"[A-Za-z]{1,6}(?:\.[A-Za-z]{2,4})?", c_clean):
                ticker_candidate = c_clean.upper()
                break
        if ticker_candidate:
            zh_pins += [
                f"{ticker_candidate} è²¡å ±",
                f"{ticker_candidate} å­£å ±",
                f"{ticker_candidate} å¹´å ±",
            ]
            en_pins += [
                f"{ticker_candidate} financial report",
                f"{ticker_candidate} earnings report",
                f"{ticker_candidate} filing",
            ]
        metrics_keywords = ["revenue", "eps", "gross margin", "guidance"]
        for m in metrics_keywords:
            if ticker_candidate:
                en_pins.append(f"{ticker_candidate} {m}")
            zh_pins.append(f"{m} è²¡å ±")
        for y in yrs:
            zh_pins += [f"{y} å¹´å ±", f"FY{y} è²¡å ±", f"{y} å¹´ 10-K"]
            en_pins += [f"FY{y} report", f"{y} annual report", f"{y} 10-K"]

    company_info_cache: Dict[str, Dict[str, Any]] = {}
    company_cache_lock = threading.Lock()
    COMPANY_RESOLVE_BUDGET = 3.0  # ç§’ï¼Œæ•´é«”å…¬å¸è§£æçš„æ™‚é–“ä¸Šé™
    company_resolve_start = time.perf_counter()
    needs_precise_symbols = bool((intents or set()) & {"stock", "financial_report", "news"})

    def _make_company_info(token: str, sym: Optional[str], needles: Iterable[str], source: str) -> Dict[str, Any]:
        base = (token or "").strip()
        needles_set = set(n.lower() for n in needles if n)
        if base:
            needles_set.add(base.lower())
        if sym:
            needles_set.add(sym.lower())
            digits = re.sub(r"\D", "", sym)
            if digits:
                needles_set.add(digits)
        return {
            "token": base,
            "sym": sym,
            "needles": frozenset(n for n in needles_set if n),
            "source": source,
        }

    def _heuristic_company_info(tok: str, tok_clean: Optional[str]) -> Optional[Dict[str, Any]]:
        basis = (tok_clean or tok or "").strip()
        if not basis:
            return None
        up = basis.upper()

        # 1) æœ€è¿‘è§£æå¿«å–
        recent = _lookup_recent_symbol(basis) or (
            _lookup_recent_symbol(tok_clean) if tok_clean and tok_clean != basis else None
        )
        if recent:
            return _make_company_info(basis, recent, [basis, recent], source="recent")

        # 2) ç¬¦è™Ÿå¿«å–å‘½ä¸­ â†’ ç›´æ¥å›å‚³
        hit = _symcache_get(basis) or (_symcache_get(tok_clean) if tok_clean and tok_clean != basis else None)
        if hit:
            return _make_company_info(basis, hit, [basis, hit], source="cache")

        # 3) å°è‚¡å…¬é–‹åå–®å…ˆè¡ŒåŒ¹é…ï¼ˆè‹¥å¿«å–å·²å°±ç·’ï¼‰
        tw_ready = _is_tw_cache_fresh()
        if not tw_ready:
            _warm_tw_lists_async()
        by_name = _TW_CACHE.get("by_name") if tw_ready else {}
        by_code = _TW_CACHE.get("by_code") if tw_ready else {}
        if by_name and by_code:
            norm_basis = _normalize_name(basis)
            tw_syms = list(by_name.get(norm_basis, []))
            if not tw_syms and tok_clean and tok_clean != basis:
                tw_syms = list(by_name.get(_normalize_name(tok_clean), []))
            if tw_syms:
                sym_pick = tw_syms[0]
                needles = [basis, tok_clean or basis, sym_pick, sym_pick.split(".")[0]]
                return _make_company_info(basis, sym_pick, needles, source="twlist")
            sym_from_code = None
            if up in by_code:
                sym_from_code = up
            elif up + ".TW" in by_code:
                sym_from_code = up + ".TW"
            elif up + ".TWO" in by_code:
                sym_from_code = up + ".TWO"
            if sym_from_code:
                needles = [basis, tok_clean or basis, sym_from_code, sym_from_code.split(".")[0]]
                return _make_company_info(basis, sym_from_code, needles, source="twlist")

        # 4) ç›´æ¥åˆ¤æ–·æ˜¯å¦ç‚º ticker / ä»£ç¢¼ï¼Œé¿å…ç«‹å³æ‰“å¤–éƒ¨ API
        if re.fullmatch(r"[A-Za-z]{1,6}(?:\.[A-Za-z]{2,4})?", up):
            return _make_company_info(basis, up, [up], source="fast")
        if re.fullmatch(r"\d{4}(?:\.[A-Za-z]{2,4})?", up):
            if "." in up:
                return _make_company_info(basis, up, [up, up.split(".")[0]], source="fast")
            tw_sym = up + ".TW"
            needles = [tw_sym, up + ".TWO", up]
            return _make_company_info(basis, tw_sym, needles, source="fast")

        # 5) fallbackï¼šè‡³å°‘ä¿ç•™åŸå­—ä¸²ä½œç‚º needle
        return _make_company_info(basis, None, [basis], source="fast")

    def _canonicalize_company_info(basis: str, seed: Optional[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        sym, needles = canonicalize_company_needles(
            basis,
            fast_mode=True,
            autoc_timeout=1.0,
            autoc_max_langs=2,
        )
        merged_needles: Set[str] = set(needles or [])
        if seed:
            merged_needles.update(seed.get("needles") or [])
        merged_needles.add(basis.lower())
        sym_final = sym or ((seed or {}).get("sym"))
        if sym_final:
            _remember_resolved_symbol(basis, sym_final)
        return _make_company_info((seed or {}).get("token") or basis, sym_final, merged_needles, source="slow")

    def _get_company_info(raw_token: str, allow_slow: bool = True) -> Optional[Dict[str, Any]]:
        tok = (raw_token or "").strip()
        if not tok:
            return None
        tok_clean = clean_company_token(tok, original_text=user_q)
        key = tok_clean or tok
        with company_cache_lock:
            cached = company_info_cache.get(key)
        if cached:
            return cached

        info_fast = _heuristic_company_info(tok, tok_clean)
        if info_fast:
            with company_cache_lock:
                company_info_cache[key] = info_fast
        else:
            info_fast = _make_company_info(tok_clean or tok, None, [tok_clean or tok], source="fast")
            with company_cache_lock:
                company_info_cache[key] = info_fast

        if info_fast.get("sym") or not allow_slow:
            return info_fast

        if (time.perf_counter() - company_resolve_start) > COMPANY_RESOLVE_BUDGET:
            return info_fast

        basis = (tok_clean or tok or "").strip()
        if not basis:
            return info_fast
        info_slow = _canonicalize_company_info(basis, info_fast)
        if info_slow:
            with company_cache_lock:
                company_info_cache[key] = info_slow
            return info_slow
        return info_fast

    company_tokens = [c for c in (companies or []) if str(c).strip()]
    allow_slow_companies = needs_precise_symbols
    company_infos_by_token: Dict[str, Optional[Dict[str, Any]]] = {}
    if company_tokens:
        _warm_tw_lists_async()
        if len(company_tokens) > 1:
            max_workers = min(4, len(company_tokens))
            with ThreadPoolExecutor(max_workers=max_workers) as pool:
                futures = {
                    pool.submit(_get_company_info, tok, allow_slow_companies): (idx, tok)
                    for idx, tok in enumerate(company_tokens)
                }
                interim: Dict[int, Optional[Dict[str, Any]]] = {}
                for fut in as_completed(futures):
                    idx, tok = futures[fut]
                    try:
                        interim[idx] = fut.result()
                    except Exception:
                        interim[idx] = None
                for idx, tok in enumerate(company_tokens):
                    company_infos_by_token[tok] = interim.get(idx)
        else:
            for tok in company_tokens:
                company_infos_by_token.setdefault(
                    tok,
                    _get_company_info(tok, allow_slow=allow_slow_companies),
                )

    # ====== æ³•èªªæœƒé¡Œï¼šå¼·åŒ–æŸ¥è©¢ï¼ˆå…¬å¸åï¼‹ä»£ç¢¼ï¼‰ ======
    ec = is_earnings_call_query(user_q)
    if ec:
        targets = [c for c in (companies or [])[:1] if str(c).strip()]
        for tok in targets:
            info = company_infos_by_token.get(tok)
            if info is None:
                info = _get_company_info(tok, allow_slow=allow_slow_companies)
                company_infos_by_token[tok] = info
            sym = (info or {}).get("sym") or ""
            if sym:
                _remember_resolved_symbol(tok, sym)
            num = re.sub(r"\D", "", sym) if sym else ""  # ä¾‹å¦‚ 3416
            # ä¸­æ–‡ pins
            zh_pins += [
                f"{tok} æ³•èªªæœƒ é‡é»",
                f"{tok} æ³•èªª é€å­—ç¨¿ æ‘˜è¦",
                f"{tok} æ³•èªªæœƒ æ‘˜è¦",
                f"{tok} æ³•èªª Q&A é‡é»",
            ]
            # è‹±æ–‡ pins
            en_pins += [
                f"{tok} earnings call transcript highlights",
                f"{tok} earnings call prepared remarks summary",
                f"{tok} conference call Q&A key points",
                f"{tok} earnings transcript summary",
            ]
            if num:
                zh_pins += [f"{num} æ³•èªªæœƒ æ‘˜è¦", f"{num} æ³•èªªæœƒ é‡é»"]
                en_pins += [f"{num} earnings call transcript"]

    # === ä¾å…¬å¸åŠ å¼·ï¼ˆæŠŠå…¬å¸ alias/ticker ç›´æ¥ç•¶æŸ¥è©¢è©ï¼‰ ===
    comp_needles: Set[str] = set()
    resolved_syms: List[str] = []
    for tok in company_tokens:
        info = company_infos_by_token.get(tok)
        if info is None and allow_slow_companies:
            info = _get_company_info(tok, allow_slow=allow_slow_companies)
            company_infos_by_token[tok] = info
        if not info:
            continue
        comp_needles.update(info.get("needles") or ())
        sym = info.get("sym")
        if sym:
            resolved_syms.append(sym)
            _remember_resolved_symbol(tok, sym)

    if resolved_syms:
        resolved_syms = list(dict.fromkeys(resolved_syms))

    if resolved_syms or comp_needles:
        boost_terms = list(sorted(comp_needles))[:6]
        # é€™äº›ä¹Ÿè¦–ç‚º pinsï¼ˆé¿å…è¢«æˆªæ–·ï¼‰
        zh_pins = boost_terms + zh_pins
        # è‹±æ–‡ pins åªç•™å«è‹±æ•¸çš„
        en_pins = list(resolved_syms) + [t for t in boost_terms if re.search(r"[A-Za-z0-9]", t)] + en_pins

    # === å¹´ä»½ pinsï¼ˆè²¡å ±é¡Œï¼‰ ===
    if intents and ("financial_report" in intents) and year_targets:
        ylist = sorted(year_targets)
        for y in ylist:
            zh_pins += [f"{y} å¹´å ±", f"{y} å¹´åº¦è²¡å ±", f"FY{y} è²¡å ±", f"{y} å¹´ EPS æ¯›åˆ©ç‡ ç‡Ÿæ”¶"]
            en_pins += [f"{y} annual report", f"FY{y} earnings", f"FY{y} results", f"{y} 10-K"]

    # === å…ˆåˆä½µ pinsï¼Œå†åˆä½µ aliasï¼›cap æ”¾å¯¬ä½†æœƒåœ¨æœ€å¾Œå»å™ªå»é‡ ===
    BIG_CAP = max(12, k + 6)
    zh_queries = _merge_cap(zh_pins, zh_queries, cap=BIG_CAP)
    en_queries = _merge_cap(en_pins, en_queries, cap=BIG_CAP)
    _lap("retrieval:merged_pins_base")

    # === åˆ¥åï¼åŒç¾©è©æ“´å±•ï¼ˆè£œ recallï¼Œä¸å£“å‰é¢çš„ pinsï¼‰ ===
    _lap("retrieval:before_alias_expand")
    aliases = expand_aliases_via_llm(user_q, max_terms=12)
    _lap("retrieval:after_alias_expand")

    # è‹¥æœ‰å¹´ä»½æ˜ç¢ºæ„åœ–ï¼Œå°±é—œé–‰ã€Œæœ€æ–°/latestã€æ¨¡æ¿ï¼Œé¿å…åç½®
    def _mk_zh(seed: str) -> list[str]:
        if year_targets:
            return [f"{seed} å…¬å¸ƒ", f"{seed} é‡é»"]
        return [f"æœ€æ–°{seed}", f"{seed} æœ€æ–°ä¸€æœŸ", f"{seed} å…¬å¸ƒ", f"{seed} é‡é»"]

    def _mk_en(seed: str) -> list[str]:
        if year_targets:
            return [f"{seed} report", f"{seed} highlights", f"{seed} update"]
        return [f"latest {seed}", f"{seed} latest report", f"most recent {seed} highlights", f"{seed} update"]

    zh_alias_qs: List[str] = []
    for z in aliases["zh_aliases"]:
        zh_alias_qs += _mk_zh(z)
    for m in aliases["metrics"]:
        zh_alias_qs += _mk_zh(m)

    en_alias_qs: List[str] = []
    for e in aliases["en_aliases"]:
        en_alias_qs += _mk_en(e)
    for m in aliases["metrics"]:
        en_alias_qs += _mk_en(m)

    zh_queries = _merge_cap(zh_queries, zh_alias_qs, cap=max(BIG_CAP, k + 12))
    en_queries = _merge_cap(en_queries, en_alias_qs, cap=max(BIG_CAP, k + 12))

    # æŸ¥è©¢å»å™ªå»é‡ï¼ˆéå¸¸é‡è¦ï¼Œé¿å…ç™¼å¤ªå¤šå†—é¤˜ RPCï¼‰
    zh_queries = _dedup_norm_queries(zh_queries)
    en_queries = _dedup_norm_queries(en_queries)
    _lap("retrieval:queries_ready")

    # â€”â€” åˆ†æ•¸æ¨™æº–åŒ–å·¥å…· â€”â€” 
    def _zscore_norm(scores: list[float]) -> list[float]:
        if not scores:
            return scores
        n = len(scores)
        mu = sum(scores) / n
        var = sum((s - mu)**2 for s in scores) / max(1, (n - 1))
        if var <= 1e-12:
            return [0.0] * n
        std = var ** 0.5
        # å°‡ z åˆ†æ•¸å£“å› 0~1ï¼ˆsigmoidï¼‰ï¼Œé¿å…è² å€¼é›£ä»¥ç›´è§€æ··åˆ†
        return [1.0 / (1.0 + math.exp(- (s - mu) / std)) for s in scores]

    # â€”â€” æ”¶é›† hitsï¼ˆä¿ç•™ query/é †ä½ä»¥ä¾¿é™¤éŒ¯æˆ–åšå¤šæ¨£æ€§ï¼‰ â€”â€”
    def _collect_hits(vs, queries: list[str], k: int):
        """
        å›å‚³ [(doc, raw_score, q_index, rank_in_q), ...]
        raw_score å¯èƒ½æ˜¯ relevance åˆ†æ•¸ï¼Œæˆ–å›é€€åæ¬¡åˆ†æ•¸
        """
        out = []
        for qi, q in enumerate(queries):
            hits = _search_with_scores(vs, q, k)
            for ri, (d, sc) in enumerate(hits):
                out.append((d, float(sc), qi, ri))
        return out

    # ===== æª¢ç´¢ï¼ˆåŠ å…¥ä¸­/è‹±åº«å…§éƒ¨æ¨™æº–åŒ–ï¼‰=====
    pool, seen = [], set()
    company_needles = _precompute_company_needles(companies, user_q, fetch_info=_get_company_info)

    # 1) å…ˆå„è‡ªæ”¶é›† hits
    _lap("retrieval:zh_search_start")
    zh_hits = _collect_hits(vectorstore_zh, zh_queries, k)  # [(d, raw_s, qi, ri), ...]
    _lap("retrieval:zh_search_done")

    _lap("retrieval:en_search_start")
    en_hits = _collect_hits(vectorstore_en, en_queries, k)
    _lap("retrieval:en_search_done")

    # 2) å„åº«å…§éƒ¨æ¨™æº–åŒ–ï¼Œé¿å…æ¨™åº¦ä¸ä¸€è‡´
    _lap("retrieval:score_norm_start")
    zh_scores_norm = _zscore_norm([h[1] for h in zh_hits])
    en_scores_norm = _zscore_norm([h[1] for h in en_hits])
    _lap("retrieval:score_norm_done")

    # 3) å°è£ä¸€å€‹å°‡ï¼ˆå‘½ä¸­æ–‡ä»¶, æ¨™æº–åŒ–åˆ†ï¼‰æ¨å…¥ pool çš„æ­¥é©Ÿï¼ˆå…ˆéæ¿¾ã€å¾ŒåŠ æ¬Šï¼‰
    def _push_after_filter(d, base_score: float):
        md = d.metadata or {}
        did  = md.get("doc_id") or md.get("doc_url") or md.get("url") or md.get("source") or md.get("filename") or ""
        page = extract_page_number(md) or -1
        fp   = _stable_fp(d.page_content or "", md)
        key  = (did, page, fp)
        if key in seen:
            return

        # å…¬å¸éæ¿¾ï¼ˆå« transcript æ„åœ–ï¼‰
        COMPANY_FILTER_ON = bool(company_needles) and (intents and ({"financial_report", "stock", "transcript"} & intents))
        if COMPANY_FILTER_ON and not _pass_company(md, d.page_content or "", company_needles):
            return

        score = float(base_score)

        # å¹´ä»½æ¬Šé‡ï¼ˆè²¡å ±é¡Œï¼‰
        if intents and ("financial_report" in (intents or set())) and year_targets:
            yw = _year_weight(md, d.page_content or "", year_targets, strict_year)
            if yw == 0.0:
                return
            score *= yw

        # é¡åˆ¥å€ç‡ / é éæ¿¾
        score2 = _apply_bucket_weight(score, md, type_scales, epsilon, prefilter)
        if score2 is None:
            return

        seen.add(key)
        pool.append((d, float(score2)))

    # 4) å°‡ä¸­/è‹±åº«æ¨™æº–åŒ–å¾Œçš„åˆ†æ•¸å¥—éæ¿¾èˆ‡åŠ æ¬Šé‚è¼¯æ¨å…¥ pool
    _lap("retrieval:pooling_start")
    for (d, _raw, _qi, _ri), s in zip(zh_hits, zh_scores_norm):
        _push_after_filter(d, s)
    for (d, _raw, _qi, _ri), s in zip(en_hits, en_scores_norm):
        _push_after_filter(d, s)
    _lap("retrieval:pooling_done")

    # 5) è‹¥å®Œå…¨å¬å›ç‚ºç©ºï¼šmetadata å›é€€ï¼ˆé™é¡ï¼‰
    if not pool and ec and companies:
        _lap("retrieval:metadata_fallback_start")
        try:
            needles = {c.lower() for c in companies}
            more, cap, cnt = [], 50, 0
            for d in getattr(vectorstore_zh, "docstore", {})._dict.values():
                md = d.metadata or {}
                if get_bucket(md) != "transcript":
                    continue
                hay = " ".join([
                    md.get("title", ""),
                    md.get("filename", ""),
                    md.get("doc_id", ""),
                    md.get("document_title", ""),
                ]).lower()
                if any(n in hay for n in needles):
                    more.append((d, 0.51))
                    cnt += 1
                    if cnt >= cap:
                        break
            pool.extend(more)
        except Exception:
            pass
        _lap("retrieval:metadata_fallback_done")

    # 6) æ’åº & å›å‚³
    if not pool:
        _lap("retrieval:done_empty")
        return ([], [], zh_queries, en_queries)

    pool.sort(key=lambda x: x[1], reverse=True)
    docs, scores = zip(*pool)
    _lap("retrieval:done_ok")
    return (list(docs), list(scores), zh_queries, en_queries)

# =========================
# LLM Rerank
# =========================
rerank_prompt = PromptTemplate.from_template(
"""
ä½ æ˜¯é‡‘è/ç”¢æ¥­æª¢ç´¢çš„ã€Œæ®µè½æ’åºå™¨ã€ã€‚è«‹æ ¹æ“šã€å•é¡Œã€‘è©•ä¼°ä¸‹åˆ—ã€æ®µè½ã€‘å°å›ç­”å•é¡Œçš„ã€Œèªæ„ç›¸é—œæ€§ã€ï¼Œä¸¦è¼¸å‡º**ç”±é«˜åˆ°ä½**çš„æ’åºã€‚

ã€è©•åˆ†æº–å‰‡ï¼ˆç”±å¼·åˆ°å¼±ï¼ŒåŠ åˆ†ï¼‰ã€‘
1) ç²¾æº–åŒ¹é…ï¼šåŒ…å«å•é¡Œä¸­çš„ã€Œå…¬å¸/ä»£è™Ÿ/åˆ¥å/ç”¢å“ç·šã€æˆ–å…¶å¸¸è¦‹ç¸®å¯«ã€Tickerï¼ˆä¾‹å¦‚ï¼š2330.TWã€TSMCã€å°ç©é›»ï¼‰ã€‚
2) æœŸé–“åŒ¹é…ï¼šå•é¡Œè‹¥æåˆ°å¹´ä»½/å­£åº¦/æœˆä»½/ã€Œæœ€æ–°/æœ€è¿‘/è¿‘æ³ã€ï¼Œæ®µè½è‹¥å‡ºç¾ç›¸åŒæœŸé–“æˆ–æ›´æ¥è¿‘çš„æœŸé–“ï¼Œæ‡‰å„ªå…ˆã€‚
3) æŒ‡æ¨™åŒ¹é…ï¼šå•é¡Œè‹¥æ¶‰åŠè²¡å ±/ç‡Ÿæ”¶(Revenue/Net sales)/æ¯›åˆ©ç‡(Gross margin)/EPS/Guidance/Outlook/Segment/Regionï¼Œå„ªå…ˆå«é€™äº›é—œéµè©èˆ‡**æ•¸å­—**çš„æ®µè½ã€‚
4) æ–‡æª”é¡å‹åå¥½ï¼šè‹¥å‡ºç¾ã€Œæ³•èªª/earnings call/conference call/transcriptã€ï¼Œå„ªå…ˆé€å­—ç¨¿/æº–å‚™ç¨¿/Q&Aï¼›è‹¥æ˜¯è²¡å ±å•é¡Œï¼Œå„ªå…ˆ 10-K/10-Q/20-F/6-K ç­‰åŸå§‹æª”ã€‚
5) å…·é«”æ€§ï¼šåŒ…å«æ˜ç¢ºæ•¸å­—ã€è¡¨æ ¼æ•˜è¿°ã€çµè«–æ€§å¥å­çš„æ®µè½ > åƒ…æ¦‚å¿µæ€§æè¿°ã€‚

ã€æ‰£åˆ†/æ’é™¤ã€‘
- æ³•å‹™/å…è²¬è²æ˜ã€ç›®éŒ„ã€å°èˆªã€å°é¢/é å°¾ã€é‡è¤‡ boilerplateã€‚
- èˆ‡å•é¡Œå…¬å¸æˆ–ä¸»é¡Œç„¡é—œè€…ã€‚
- åªæœ‰è¡ŒéŠ·å£è™Ÿã€ç„¡ä»»ä½•æŒ‡æ¨™æˆ–äº‹å¯¦ã€‚

ã€è¼¸å‡ºæ ¼å¼ï¼ˆåš´æ ¼ï¼‰ã€‘
- åªè¼¸å‡º**ä¸€è¡Œ**ã€Œé€—è™Ÿåˆ†éš”çš„æ•´æ•¸æ’åºã€ï¼Œæ¶µè“‹å…¨éƒ¨æ®µè½ä¸”æ¯å€‹æ•´æ•¸å‡ºç¾ä¸€æ¬¡ï¼Œä¾‹å¦‚ï¼š`2,1,4,3`
- ä¸è¦ä»»ä½•æ–‡å­—èªªæ˜ã€ä¸è¦ç©ºç™½è¡Œã€‚

å•é¡Œï¼š
{question}

æ®µè½ï¼š
{contexts}
"""
)

def llm_rerank(question: str, docs: List[Any], cos_scores: List[float], candidate_cap: int = 8, clip_chars: int = 900) -> List[float]:
    if not docs: return []
    order = np.argsort(np.asarray(cos_scores, dtype=float))[::-1]
    keep = [int(i) for i in order[:min(candidate_cap, len(docs))]]

    def clip(s: str, n: int) -> str:
        s = (s or "").strip()
        return s if len(s) <= n else (s[:n] + " â€¦")

    contexts = "\n\n".join(
        f"æ®µè½ {j+1}: {clip(docs[i].page_content, clip_chars)}" for j, i in enumerate(keep)
    )
    try:
        resp = rerank_llm.invoke(rerank_prompt.format(question=question, contexts=contexts)).content.strip()
        idxs = [int(x)-1 for x in re.findall(r"\d+", resp)]
        chosen_global = [keep[i] for i in idxs if 0 <= i < len(keep)]
        scores = [0.0] * len(docs)
        for rank, g in enumerate(chosen_global):
            scores[g] = len(docs) - rank
        return scores
    except Exception:
        return [0.0]*len(docs)

# =========================
# åå¥½å€ç‡æ‡‰ç”¨ï¼ˆå”¯ä¸€æ¬Šé‡ä¾†æºï¼‰
# =========================
def hybrid_sort(docs: List[Any], cos_scores: List[float], llm_scores: List[float],
                top_k: int, type_scales: Dict[str, float],
                recency_boost: bool = False, recency_half_life: int = 45) -> Tuple[List[Any], List[float], str]:
    if not docs: return [], [], "no docs"

    def norm(arr_in: np.ndarray) -> np.ndarray:
        a = np.asarray(arr_in, dtype=float)
        a = np.atleast_1d(a)
        if a.size == 0:
            return np.zeros(0, dtype=float)
        return (a - a.min())/(a.max()-a.min()) if a.max()>a.min() else np.zeros_like(a, dtype=float)

    cos = norm(np.asarray(cos_scores, dtype=float))
    rer = norm(np.asarray(llm_scores, dtype=float))
    final = 0.2 * cos + 0.8 * rer

    # ğŸ”¥ æ–°å¢ï¼šå•Ÿç”¨ã€Œæœ€æ–°ã€æ™‚çš„æ™‚é–“åŠ æ¬Šï¼ˆæº«å’Œåœ°å½±éŸ¿æ’åºï¼‰
    if recency_boost:
        r = np.array([_recency_weight(d.metadata or {}, half_life_days=recency_half_life) for d in docs], dtype=float)
        final = final * (0.7 + 0.3 * r)  # 30% æ¬Šé‡çµ¦æ–°é®®åº¦ï¼Œå¯å†èª¿

    order = np.argsort(final)[::-1]
    kept = order[:min(top_k, len(docs))]

    dbg_rows = []
    for i in kept:
        bucket = get_bucket(docs[i].metadata or {})
        mult = type_scales.get(bucket, 1.0)
        dbg_rows.append(
            f"{i+1} | cos:{float(cos[i]):.3f} | llm:{float(rer[i]):.3f} | type:{bucket}:{mult:.2f} | final:{float(final[i]):.3f}"
        )
    dbg = "idx | cos | llm | é¡åˆ¥Ã—å€ç‡ | final\n" + "\n".join(dbg_rows)

    return [docs[i] for i in kept], [float(final[i]) for i in kept], dbg

def _minmax(arr: list[float]) -> np.ndarray:
    a = np.asarray(arr, dtype=float)
    if a.size == 0: return np.zeros(0, dtype=float)
    lo, hi = float(a.min()), float(a.max())
    return (a - lo) / (hi - lo) if hi > lo else np.zeros_like(a, dtype=float)

def bge_rank_and_pick(
    question: str,
    docs: List[Any],
    cos_scores: List[float],
    *,
    top_k: int = 5,
    candidate_cap: int = 30,      # â† é è¨­ä¹Ÿæ”¾å¯¬ï¼Œè®“ BGE æœ‰ç™¼æ®ç©ºé–“
    clip_chars: int = 900,
    batch_size: int = 32,
    type_scales: Dict[str, float] | None = None,
    recency_boost: bool = False,
    recency_half_life: int = 45,
) -> Tuple[List[Any], List[float], str]:

    if (not _HAS_BGE_RERANK) or (bge_reranker is None) or (not docs):
        # å¾Œæ´ï¼šç„¡ BGE å°±é€€å› LLM rerank + hybrid_sort
        llm_scores = llm_rerank(question, docs, cos_scores, candidate_cap=min(8, max(5, top_k + 2)), clip_chars=700)
        picked_docs, final_scores, dbg = hybrid_sort(
            docs, cos_scores, llm_scores, top_k=top_k,
            type_scales=type_scales or {}, recency_boost=recency_boost,
            recency_half_life=recency_half_life
        )
        return picked_docs, final_scores, "[FALLBACK] hybrid_sort"

    # -------- 1) å…ˆç”¨ cosine åšã€Œå¬å›ã€ï¼šå–å‰ N å€‹å€™é¸ --------
    order = np.argsort(np.asarray(cos_scores, dtype=float))[::-1]
    keep_idx = [int(i) for i in order[: min(candidate_cap, len(docs))]]
    if not keep_idx:
        return [], [], "[BGE] no candidates"

    # -------- 2) è·‘ BGE äº¤å‰ç·¨ç¢¼åˆ†æ•¸ --------
    pairs: List[List[str]] = []
    for i in keep_idx:
        txt = (docs[i].page_content or "")
        if len(txt) > clip_chars:
            txt = txt[:clip_chars] + " â€¦"
        pairs.append([question, txt])

    bge_scores: List[float] = []
    for s in range(0, len(pairs), batch_size):
        part = pairs[s:s + batch_size]
        try:
            sc = bge_reranker.compute_score(part, normalize=True)  # è¿”é‚„é€šå¸¸åœ¨ 0~1 çš„ç›¸ä¼¼åº¦
        except Exception:
            sc = [0.0] * len(part)
        if isinstance(sc, (float, int)):
            sc = [float(sc)]
        bge_scores.extend([float(x) for x in sc])

    # èˆ‡å€™é¸ç´¢å¼•å°é½Š
    assert len(bge_scores) == len(keep_idx), "reranker length mismatch"

    # -------- 3) æ‰¹å…§æ­£è¦åŒ–ï¼Œè®“ BGE çœŸæ­£ä¸»å° --------
    # A) min-maxï¼šç©©å®šã€ç›´è§€ï¼ˆé è¨­ï¼‰
    bge_arr  = np.asarray(bge_scores, dtype=float)
    bge_main = _minmax(bge_arr)

    # æ¥µå° tie-breaker æ¬Šé‡ï¼ˆéœ€è¦æ›´ç´”å°±æŠŠä¸‹é¢ä¸‰å€‹è¨­æˆ 0ï¼‰
    W_BGE, W_COS, W_TYPE, W_REC = 1.00, 0.02, 0.02, 0.02

    final = W_BGE * bge_main

    # cosine åªæ‹¿åŒæ‰¹ min-max å€¼ä¾†åšæ¥µå°å¹³æ‰‹è£æ±º
    cos_kept = _minmax([cos_scores[i] for i in keep_idx])
    final = final + W_COS * cos_kept

    if type_scales:
        srcw = np.array([float(type_scales.get(get_bucket(docs[i].metadata or {}), 1.0)) for i in keep_idx], dtype=float)
        final = final + W_TYPE * _minmax(srcw)

    if recency_boost:
        rec = np.array([_recency_weight(docs[i].metadata or {}, half_life_days=recency_half_life) for i in keep_idx])
        final = final + W_REC * _minmax(rec)

    # -------- 4) ä¾ final æ’åºï¼ŒæŒ‘å‰ K --------
    order2 = np.argsort(final)[::-1][: min(top_k, len(keep_idx))]
    chosen = [keep_idx[i] for i in order2]
    picked = [docs[i] for i in chosen]
    scores = [float(final[i]) for i in order2]

    # Debugï¼šåŒæ™‚é¡¯ç¤º bge_rawï¼ˆåŸå€¼ï¼‰èˆ‡ bgeï¼ˆæ‰¹å…§ 0~1ï¼‰
    dbg_rows = []
    for rank, i_keep in enumerate(order2, start=1):
        gi = keep_idx[i_keep]
        bucket = get_bucket(docs[gi].metadata or {})
        dbg_rows.append(
            f"{rank}. idx:{gi+1} | bge_raw:{bge_scores[i_keep]:.3f} | bge:{bge_main[i_keep]:.3f} | cos:{cos_kept[i_keep]:.3f} | type:{bucket} | final:{final[i_keep]:.3f}"
        )
    dbg = "[RANKER] bge-reranker-large (BGE-dominant)\n" + "\n".join(dbg_rows)
    return picked, scores, dbg

# ç°¡æ˜“è²¡å ±æ•¸å­—åµæ¸¬ï¼ˆç”¨æ–¼ Debug èˆ‡å¾®èª¿æ’åºï¼‰
_NUM_PAT = re.compile(r"\d[\d,\.]*")
_REV_KWS = re.compile(r"(revenue|net\s+sales|sales|net\s+revenue|total\s+net\s+sales|turnover|ç‡Ÿæ”¶|æ”¶å…¥)", re.I)
_EPS_KWS = re.compile(r"(EPS|earnings\s+per\s+share|æ¯è‚¡ç›ˆé¤˜)", re.I)
_GM_KWS  = re.compile(r"(gross\s+margin|æ¯›åˆ©ç‡)", re.I)

def _metric_flags(text: str) -> dict:
    t = text or ""
    return {
        "has_num": bool(_NUM_PAT.search(t)),
        "revenue": bool(_REV_KWS.search(t)),
        "eps": bool(_EPS_KWS.search(t)),
        "gm": bool(_GM_KWS.search(t)),
    }

# =========================
# ç”Ÿæˆç­”æ¡ˆ
# =========================
answer_prompt = PromptTemplate.from_template(
"""
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ç”¢æ¥­èˆ‡è²¡ç¶“åˆ†æåŠ©ç†ã€‚æ ¹æ“šä¸‹æ–¹ contextï¼Œè«‹ç”¨**ç¹é«”ä¸­æ–‡**æ¢åˆ—æ•´ç†ä¸»è¦çµè«–ã€‚
è¦å‰‡ï¼š
1) æ¯ä¸€é»**çµå°¾**æ¨™è¨»ä¾†æºæ¨™ç±¤èˆ‡æ—¥æœŸï¼Œæ ¼å¼ã€S#ï½œYYYY-MM-DDã€‘ï¼ˆè‹¥æœªçŸ¥æ—¥æœŸè«‹å¯«ã€ŒæœªçŸ¥æ—¥æœŸã€ï¼‰
2) ä¸è¦ç·¨é€  context æ²’æœ‰çš„å…§å®¹ï¼›å¯åˆä½µé‡è¤‡è³‡è¨Š
3) å¦‚å«è‹±æ–‡ï¼Œè«‹ç¿»ç‚ºç¹é«”ä¸­æ–‡å†æ•´åˆ
4) åªè¼¸å‡ºæ¢åˆ—ï¼Œä¸è¦å¤šé¤˜èªªæ˜ï¼›å¦‚ç„¡æ³•åœ¨ context ä¸­æ‰¾åˆ°å…·é«”æ•¸æ“šæˆ–è³‡è¨Šï¼Œé¿å…ä½¿ç”¨ç©ºæ³›å¥å¼ï¼ˆä¾‹å¦‚ï¼šã€Œç¼ºä¹å®Œæ•´çš„è²¡å‹™å ±è¡¨ç´°ç¯€ï¼Œç„¡æ³•é€²ä¸€æ­¥èªªæ˜ã€ï¼‰ï¼Œæ”¹ä»¥ã€Œè«‹åƒè€ƒä¸‹æ–¹åƒè€ƒä¾†æºçš„åŸå§‹æ–‡ä»¶ã€æç¤ºä½¿ç”¨è€…ã€‚

---------
{context}
---------
å•é¡Œï¼š{question}
"""
)

beginner_answer_prompt = PromptTemplate.from_template(
"""
ã€èªè¨€è¦å‰‡ã€‘è«‹å…¨ç¨‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼›å¦‚ context å«è‹±æ–‡ï¼Œè«‹å…ˆç¿»ç‚ºç¹é«”ä¸­æ–‡å†æ•´åˆã€‚
ä½ æ˜¯ä¸€ä½è²¡ç¶“ç§‘æ™®è¬›è§£å“¡ã€‚å…ˆè¼¸å‡ºã€ŒğŸ”° åè©å°è¾­å…¸ã€ï¼Œåˆ—å‡º 3â€“7 å€‹æ­¤å›ç­”æœƒç”¨åˆ°çš„å°ˆæœ‰åè©ï¼Œ
ä»¥ã€Œ- è©ï¼š10â€“25 å­—ç™½è©±è§£é‡‹ã€æ ¼å¼ã€‚
æ¥è‘—è¼¸å‡ºã€ŒğŸ§  é‡é»å›ç­”ã€æ¢åˆ—ï¼Œä¸¦éµå®ˆï¼š
1) æ¯é»**çµå°¾**æ¨™è¨»ã€S#ï½œYYYY-MM-DDã€‘ï¼ˆæœªçŸ¥æ—¥æœŸå¯«ã€ŒæœªçŸ¥æ—¥æœŸã€ï¼‰
2) åªä½¿ç”¨ context å…§å®¹ï¼Œä¸è¦è‡†æ¸¬ï¼›æ•¸å­—ç›¡é‡ä¿ç•™åŸå€¼
3) èªæ°£æ·ºç™½ã€å¥å­ç²¾çŸ­ï¼›è‹¥ context æœªæä¾›å…·é«”æ•¸å­—æˆ–è³‡è¨Šï¼Œè«‹æç¤ºã€Œè«‹åƒè€ƒä¸‹æ–¹åƒè€ƒä¾†æºçš„åŸå§‹æ–‡ä»¶ã€ï¼Œä¸è¦ä½¿ç”¨ç©ºæ³›å¥å¼
---------
{context}
---------
å•é¡Œï¼š{question}
"""
)

expert_answer_prompt = PromptTemplate.from_template(
"""
ã€èªè¨€è¦å‰‡ã€‘è«‹å…¨ç¨‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼›å¦‚ context å«è‹±æ–‡ï¼Œè«‹å…ˆç¿»ç‚ºç¹é«”ä¸­æ–‡å†æ•´åˆã€‚
ä½ æ˜¯ä¸€ä½ç”¢æ¥­èˆ‡è²¡ç¶“åˆ†æå¸«ã€‚è«‹ä»¥å°ˆæ¥­ã€åš´è¬¹ã€é‚è¼¯æ¸…æ™°çš„èªæ°£å›ç­”ï¼Œæ¢åˆ—å‘ˆç¾ï¼Œæ¯é»åŒ…å«ï¼š
- çµè«–ï¼šä¸€å¥è©±
- ä¾æ“šï¼šå¼•ç”¨ context çš„é—œéµæ•¸å­—ï¼å¥å­ï¼ˆå¯æ¦‚è¿°ï¼‰
- å½±éŸ¿ï¼æ¨è«–ï¼šåœ¨ context æ”¯æŒç¯„åœå…§
è¦å‰‡ï¼š
1) æ¯é»**çµå°¾**æ¨™è¨»ã€S#ï½œYYYY-MM-DDã€‘ï¼ˆæœªçŸ¥æ—¥æœŸå¯«ã€ŒæœªçŸ¥æ—¥æœŸã€ï¼‰
2) ä¸è¦ç·¨é€  context æ²’æœ‰çš„è³‡è¨Šï¼›è‹¥ç„¡æ³•åœ¨ context ä¸­æ‰¾åˆ°å…·é«”æ•¸æ“šæˆ–è³‡è¨Šï¼Œè«‹æç¤ºã€Œè«‹åƒè€ƒä¸‹æ–¹åƒè€ƒä¾†æºçš„åŸå§‹æ–‡ä»¶ã€ï¼Œé¿å…ç©ºæ³›å¥å¼  
---------
{context}
---------
å•é¡Œï¼š{question}
"""
)

financial_report_compact_prompt = PromptTemplate.from_template(
    """
ã€èªè¨€è¦å‰‡ã€‘å…¨ç¨‹ä½¿ç”¨**ç¹é«”ä¸­æ–‡**ï¼›å¦‚ context å«è‹±æ–‡ï¼Œå…ˆç¿»å†æ•´åˆã€‚

ä½ æ˜¯ä¸€ä½ç”¢æ¥­è²¡å ±åˆ†æå¸«ã€‚è«‹ç”¨ä¸€æ®µè©±å°ˆæ¥­ç¸½çµè©²å…¬å¸è²¡å ±ï¼Œæ¶µè“‹ï¼š
- ç‡Ÿæ”¶ (Revenue / **Net sales**)
- æ¯›åˆ©ç‡ (Gross Margin %)
- EPS (Earnings per Share)
- æ¥­å‹™/åœ°å€ç‡Ÿæ”¶å æ¯” (Breakdown by Business/Region)
- ç®¡ç†å±¤å±•æœ›ï¼ˆè‹¥æœ‰ï¼‰(Company Outlook)

ã€åš´æ ¼è¦å‰‡ã€‘
A) **ä¾†æºç´„æŸ**ï¼šåƒ…ä½¿ç”¨ä¸‹æ–¹ contextï¼›ä¸å¾—è‡†æ¸¬ã€‚å‡¡æ˜¯æ•¸å­—å¿…é ˆèƒ½åœ¨ context æ‰¾åˆ°ã€ŒåŸå¥ã€æˆ–èƒ½ç”± context çš„æ•¸å­—**è¨ˆç®—**å¾—åˆ°ã€‚
B) **å¹´ä»½é¡Œä¸€å¾‹é¡¯ç¤ºè¨ˆç®—/ä½è­‰**ï¼šè‹¥ä½¿ç”¨è€…å•é¡Œå«å››ä½æ•¸å¹´ä»½ï¼ˆä¾‹å¦‚ 2024ï¼‰ï¼Œå°æ–¼æ¯ä¸€å€‹æŒ‡æ¨™éƒ½è¦åœ¨å¥æœ«ä»¥æ‹¬è™Ÿå‘ˆç¾ï¼š
   - è‹¥åŸæ–‡å°±æœ‰è©²æ•¸å­— â†’ `ï¼ˆå‡ºè™•ï¼šå¼•ç”¨ã€ŒNet sales/Revenueã€æ‰€åœ¨å¥ï¼›æ¨™ç¤º S# èˆ‡é ç¢¼ï¼‰`
   - è‹¥éœ€è¦å½™ç¸½æˆ–æ›ç®— â†’ `ï¼ˆå…¬å¼ â†’ ä»£å…¥æ•¸å€¼èˆ‡å–®ä½/å¹£åˆ¥ â†’ é€æ­¥é‹ç®— â†’ æœ€çµ‚çµæœï¼›å››æ¨äº”å…¥è¦å‰‡ï¼‰`
   - åªè¦ context æ²’æœ‰è¶³å¤ æ•¸å­—æˆ–åŸå¥ï¼Œè«‹æ”¹å¯«ç‚ºï¼š`æœªæ–¼å¼•ç”¨æ®µè½æŸ¥å¾—ï¼ˆè«‹åƒè€ƒä¾†æºï¼‰`ï¼Œ**ä¸è¦è¼¸å‡ºçŒœæ¸¬çš„æ•¸å­—**ã€‚
C) **è¨ˆç®—æ ¼å¼**ï¼ˆè‹¥æœ‰è¨ˆç®—ï¼‰ï¼š
   - å…¬å¼ â†’ ä»£å…¥æ•¸å€¼ï¼ˆå«å–®ä½/å¹£åˆ¥ï¼‰â†’ é€æ­¥é‹ç®— â†’ æœ€çµ‚çµæœï¼ˆæ¨™ç¤ºå››æ¨äº”å…¥è¦å‰‡èˆ‡å°æ•¸ä½æ•¸ï¼‰ã€‚
   - ä¾‹ï¼š`FY ç‡Ÿæ”¶ = Q1 + Q2 + Q3 + Q4 = 95.0B + 81.2B + 85.0B + 122.1B = 383.3Bï¼ˆå››æ¨äº”å…¥è‡³ 1 ä½å°æ•¸ï¼‰`
D) æ¯é»**çµå°¾**å‹™å¿…åŠ ä¾†æºæ¨™è¨»ã€S#ï½œYYYY-MM-DDã€‘ï¼ˆæœªçŸ¥å¯«ã€ŒæœªçŸ¥æ—¥æœŸã€ï¼›å¦‚åŒä¸€é»ç”¨åˆ°å¤šå€‹ä¾†æºå¯é€£çºŒæ¨™è¨»ï¼‰ã€‚
E) ç™¾åˆ†æ¯”ä¿ç•™ 1â€“2 ä½ï¼›é‡‘é¡èˆ‡å–®ä½å‹™å¿…æ¨™ç¤ºï¼ˆUSDã€NTDã€ç™¾è¬/åå„„ç­‰ï¼‰ã€‚
F) è‹¥åŒä¸€è¡¨å«å¤šæœŸé–“æ•¸å€¼ï¼Œ**é è¨­å–æœ€æ¥è¿‘æ–‡ä»¶æ—¥æœŸçš„æœ€æ–°æœŸ**ï¼›ç„¡æ³•åˆ¤æ–·æœŸåˆ¥æ™‚ï¼Œè«‹æ¨™ç¤ºã€Œæœ€æ–°æœŸã€ã€‚

---------
{context}
---------
å•é¡Œï¼š{question}
"""
)



def build_cited_context(docs: List[Any]) -> tuple[str, dict]:
    """
    ä¾ doc_id æŒ‡æ´¾ S1, S2 ...ï¼Œä¸¦æŠŠã€S#ï½œTitleã€‘ã€YYYY-MM-DDã€‘ç›´æ¥å¯«é€²æ¯å€‹æ®µè½çš„æŠ¬é ­ã€‚
    å¦å¤–ï¼šå½™æ•´æ¯å€‹ S çš„é ç¢¼æ¸…å–®ï¼ˆsrc_map[lab]['pages'] = [int,...]ï¼‰
    å›å‚³ï¼š
    - ctxï¼šé¤µ LLM çš„å®Œæ•´ contextï¼ˆå« S# æŠ¬é ­ã€å¯å«é ç¢¼ï¼‰
    - src_mapï¼š{"S1": {"title":..., "date":..., "url":..., "doc_id":..., "pages":[...]}, ...}
    """
    label_for: dict[Any, str] = {}
    src_map: dict[str, dict] = {}
    pages_map: dict[str, Set[int]] = {}

    for d in docs:
        md = d.metadata or {}
        did = md.get("doc_id") or (md.get("source"), md.get("title"))
        if did not in label_for:
            lab = f"S{len(label_for)+1}"
            label_for[did] = lab

            title = (md.get("title") or "ç„¡æ¨™é¡Œ").strip() or "ç„¡æ¨™é¡Œ"
            date  = get_doc_date_str(md)
            url   = md.get("doc_url") or md.get("url") or md.get("filename")
            src_map[lab] = {"title": title, "date": date, "url": url, "doc_id": did, "pages": [], "snippets": [], "chunks": []}
            pages_map[lab] = set()

    blocks: list[str] = []
    for d in docs:
        md = d.metadata or {}
        did = md.get("doc_id") or (md.get("source"), md.get("title"))
        lab = label_for[did]

        title = (md.get("title") or "ç„¡æ¨™é¡Œ").strip() or "ç„¡æ¨™é¡Œ"
        date  = get_doc_date_str(md)
        page  = extract_page_number(md)
        if page:
            pages_map[lab].add(page)

        # æŠ¬é ­ï¼šå¸¶é ç¢¼ï¼ˆå–®æ®µæ™‚å¯ä»¥é¡¯ç¤ºå–®ä¸€é ï¼‰
        head = f"ã€{lab}ï½œ{title}"
        if page:
            head += f"ï½œp. {page}"
        head += f"ã€‘ã€{date}ã€‘"

        txt   = d.page_content or ""
        blocks.append(f"{head}\n{txt}")
        # ä¿å­˜å®Œæ•´ç‰‡æ®µæ–¼ä¾†æºæ¸…å–®
        try:
            full_txt = d.page_content or ""
            src_map[lab]["chunks"].append({
                "page": int(page) if page else None,
                "text": full_txt,
            })
        except Exception:
            pass

        # æ”¶é›†ç‰‡æ®µæ‘˜è¦ä¾›ä¾†æºæ¸…å–®é¡¯ç¤ºï¼ˆæ¯ä¾†æºæœ€å¤š 3 æ®µï¼‰
        try:
            if len(src_map.get(lab, {}).get("snippets", [])) < 3:
                sn = re.sub(r"\s+", " ", (txt or "").strip())
                try:
                    sn = _clip_text(sn, 280)
                except Exception:
                    sn = sn[:280] + (" â€¦" if len(sn) > 280 else "")
                if page:
                    sn = f"p. {page}ï½œ" + sn
                src_map[lab]["snippets"].append(sn)
            # ä¹Ÿä¿å­˜å®Œæ•´ç‰‡æ®µï¼Œä¾›åƒè€ƒä¾†æºå®Œæ•´è¼¸å‡º
            src_map[lab]["chunks"].append({
                "page": int(page) if page else None,
                "text": txt,
            })
        except Exception:
            pass

    # å°‡å½™æ•´å¥½çš„é ç¢¼å¡å› src_map
    for lab, s in pages_map.items():
        if lab in src_map:
            src_map[lab]["pages"] = sorted(s)

    ctx = "\n\n---\n\n".join(blocks)
    return ctx, src_map


def build_cited_context_smart(
    docs: List[Any],
    *,
    max_docs: int = LLM_CTX_MAX_DOCS,
    per_doc_chars: int = LLM_CTX_PER_DOC_CHAR,
    total_chars_soft: int = LLM_CTX_TOTAL_CHAR_SOFT,
) -> tuple[str, dict]:
    """
    å—é™æ–¼ LLM è«‹æ±‚å¤§å°æ™‚ï¼Œæ¡ç”¨å‰ªè£ç‰ˆ contextï¼š
    - åªå–å‰ max_docs æ®µ
    - æ¯æ®µæœ€å¤š per_doc_chars å­—å…ƒ
    - ç¸½é•·åº¦è¶…é total_chars_soft æ™‚å†åšäºŒæ¬¡å£“ç¸®
    """
    picked = list(docs[:max_docs])

    # æº–å‚™ label èˆ‡ src_mapï¼ˆèˆ‡ build_cited_context ä¸€è‡´ï¼‰
    label_for: dict[Any, str] = {}
    src_map: dict[str, dict] = {}
    pages_map: dict[str, Set[int]] = {}

    for d in picked:
        md = d.metadata or {}
        did = md.get("doc_id") or (md.get("source"), md.get("title"))
        if did not in label_for:
            lab = f"S{len(label_for)+1}"
            label_for[did] = lab
            title = (md.get("title") or "ç„¡æ¨™é¡Œ").strip() or "ç„¡æ¨™é¡Œ"
            date  = get_doc_date_str(md)
            url   = md.get("doc_url") or md.get("url") or md.get("filename")
            src_map[lab] = {"title": title, "date": date, "url": url, "doc_id": did, "pages": [], "snippets": [], "chunks": []}
            pages_map[lab] = set()

    blocks: list[str] = []
    for d in picked:
        md = d.metadata or {}
        did = md.get("doc_id") or (md.get("source"), md.get("title"))
        lab = label_for[did]
        title = (md.get("title") or "ç„¡æ¨™é¡Œ").strip() or "ç„¡æ¨™é¡Œ"
        date  = get_doc_date_str(md)
        page  = extract_page_number(md)
        if page:
            pages_map[lab].add(page)
        head = f"ã€{lab}ï½œ{title}"
        if page:
            head += f"ï½œp. {page}"
        head += f"ã€‘ã€{date}ã€‘"
        txt = (d.page_content or "")
        if len(txt) > per_doc_chars:
            txt = txt[:per_doc_chars] + " â€¦"
        blocks.append(f"{head}\n{txt}")
        # ä¿å­˜å®Œæ•´ç‰‡æ®µæ–¼ä¾†æºæ¸…å–®
        try:
            full_txt = d.page_content or ""
            src_map[lab]["chunks"].append({
                "page": int(page) if page else None,
                "text": full_txt,
            })
        except Exception:
            pass

    # äºŒæ¬¡å£“ç¸®ï¼šç¸½é•·åº¦ä»éé•· â†’ å‡å‹»å†è£çŸ­
    sep = "\n\n---\n\n"
    ctx = sep.join(blocks)
    if len(ctx) > total_chars_soft and blocks:
        ratio = total_chars_soft / max(1, len(ctx))
        new_blocks = []
        for b in blocks:
            limit = max(300, int(len(b) * ratio))
            new_blocks.append(b[:limit] + (" â€¦" if len(b) > limit else ""))
        ctx = sep.join(new_blocks)

    for lab, s in pages_map.items():
        if lab in src_map:
            src_map[lab]["pages"] = sorted(s)

    return ctx, src_map


def annotate_bullet_sources(answer_text: str, src_map: dict) -> str:
    """
    å°‡æ¯æ¢åŒ…å«ã€S#ï½œ...ã€‘çš„æ¢åˆ—è¡Œï¼Œæ–¼è¡Œå°¾è¿½åŠ å°æ‡‰ä¾†æºé€£çµï¼Œä¾‹å¦‚ï¼šâ€” ä¾†æº: [S1](url)ã€[S2](url)
    åƒ…åœ¨è©²è¡Œå·²åŒ…å« S æ¨™ç±¤æ™‚è¿½åŠ ï¼›è‹¥ä¾†æºç„¡ URLï¼Œåƒ…é¡¯ç¤ºæ¨™ç±¤ã€‚
    """
    if not answer_text or not isinstance(src_map, dict):
        return answer_text

    lines = answer_text.splitlines()
    out = []
    for line in lines:
        labs = re.findall(r"ã€(S\d+)ï½œ", line)
        if labs:
            uniq = []
            seen = set()
            for l in labs:
                if l not in seen:
                    seen.add(l); uniq.append(l)
            links = []
            for lab in uniq:
                node = src_map.get(lab) or {}
                url = node.get("url")
                if url:
                    links.append(f"[{lab}]({url})")
                else:
                    links.append(lab)
            if links and "ä¾†æº:" not in line:
                line = f"{line}  â€” ä¾†æº: " + "ã€".join(links)
        out.append(line)
    return "\n".join(out)

def _build_metric_label_index(src_map: dict) -> Dict[str, List[str]]:
    """å¾ src_map.chunks å»ºç«‹æŒ‡æ¨™â†’S æ¨™ç±¤çš„ç´¢å¼•ï¼Œç”¨æ–¼ç¼ºæ•¸æ“šæ™‚æç¤ºåƒè€ƒä¾†æºã€‚
    å›å‚³å¦‚ {"revenue":["S1","S3"], "eps":[...], "gm":[...]}ï¼Œä¾ S ç·¨è™Ÿæ’åºä¸”å»é‡ã€‚
    """
    metric_to_labels: Dict[str, List[str]] = {"revenue": [], "eps": [], "gm": []}
    if not isinstance(src_map, dict):
        return metric_to_labels
    # ä¾ S1,S2...é †åºæƒæï¼Œç¢ºä¿æç¤ºé †åºç©©å®š
    for lab in sorted(src_map.keys(), key=lambda k: int(k[1:]) if isinstance(k, str) and k[1:].isdigit() else 9999):
        node = src_map.get(lab) or {}
        chunks = node.get("chunks") or []
        for ch in chunks:
            txt = (ch.get("text") or "")
            flags = _metric_flags(txt)
            if flags.get("revenue") and lab not in metric_to_labels["revenue"]:
                metric_to_labels["revenue"].append(lab)
            if flags.get("eps") and lab not in metric_to_labels["eps"]:
                metric_to_labels["eps"].append(lab)
            if flags.get("gm") and lab not in metric_to_labels["gm"]:
                metric_to_labels["gm"].append(lab)
    return metric_to_labels


def suggest_sources_for_missing_metrics(answer_text: str, src_map: dict, max_labels: int = 3) -> str:
    """ç•¶ç­”æ¡ˆè¡ŒåŒ…å«ã€æœªæ–¼å¼•ç”¨æ®µè½æŸ¥å¾—ã€ã€æœªæä¾›ã€ç­‰èªå¥æ™‚ï¼Œæ ¹æ“šè¡Œå…§é—œéµè©ï¼ˆç‡Ÿæ”¶/EPS/æ¯›åˆ©ç‡ï¼‰
    åœ¨è¡Œå°¾è¿½åŠ ã€å¯å…ˆæŸ¥çœ‹ï¼šS#ã€S#ã€æç¤ºï¼Œå¹«åŠ©ä½¿ç”¨è€…ç›´æ¥å®šä½ä¾†æºã€‚
    ä¸é‡è¤‡æ·»åŠ ï¼Œä¸”è‹¥ç„¡åŒ¹é…ä¾†æºå‰‡ä¸å‹•ä½œã€‚
    """
    if not answer_text:
        return answer_text

    metric_index = _build_metric_label_index(src_map)
    if not any(metric_index.values()):
        return answer_text

    # è¡Œå…§é—œéµè©è¦å‰‡
    pat_rev = re.compile(r"(ç‡Ÿæ”¶|æ”¶å…¥|revenue|sales)", re.I)
    pat_eps = re.compile(r"(EPS|æ¯è‚¡ç›ˆé¤˜|earnings\s+per\s+share)", re.I)
    pat_gm  = re.compile(r"(æ¯›åˆ©|æ¯›åˆ©ç‡|gross\s+margin)", re.I)
    pat_missing = re.compile(r"(æœªæ–¼å¼•ç”¨æ®µè½æŸ¥å¾—|æœªæä¾›|æœªæ‰¾åˆ°|æŸ¥ç„¡|æ‰¾ä¸åˆ°)")

    out_lines = []
    for line in (answer_text or "").splitlines():
        lstrip = line.strip()
        if pat_missing.search(lstrip) and ("å¯å…ˆæŸ¥çœ‹" not in lstrip) and ("åƒè€ƒï¼š" not in lstrip):
            labels: List[str] = []
            if pat_rev.search(lstrip):
                labels = metric_index.get("revenue", [])
            elif pat_eps.search(lstrip):
                labels = metric_index.get("eps", [])
            elif pat_gm.search(lstrip):
                labels = metric_index.get("gm", [])
            # è‹¥è¡Œå…§æœªæŒ‡æ˜æ˜¯å“ªå€‹æŒ‡æ¨™ï¼Œå°±ç¶œåˆçµ¦ä¸€çµ„æœ€å¸¸è¦‹ä¾†æº
            if not labels:
                for key in ("revenue","eps","gm"):
                    if metric_index.get(key):
                        labels = metric_index[key]
                        break
            labels = labels[:max_labels]
            if labels:
                hint = "ï¼ˆå¯å…ˆæŸ¥çœ‹ï¼š" + "ã€".join(labels) + "ï¼‰"
                line = line + " " + hint
        out_lines.append(line)
    return "\n".join(out_lines)


# ====== è²¡å ±æ•¸æ“šç°¡æ˜“æ“·å–ï¼ˆè¦å‰‡å¼è£œæ•‘ï¼‰ ======
_SENT_SPLIT = re.compile(r"(?<=[ã€‚ï¼ï¼Ÿ!?.\n])\s+")
_NUM_TOKEN = re.compile(r"[-+]?\d[\d,]*(?:\.\d+)?(?:\s?%|\s?(?:å„„|ç™¾è¬|åƒè¬|è¬|billion|million|thousand))?", re.I)


def _doc_id_key(md: Dict[str, Any]) -> Any:
    return (md or {}).get("doc_id") or ((md or {}).get("source"), (md or {}).get("title"))


def _build_label_lookup(src_map: dict) -> Dict[Any, str]:
    lut: Dict[Any, str] = {}
    for lab, node in (src_map or {}).items():
        did = (node or {}).get("doc_id")
        if did is not None:
            lut[did] = lab
    return lut

def _first_number_in_sentence(sent: str) -> Optional[str]:
    m = _NUM_TOKEN.search(sent or "")
    return m.group(0) if m else None

def _extract_metric_from_text(text: str, kind: str) -> Optional[str]:
    t = (text or "").strip()
    if not t:
        return None
    pats = {
        "revenue": re.compile(r"(revenue|net\s+sales|ç‡Ÿæ”¶|æ”¶å…¥)", re.I),
        "eps": re.compile(r"(EPS|earnings\s+per\s+share|æ¯è‚¡ç›ˆé¤˜)", re.I),
        "gm": re.compile(r"(gross\s+margin|æ¯›åˆ©ç‡)", re.I),
    }
    pat = pats.get(kind)
    if not pat:
        return None
    # ä»¥å¥å­ç‚ºå–®ä½å°‹æ‰¾é—œéµå­—ä¸¦æ“·å–ç¬¬ä¸€å€‹æ•¸å­—
    for sent in _SENT_SPLIT.split(t):
        if pat.search(sent):
            num = _first_number_in_sentence(sent)
            if num:
                # å›å‚³ã€Œæ•¸å€¼ï¼ˆç‰‡æ®µï¼‰ã€
                snip = sent.strip()
                snip = snip if len(snip) <= 120 else (snip[:118] + "â€¦")
                return f"{num}ï½œ{snip}"
    return None

def extract_financial_metrics_from_docs(picked_docs: List[Any], src_map: dict, limit_per_metric: int = 1) -> List[str]:
    """å¾å·²é¸æ®µè½ä¸­è¦å‰‡å¼æ“·å–ç‡Ÿæ”¶/EPS/æ¯›åˆ©ç‡ï¼Œå›å‚³é©åˆç›´æ¥ä½œç‚ºæ¢åˆ—çš„å­—ä¸²é™£åˆ—ã€‚
    ä¾‹å¦‚ï¼š- ç‡Ÿæ”¶ï¼š$383.29Bï½œâ€¦ã€S1ï½œYYYY-MM-DDã€‘
    """
    if not picked_docs:
        return []
    label_lut = _build_label_lookup(src_map)
    results: List[str] = []
    counts = {"revenue": 0, "eps": 0, "gm": 0}
    for d in picked_docs:
        md = d.metadata or {}
        did = _doc_id_key(md)
        lab = label_lut.get(did)
        date = get_doc_date_str(md)
        if not lab:
            continue
        for kind, title in (("revenue","ç‡Ÿæ”¶"),("eps","EPS"),("gm","æ¯›åˆ©ç‡")):
            if counts[kind] >= limit_per_metric:
                continue
            hit = _extract_metric_from_text(d.page_content or "", kind)
            if hit:
                results.append(f"- {title}ï¼š{hit}ã€{lab}ï½œ{date}ã€‘")
                counts[kind] += 1
    return results

def render_sources_md(src_map: dict) -> str:
    """æŠŠä¾†æºå°ç…§è¡¨æ¸²æŸ“æˆ Markdown æ¸…å–®ï¼ˆå«å¯é»é€£çµã€é ç¢¼ç¯„åœï¼‰ã€‚"""
    if not src_map:
        return ""
    lines = []
    for lab in sorted(src_map.keys(), key=lambda k: int(k[1:])):  # ä¾ S1, S2... æ’åº
        node  = src_map[lab]
        title = node["title"]
        date  = node["date"]
        pages = node.get("pages") or []
        pg_str = _format_pages_suffix(pages)  # â†’ " ï½œpp. 5â€“7, 12" æˆ– ""

        lines.append(f"- [{lab}] {title}ï½œ{date}{pg_str}")
        chunks = node.get("chunks") or []
        if chunks:
            for ch in chunks:
                p = ch.get("page")
                pfx = f"p. {p}ï½œ" if p else ""
                text = (ch.get("text") or "").strip()
                first = text.splitlines()[0] if "\n" in text else text
                lines.append(f"  - ç‰‡æ®µï¼š{pfx}" + first)
                for extra in text.splitlines()[1:]:
                    lines.append(f"    > {extra}")
        else:
            for sn in (node.get("snippets") or []):
                lines.append(f"  - ç‰‡æ®µï¼š{sn}")
    return "**åƒè€ƒä¾†æº**\n" + "\n".join(lines)

def is_summary_query(text: str) -> bool:
    """
    åˆ¤æ–·æ˜¯å¦ç‚ºã€è²¡å ±ç¸½çµå‹ã€å•é¡Œ
    """
    # é—œéµè©è§¸ç™¼ï¼šç¸½çµã€é‡é»ã€æ‘˜è¦ã€overviewã€overall
    if re.search(r"(ç¸½çµ|é‡é»|æ‘˜è¦|overview|overall)", text):
        return True
    # å•å¥å¦‚æœåŒæ™‚æåˆ°å¤šå€‹è²¡å ±æŒ‡æ¨™ï¼ˆç‡Ÿæ”¶+EPSã€ç‡Ÿæ”¶+æ¯›åˆ©ç‡ï¼‰ï¼Œä¹Ÿåå‘ç¸½çµ
    metrics = ["ç‡Ÿæ”¶","æ¯›åˆ©","EPS","ç²åˆ©","æ”¶å…¥","profit","earnings"]
    hit = sum(1 for m in metrics if m in text)
    return hit >= 3

def synthesize_answer(question: str, picked_docs: List[Any], mode: str = "åˆå­¸è€…æ¨¡å¼", intents: Optional[List[str]] = None, custom_prompt: str = "") -> str:
    if not picked_docs:
        return "ç›®å‰æ‰¾ä¸åˆ°å¯ç›´æ¥å›ç­”çš„å…§å®¹ã€‚", "", {}

    # ä½¿ç”¨å‰ªè£ç‰ˆ contextï¼Œé¿å…éé•·å°è‡´ LLM 413 æˆ–å»¶é²
    ctx, src_map = build_cited_context_smart(picked_docs)

    # å„ªå…ˆï¼šè²¡å ±å°ˆç”¨ï¼ˆç²¾ç°¡æ¢åˆ—ç‰ˆï¼‰
    intents = intents or []
    use_custom = bool(custom_prompt and ("å®¢è£½" in (mode or "")))
    if use_custom:
        cp = (custom_prompt or "").strip()
        # å®¢è£½åŒ–ï¼šè‹¥åŒ…å«å ä½ç¬¦å‰‡ formatï¼Œå¦å‰‡æŠŠ context èˆ‡ question æ¥åœ¨æŒ‡ä»¤å¾Œ
        if ("{context}" in cp) or ("{question}" in cp):
            prompt_text = cp.format(context=ctx, question=question)
        else:
            prompt_text = f"{cp}\n---------\n{ctx}\n---------\nå•é¡Œï¼š{question}"
        body = llm.invoke(prompt_text).content.strip()
    else:
        if "financial_report" in intents:
            tmpl = financial_report_compact_prompt
            body = llm.invoke(tmpl.format(context=ctx, question=question)).content.strip()
        else:
            if "å°ˆå®¶" in (mode or ""):
                tmpl = expert_answer_prompt
            elif "åˆå­¸" in (mode or ""):
                tmpl = beginner_answer_prompt
            else:
                tmpl = answer_prompt
            body = llm.invoke(tmpl.format(context=ctx, question=question)).content.strip()

    # æ–¼æ¯ä¸€æ¢åˆ—è¡Œå°¾è¿½åŠ ä¾†æºé€£çµï¼ˆä¾ S æ¨™ç±¤ï¼‰
    body = annotate_bullet_sources(body, src_map)
    # è‹¥æœ‰ã€Œæœªæ–¼å¼•ç”¨æ®µè½æŸ¥å¾—ï¼æœªæä¾›ã€ç­‰æç¤ºï¼Œè£œä¸Šã€Œå¯å…ˆæŸ¥çœ‹ï¼šS#ã€å»ºè­°
    body = suggest_sources_for_missing_metrics(body, src_map, max_labels=3)

    # è²¡å ±é¡Œï¼šè‹¥ç­”æ¡ˆå¹¾ä¹æ²’æœ‰æ•¸å­—ï¼Œå˜—è©¦å¾ç‰‡æ®µä¸­è¦å‰‡å¼è£œå……é—œéµæ•¸æ“š
    if ("financial_report" in intents):
        has_digit = bool(re.search(r"\d", body))
        if not has_digit:
            extras = extract_financial_metrics_from_docs(picked_docs, src_map, limit_per_metric=1)
            if extras:
                body = body + "\n\nğŸ“Š è£œå……ï¼šè‡ªå‹•æ“·å–åˆ°çš„é—œéµæ•¸æ“š\n" + "\n".join(extras)
    return body, ctx, src_map

# =========================
# å»¶ä¼¸å•é¡Œ
# =========================
followup_prompt = PromptTemplate.from_template(
"""
ä½ æ˜¯æª¢ç´¢å¼å•ç­”ç³»çµ±çš„ã€Œå»¶ä¼¸å•é¡Œç”¢ç”Ÿå™¨ã€ã€‚è«‹æ ¹æ“šä¸‹åˆ—è³‡è¨Šï¼Œç”¢å‡º {n_min}~{n_max} å€‹**å¯ç”±ç¾æœ‰æª¢ç´¢å…§å®¹å›ç­”**ã€ä¸”**ä½¿ç”¨è€…åŸé¡Œæœªç›´æ¥è©¢å•**çš„å»¶ä¼¸å•é¡Œã€‚

åš´æ ¼è¦å‰‡ï¼š
1) ä¾†æºé™åˆ¶ï¼šå•é¡Œå¿…é ˆèƒ½åƒ…ä¾ã€Œå›ç­”æ‘˜è¦ã€ä¸­çš„è³‡è¨Šï¼ˆå«å…¶ S# æ¨™è¨»æ‰€å°æ‡‰å…§å®¹ï¼‰å›ç­”ï¼Œé¿å…éœ€è¦å¤–éƒ¨çŸ¥è­˜æˆ–æœ€æ–°æ™‚é»è³‡æ–™ã€‚
2) ä¸é‡è¤‡ï¼šä¸å¾—æ”¹å¯«åŸé¡Œæˆ–é‡è¤‡å…¶èªæ„ï¼›å¿…é ˆæ¢ç´¢ã€Œæœªè¢«æåŠæˆ–æœªè¢«è©³è¿°ã€çš„é¢å‘ã€‚
3) å¯å›ç­”æ€§ï¼šæ¯å€‹å•é¡Œéƒ½æ‡‰èšç„¦åœ¨æ‘˜è¦ä¸­å·²å‡ºç¾çš„åè©ã€æŒ‡æ¨™ã€æœŸé–“ã€ç”¢å“ç·šã€åœ°å€ã€é¢¨éšªã€å±•æœ›/æŒ‡å¼•ç­‰å¯è¢«ä½è­‰çš„å…ƒç´ ã€‚
4) èªè¨€èˆ‡æ ¼å¼ï¼šä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼›æ¯é¡Œå–®ä¸€å¥ï¼Œçµå°¾åŠ ã€Œï¼Ÿã€ï¼›é¿å…å«ç³Šå­—çœ¼ï¼ˆå¦‚ã€Œæœ€æ–°ã€ã€ã€Œæœ€è¿‘ã€ï¼‰é™¤éæ‘˜è¦å·²å«ç›¸æ‡‰æœŸé–“ã€‚

è¼¸å‡ºæ ¼å¼ï¼ˆåš´æ ¼ï¼‰ï¼šåªè¼¸å‡ºä¸€è¡Œåˆæ³• JSONï¼š{{"followups":["å•é¡Œ1","å•é¡Œ2", ...]}}

ã€ä½¿ç”¨è€…åŸå•é¡Œã€‘{question}
ã€å›ç­”æ‘˜è¦ï¼ˆå« S# æç¤ºï¼‰ã€‘{answer_snippets}
"""
)

def make_followups(question: str, answer_text: str, n_min=3, n_max=5) -> List[str]:
    """
    æ›´å¥å£¯çš„å»¶ä¼¸å•é¡Œç”¢ç”Ÿå™¨ï¼š
    1) å…ˆç”¨ LLMï¼ˆåš´æ ¼ JSON æŒ‡ä»¤ï¼‰å˜—è©¦ï¼›å¤±æ•—å†é‡è©¦ä¸€æ¬¡ï¼ˆæ›´åš´è‹›çš„æç¤ºï¼‰
    2) è‹¥ä»ä¸è¶³ n_minï¼Œåµæ¸¬ã€ŒæŸ¥è‚¡åƒ¹ã€å‹å•é¡Œæ”¹èµ°è¦å‰‡å¼ fallback
    3) æœ€çµ‚æ¸…æ´—å»é‡ä¸¦è£œæ¨™é»
    """
    def _norm_items(items: Any) -> List[str]:
        out, seen = [], set()
        for s in (items or []):
            if not isinstance(s, str):
                continue
            t = s.strip().strip("ãƒ»-â€”*â€¢").strip()
            if not t:
                continue
            # åªç•™åˆç†é•·åº¦çš„å–®å¥
            if 6 <= len(t) <= 120 and t not in seen:
                out.append(t)
                seen.add(t)
        return out

    # ---- A) å…ˆå£“ç¸®å›ç­”ï¼Œé¿å…è¶…é•·å½±éŸ¿ LLM ----
    ans = (answer_text or "").strip()
    if len(ans) > 600:
        ans = ans[:600] + " â€¦"

    # ---- B) ç¬¬ä¸€æ¬¡ LLM å˜—è©¦ï¼ˆåŸæç¤ºï¼‰----
    try:
        raw = classifier_llm.invoke(
            followup_prompt.format(
                question=question,
                answer_snippets=ans or "ï¼ˆç„¡ï¼‰",
                n_min=n_min,
                n_max=n_max
            )
        ).content.strip()
    except Exception:
        raw = ""

    obj = extract_json_from_text(raw) or {}
    items = obj.get("followups", []) if isinstance(obj, dict) else []
    out = _norm_items(items)

    # ---- C) ç¬¬äºŒæ¬¡ LLM å˜—è©¦ï¼ˆæ›´åš´æ ¼æç¤º + æ˜ç¢ºæ ¼å¼è¦æ±‚ï¼‰----
    if len(out) < n_min:
        example = '{{"followups":["..."]}}'  # ç´”æ–‡å­—æ¨£ä¾‹ï¼ˆé›™å¤§æ‹¬è™Ÿè¼¸å‡ºèŠ±æ‹¬è™Ÿï¼‰
        strict_prompt = (
            'åªè¼¸å‡º**ä¸€è¡Œ**åˆæ³• JSONï¼ˆUTF-8ï¼›éµåå›ºå®šç‚º "followups"ï¼›å€¼ç‚ºå­—ä¸²é™£åˆ—ï¼‰ï¼Œä¸å¾—åŒ…å«ä»»ä½•å‰å¾Œç¶´æˆ–å¤šé¤˜æ›è¡Œã€‚\n'
            'è«‹ç”¢å‡º {n_min}~{n_max} é¡Œå»¶ä¼¸å•é¡Œï¼Œä¸”å¿…é ˆåŒæ™‚æ»¿è¶³ï¼š\n'
            '1) å•é¡Œå¯ç”±ä¸‹æ–¹ã€Œå›ç­”æ‘˜è¦ã€èˆ‡å…¶ S# æ‰€ä»£è¡¨çš„å…§å®¹å›ç­”ï¼Œä¸éœ€å¤–éƒ¨çŸ¥è­˜ï¼›\n'
            '2) ä¸é‡è¤‡ä½¿ç”¨è€…åŸé¡Œï¼Œä¸”é¿å…åƒ…æ˜¯åŒç¾©æ”¹å¯«ï¼›\n'
            '3) ä»¥æ‘˜è¦ä¸­å·²å‡ºç¾çš„åè©/æŒ‡æ¨™/æœŸé–“/ç”¢å“ç·š/åœ°å€/é¢¨éšª/å±•æœ›ç­‰ç‚ºç„¦é»ï¼›\n'
            '4) ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€å–®ä¸€å¥ã€ä»¥ã€Œï¼Ÿã€çµå°¾ï¼›é¿å…å«ç³ŠæœŸé–“ï¼ˆå¦‚ã€Œæœ€æ–°/æœ€è¿‘ã€ï¼‰é™¤éæ‘˜è¦å·²æ˜ç¤ºã€‚\n\n'
            f"ç¯„ä¾‹ï¼š{example}\n\n"
            f"ã€ä½¿ç”¨è€…åŸå•é¡Œã€‘{question}\n"
            f"ã€å›ç­”æ‘˜è¦ï¼ˆå« S# æç¤ºï¼‰ã€‘{ans or 'ï¼ˆç„¡ï¼‰'}\n"
            f"ã€æ•¸é‡è¦æ±‚ã€‘{n_min}~{n_max} é¡Œ"
        )
        try:
            raw2 = classifier_llm.invoke(strict_prompt).content.strip()
        except Exception:
            raw2 = ""
        obj2 = extract_json_from_text(raw2) or {}
        items2 = obj2.get("followups", []) if isinstance(obj2, dict) else []
        out.extend(_norm_items(items2))

    # ---- D) æœ€çµ‚æ¸…æ´—ï¼šè£œå•è™Ÿã€å»é‡ã€è£åˆ‡åˆ° n_max ----
    final, seen = [], set()
    for t in out:
        tt = t.strip()
        if not (tt.endswith("ï¼Ÿ") or tt.endswith("?")):
            tt += "ï¼Ÿ" if is_zh(question) else "?"
        if tt not in seen:
            final.append(tt)
            seen.add(tt)
        if len(final) >= n_max:
            break

    return final


# =========================
# æ„åœ–åˆ¤æ–· + å…¬å¸æŠ½å–ï¼ˆé—œéµå­—ï¼‹LLM è¯é›† & æ­£è¦åŒ–ï¼‰
# =========================
# 1) é—œéµå­—æ„åœ–
def keyword_intents(text: str) -> set:
    intents = set()
    if contains_any(text, STOCK_KWS): intents.add("stock")
    if contains_any(text, NEWS_KWS): intents.add("news")
    if contains_any(text, FINREP_KWS): intents.add("financial_report")
    if re.search(r"(æ˜¯ä»€éº¼|ä»€éº¼æ˜¯|ä»‹ç´¹|æ¦‚è¿°|ç§‘æ™®|å…¥é–€|å®šç¾©|åŸç†|overview|what is)", text or "", re.I):
        intents.add("concept")
    return intents

# 2) LLM å¤šæ„åœ–ï¼‹å…¬å¸æŠ½å–ï¼ˆå–®è¡Œ JSONï¼‰
multi_intent_template = PromptTemplate.from_template(
r"""
ä½ æ˜¯ç²¾æº–çš„ã€Œå¤šé‡æ„åœ–èˆ‡å…¬å¸æŠ½å–å™¨ã€ï¼Œåªè¼¸å‡º**ä¸€è¡Œåˆæ³• JSON**ï¼ˆUTF-8ï¼‰ï¼š
{{"intents":[...], "companies":[...]}}

ã€ä»»å‹™ã€‘
- åƒ…åœ¨**æ˜ç¢º**è¦æ±‚ã€Œç‰¹å®šä¸Šå¸‚å…¬å¸/ä»£è™Ÿçš„å³æ™‚æˆ–æ­·å²**è‚¡åƒ¹è³‡è¨Š**ã€æ™‚ï¼Œæ‰æŠŠ "stock" æ”¾å…¥ intentsã€‚
- åªè¦å•åˆ°è²¡å ±/ä¸‰å¤§è¡¨/ç‡Ÿæ”¶/EPS/æ¯›åˆ©ç‡/å±•æœ›/guidance/segmentï¼Œå°±æŠŠ "financial_report" æ”¾å…¥ intentsã€‚
- å•ã€Œæ–°è/æ›´æ–°/å ±å°ã€æ™‚ï¼Œå¯åŠ å…¥ "news"ã€‚

ã€å…¬å¸æŠ½å–ï¼ˆæ¥µåº¦åš´æ ¼ï¼‰ã€‘
- "companies" åƒ…å…è¨±ï¼š
  1) **å–®ä¸€å…¬å¸ä¸­æ–‡å**ï¼ˆä¾‹å¦‚ï¼šå°ç©é›»ã€å¾®è»Ÿã€ç¾å…‰ã€è¼é”ï¼‰
  2) **å…¬å¸è‹±æ–‡å**ï¼ˆä¾‹å¦‚ï¼šTSMCã€Microsoftã€Micronã€NVIDIAã€Appleï¼‰
  3) **è‚¡ç¥¨ä»£è™Ÿ**ï¼ˆä¾‹å¦‚ï¼š2330ã€2330.TWã€AAPLã€MSFTï¼‰
- ä¸å¾—åŒ…å«ä»»ä½•åŠ©è©ï¼ˆçš„/ä¹‹â€¦ï¼‰ã€ç©ºæ ¼å¥å­ã€æ¨™é»å°¾ç¶´æˆ–æ•´æ®µè©±ã€‚
- æ¯å€‹é …ç›®é•·åº¦ 2~20 å­—å…ƒï¼Œä¸”ä¸åŒ…å«ç©ºç™½ã€‚
- ç„¡æ³•ç¢ºå®šå°±è¼¸å‡ºç©ºé™£åˆ— []ã€‚

ã€æ’é™¤è¦å‰‡ã€‘
- å®è§€åè©ï¼ˆOPECã€Fedã€CPIã€åŸæ²¹ã€é»ƒé‡‘â€¦ï¼‰è‹¥æœªå•å€‹è‚¡æˆ–è²¡å ±ï¼Œintents çš†ç•™ç©ºã€‚
- çµ„ç¹”/å•†å“/ç”¢æ¥­åç¨±ä¸ç®—å…¬å¸ã€‚

ã€åš´æ ¼æ ¼å¼è¦æ±‚ã€‘
- åƒ…è¼¸å‡ºä¸€è¡Œ JSONï¼Œéµåå›ºå®šç‚º "intents" èˆ‡ "companies"ï¼Œä¸å¾—åŒ…å«å…¶ä»–æ¬„ä½æˆ–èªªæ˜æ–‡å­—ã€‚
- ä¾‹ï¼ˆæ­£ç¢ºï¼‰ï¼š
  {{"intents":["financial_report"], "companies":["å¾®è»Ÿ","MSFT"]}}
- ä¾‹ï¼ˆéŒ¯èª¤ï¼Œæœƒè¢«åˆ¤å®šç‚ºç„¡æ•ˆï¼‰ï¼š{{"intents":["financial_report"], "companies":["å¯ä»¥çµ¦æˆ‘ å¾®è»Ÿ çš„è²¡å ±é‡é»å—"]}}

è¼¸å…¥ï¼š{text}
"""
)


# === æ–°å¢ï¼šå¹´ä»½åˆ¤æ–·å°å·¥å…· ===
_YEAR_PAT = re.compile(r"\b(19\d{2}|20\d{2})\b")

def looks_like_year(token: str, text: str) -> bool:
    if not re.fullmatch(r"\d{4}", token):
        return False
    y = int(token)
    if 1900 <= y <= 2099:
        # å°±ç®—æ²’ä¸Šä¸‹æ–‡ï¼Œå››ç¢¼å¤šåŠæ˜¯å¹´ä»½ï¼šå…ˆåˆ¤ True
        return True
    return False

# 3) å…¬å¸æ¸…å–®æ­£è¦åŒ–
def normalize_companies(companies_raw: Any, *, original_text: str = "") -> List[str]:
    if not companies_raw:
        return []
    out: List[str] = []

    def _extract_one(x: Any) -> Optional[str]:
        if isinstance(x, str):
            return clean_company_token(x, original_text=original_text)
        if isinstance(x, dict):
            for k in ("symbol", "ticker", "code", "name", "company", "company_name", "title"):
                v = x.get(k)
                if isinstance(v, str):
                    t = clean_company_token(v, original_text=original_text)
                    if t:
                        return t
        return None

    for item in (companies_raw or []):
        v = _extract_one(item)
        if v:
            out.append(v)

    # å»é‡
    seen, uniq = set(), []
    for s in out:
        if s not in seen:
            seen.add(s)
            uniq.append(s)
    return uniq[:3]

# 4) ä¸­æ–‡å¥å‹èˆ‡ä¸­æ–‡ç‰‡æ®µè¼”åŠ©ï¼ˆä¿ç•™ä½ åŸæœ¬çš„å¼·éŸŒå‚™æ´ï¼‰
_CJK_HINTS = ("è‚¡åƒ¹","å ±åƒ¹","æ–°è","å¿«è¨Š","ç›®æ¨™åƒ¹","è²¡å ±","æ³•èªª","æ³•èªªæœƒ","ç‡Ÿæ”¶","å…¬å‘Š","é‡è¨Š","èµ°å‹¢")
_CJK_STOP = {"è‚¡åƒ¹","å ±åƒ¹","æ–°è","å¿«è¨Š","ç›®æ¨™åƒ¹","è²¡å ±","æ³•èªª","ç‡Ÿæ”¶","å…¬å‘Š","é‡è¨Š",
             "å…¬å¸","è‚¡ä»½","æœ‰é™","æ§è‚¡","é›†åœ˜","ETF","åŸºé‡‘","æŒ‡æ•¸"}
_CJK_STOP |= {"è‚¡ç¥¨","å“ªæ”¯","å“ªæª”","å“ªå®¶","å“ªé–“","å“ªå€‹","å“ªä¸€æ”¯","å“ªä¸€æª”","ä»€éº¼","æ˜¯ä»€éº¼"}
_CJK_STOP |= {"æ˜¯ä»€éº¼", "æ˜¯ä»€éº¼å—", "ä»€éº¼æ˜¯", "ä»‹ç´¹", "æ¦‚è¿°", "ç°¡ä»‹", "å®šç¾©", "åŸç†", "è«‹å•", "å¯ä»¥", "æ˜¯å¦", "å—", "å‘¢"}

def guess_companies_from_text(text: str, limit: int = 3) -> List[str]:
    text = (text or "").strip()

    cand: List[str] = []

    # å…è¨±å…¬å¸åèˆ‡é—œéµè©ä¹‹é–“æœ‰ç©ºç™½ï¼›ä¸”è‹¥å…¬å¸åå‰é¢æœ‰å¹´ä»½ï¼Œå„ªå…ˆå–å…¬å¸åæœ¬èº«
    # ä¾‹å¦‚ï¼šã€Œ2025 å¾®è»Ÿ çš„è²¡å ±ã€ã€Œå¾®è»Ÿ è²¡å ±ã€ã€Œå°ç©é›»çš„è²¡å ±ã€
    hints = "|".join(_CJK_HINTS)
    pat = rf"(?:^|[\sï¼ˆ(,ï¼Œ])(?:19\d{{2}}|20\d{{2}})?\s*([\u4e00-\u9fffA-Za-z0-9\.\-]{{2,18}})\s*(?:çš„)?(?:{hints})"
    m = re.search(pat, text)
    if m:
        name = m.group(1)
        # ç§»é™¤èª¤æŠ“åˆ°çš„å‰å°å››ç¢¼å¹´ä»½ï¼ˆæ¥µå°‘æ•¸æœƒè²¼åœ¨ä¸€èµ·ï¼‰
        name = re.sub(r"^(?:19\d{2}|20\d{2})", "", name).strip()
        if name:
            cand.append(name)

    # åŸæœ¬çš„è£œå¼·
    cand += re.findall(r"\b\d{4}\b", text)
    cand += re.findall(r"\b[A-Za-z]{1,6}(?:\.[A-Za-z]{2,4})?\b", text)

    # ä¸­æ–‡ç‰‡æ®µå‚™æ´ï¼ˆå»æ‰ stop è©ï¼‰
    chunks = re.findall(r"[\u4e00-\u9fff]{2,8}", text)
    chunks = [c for c in chunks if c not in _CJK_STOP]
    head = cand[0] if cand and re.search(r"[\u4e00-\u9fff]", cand[0]) else None
    ordered = ([head] if head else []) + [c for c in chunks if c and c != head]

    out: List[str] = []
    seen: set = set()
    for token in cand + ordered:
        if not token or token in seen:
            continue
        seen.add(token)
        if re.fullmatch(r"\d{4}", token):
            # å¹´ä»½ä¸è¦
            continue
        if re.fullmatch(r"[A-Za-z]{1,6}(?:\.[A-Za-z]{2,4})?", token):
            out.append(token)
        else:
            if 2 <= len(token) <= 18:
                out.append(token)
        if len(out) >= limit:
            break

    uniq, seen2 = [], set()
    for x in out:
        if x not in seen2:
            seen2.add(x); uniq.append(x)
    return uniq[:limit]

# 5) æœ€çµ‚å°å¤–ï¼šæ„åœ–ï¼‹å…¬å¸ï¼ˆé—œéµå­— âˆª LLMï¼›å…¬å¸æ­£è¦åŒ–ï¼‹å‚™æ´ï¼‰
TICKER_RE = re.compile(
    r"(\$[A-Za-z]{1,5}\b|\b[A-Z]{1,5}\.[A-Z]{2,4}\b|\b\d{4}\.[A-Za-z]{2,4}\b)"
)
PRICE_RE = re.compile(
    r"(è‚¡[\s\u200B-\u200D\uFEFF]*åƒ¹|å ±[\s\u200B-\u200D\uFEFF]*åƒ¹|ç¾[\s\u200B-\u200D\uFEFF]*åƒ¹|"
    r"æ”¶[\s\u200B-\u200D\uFEFF]*ç›¤|é–‹[\s\u200B-\u200D\uFEFF]*ç›¤|ç›®æ¨™[\s\u200B-\u200D\uFEFF]*åƒ¹|"
    r"price|quote|target\s*price)",
    re.I
)
def classify_intents_and_companies(text: str) -> Tuple[List[str], List[str]]:
    kw_ints = keyword_intents(text)

    llm_ints, llm_companies = set(), []
    try:
        resp = classifier_llm.invoke(multi_intent_template.format(text=text)).content.strip()
        info = extract_json_from_text(resp) or {}
        llm_ints = set(info.get("intents", []) or [])
        llm_companies = normalize_companies(info.get("companies", []), original_text=text)
    except Exception:
        pass

    final_intents = set()
    if "stock" in kw_ints:
        final_intents.add("stock")
    else:
        if ("stock" in llm_ints) and TICKER_RE.search(text):
            final_intents.add("stock")
        companies_probe = llm_companies or re.findall(r"\b[A-Za-z]{1,6}(?:\.[A-Za-z]{2,4})?\b", text) or re.findall(r"\b\d{4}\b", text)
        if "stock" not in final_intents and (companies_probe or guess_companies_from_text(text)) and PRICE_RE.search(text):
            final_intents.add("stock")

    if ("financial_report" in kw_ints) or ("financial_report" in llm_ints):
        final_intents.add("financial_report")

    # æ–°èæ„åœ–ï¼šé—œéµå­—æˆ– LLM ä»»ä¸€å‘½ä¸­å³åŠ å…¥
    if ("news" in kw_ints) or ("news" in llm_ints):
        final_intents.add("news")

    # âœ… é€™è£¡æ”¹æˆã€Œç¸½æ˜¯ä½µå…¥ã€è¦å‰‡æŠ½å–çš„çµæœ
    companies: List[str] = []
    companies += llm_companies
    companies += re.findall(r"\b[A-Za-z]{1,6}(?:\.[A-Za-z]{2,4})?\b", text)
    companies += re.findall(r"\b\d{4}\b", text)
    companies += guess_companies_from_text(text, limit=3)   # << æ–°å¢ï¼šæ°¸é  union

    cleaned = []
    seen = set()
    for c in (llm_companies +
              re.findall(r"\b[A-Za-z]{1,6}(?:\.[A-Za-z]{2,4})?\b", text) +
              re.findall(r"\b\d{4}\b", text) +
              guess_companies_from_text(text, limit=3)):
        cc = clean_company_token(c, original_text=text)
        if not cc:
            continue
        if re.fullmatch(r"\d{4}", cc) and looks_like_year(cc, text):
            continue  # 2024 é€™ç¨®å¹´ä»½è¸¢æ‰
        key = _normalize_name(re.sub(r"(?:çš„|ä¹‹)?(?:è‚¡ç¥¨ä»£è™Ÿ|è‚¡ç¥¨|å€‹è‚¡|æœ¬è‚¡|è‚¡|ä»£è™Ÿ|ä»£ç¢¼)+$", "", cc))
        if key not in seen:
            seen.add(key)
            cleaned.append(cc if cc else key)

    if re.search(r"(æ˜¯ä»€éº¼|ä»€éº¼æ˜¯|ä»‹ç´¹|æ¦‚è¿°|ç§‘æ™®|å…¥é–€|å®šç¾©|åŸç†|overview|what\s+is)", text, re.I):
        final_intents.add("concept")
    # é¡å¤–ï¼šæ¶‰åŠã€Œç”¢æ¥­/è¡Œæ¥­/ä¾›æ‡‰éˆ/ç”Ÿæ…‹ç³»/åƒ¹å€¼éˆã€è¦–ç‚ºç”¢æ¥­æ¦‚å¿µ
    if re.search(r"(ç”¢æ¥­|è¡Œæ¥­|ä¾›æ‡‰éˆ|ç”Ÿæ…‹ç³»|åƒ¹å€¼éˆ|å¸‚å ´)", text):
        final_intents.add("industry")

    return list(final_intents), cleaned[:3]

# =========================
# æœˆä»½åµæ¸¬
# =========================
def chinese_month_to_int(s: str) -> Optional[int]:
    s = (s or "").strip()
    if s in CHI_NUM:
        v = CHI_NUM[s]
        return v if 1 <= v <= 12 else None
    if s.startswith("å") and len(s) == 2 and s[1] in CHI_NUM:
        v = 10 + CHI_NUM[s[1]]; return v if 1 <= v <= 12 else None
    if s.endswith("å") and len(s) == 2 and s[0] in CHI_NUM:
        v = CHI_NUM[s[0]] + 10; return v if 1 <= v <= 12 else None
    return None

def extract_date_range_from_text(text: str) -> Optional[Tuple[datetime, datetime]]:
    t = (text or "").strip()
    m_zh = re.search(r"(\d{4})\s*å¹´\s*([0-9]{1,2}|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åã€‡â—‹ï¼¯]{1,3})\s*æœˆ", t)
    if m_zh:
        y = int(m_zh.group(1)); raw = m_zh.group(2)
        mo = int(raw) if raw.isdigit() else chinese_month_to_int(raw)
        if mo and 1<=mo<=12:
            start = datetime(y, mo, 1)
            return start, start + relativedelta(months=1)
    m = re.search(r"(\d{4})[/-](\d{1,2})[/-](\d{1,2})", t)
    if m:
        y, mo, d = map(int, m.groups())
        start = datetime(y, mo, d); return start, start + timedelta(days=1)
    m2 = re.search(r"(\d{4})[/-](\d{1,2})", t)
    if m2:
        y, mo = map(int, m2.groups())
        start = datetime(y, mo, 1); return start, start + relativedelta(months=1)
    if not re.search(r"(å»å¹´|ä»Šå¹´|æ˜å¹´)", t):
        m3 = re.search(r"(^|[^å¹´])([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åã€‡â—‹ï¼¯]{1,3}|[0-9]{1,2})\s*æœˆ", t)
        if m3:
            raw = m3.group(2)
            mo = int(raw) if raw.isdigit() else chinese_month_to_int(raw)
            if mo and 1<=mo<=12:
                y = datetime.now().year
                start = datetime(y, mo, 1); return start, start + relativedelta(months=1)
    return None

def resolve_relative_span(text: str, now: Optional[datetime] = None) -> Optional[Tuple[datetime, datetime]]:
    if not text: return None
    now = now or datetime.now()
    t = re.sub(r"\s+", "", text)
    m3 = re.search(r"(ä»Šå¹´|å»å¹´|æ˜å¹´)\s*(\d{1,2}|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åã€‡â—‹ï¼¯]{1,3})\s*æœˆ", t)
    if m3:
        base = now.year if m3.group(1)=="ä»Šå¹´" else (now.year-1 if m3.group(1)=="å»å¹´" else now.year+1)
        raw = m3.group(2); mo = int(raw) if raw.isdigit() else chinese_month_to_int(raw)
        if mo and 1<=mo<=12:
            start = datetime(base, mo, 1); return start, start + relativedelta(months=1)
    if "æœ¬æœˆ" in t or "é€™å€‹æœˆ" in t:
        start = now.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
        return start, start + relativedelta(months=1)
    if "ä¸Šæœˆ" in t or "ä¸Šå€‹æœˆ" in t:
        start = (now.replace(day=1) - relativedelta(months=1))
        return start, start + relativedelta(months=1)
    if "ä¸‹æœˆ" in t or "ä¸‹å€‹æœˆ" in t:
        start = (now.replace(day=1) + relativedelta(months=1))
        return start, start + relativedelta(months=1)
    return None

def is_whole_month_query(user_text: str) -> Tuple[bool, Optional[Tuple[datetime, datetime]]]:
    rel = resolve_relative_span(user_text)
    absr = extract_date_range_from_text(user_text)
    rng = rel or absr
    if not rng: return False, None
    start, end = rng
    is_full = (start.day==1) and ((end-start).days in (28,29,30,31))
    return (True, (start, end)) if is_full else (False, None)


# =========================
# æ•´æœˆæƒæèˆ‡æ‘˜è¦
# =========================
def scan_docs_by_month(year: int, month: int) -> List[Any]:
    pools: List[Any] = []
    for vs in [vectorstore_zh, vectorstore_en]:
        store = getattr(vs, "docstore", None)
        if not store or not hasattr(store, "_dict"): continue
        for d in store._dict.values():
            md = d.metadata or {}
            dt = get_doc_date_dt(md)
            if dt and dt.year==year and dt.month==month:
                pools.append(d)
    pools.sort(key=lambda x: get_doc_date_dt(x.metadata or {}) or datetime.min, reverse=True)
    return pools

def pick_representative_chunks_per_doc(chunks: List[Any], k: int = 1) -> List[Any]:
    prefer_kw = ("é‡é»","æ‘˜è¦","çµè«–","è¦é»","takeaways")
    scored = []
    for ch in chunks:
        txt = (ch.page_content or "")
        score = sum(1 for kw in prefer_kw if kw.lower() in txt.lower())*3
        L = len(txt)
        score += 2 if 300<=L<=1200 else 0
        try:
            m = re.search(r"(\d+)$", (ch.metadata or {}).get("chunk_id", ""))
            cid = int(m.group(1)) if m else 9999
        except Exception:
            cid = 9999
        score -= cid*0.001
        scored.append((score, ch))
    scored.sort(key=lambda x: x[0], reverse=True)
    return [c for _, c in scored[:max(1,int(k))]]

month_digest_prompt = PromptTemplate.from_template(
    """
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„è²¡ç¶“èˆ‡ç”¢æ¥­åˆ†æé¡§å•ã€‚ä»¥ä¸‹æ˜¯ã€ŒæŸæœˆä»½å…§çš„å¤šç¯‡æ–‡ç« ä»£è¡¨æ®µè½ã€ï¼Œ
è«‹**é€ç¯‡**æ•´ç† 1â€“2 å¥é‡é»ï¼ˆä¸å¯è·¨æ–‡æ··å¯«ï¼‰ï¼Œæ¯é»**æœ«å°¾é™„ä¾†æºæ¨™ç±¤èˆ‡æ—¥æœŸ**ï¼ˆæ ¼å¼ï¼šã€S#ï½œYYYY-MM-DDã€‘ï¼‰ï¼Œ**ç¹é«”ä¸­æ–‡**è¼¸å‡ºã€‚
---------
{context}
---------
å•é¡Œï¼š{question}
"""
)

month_digest_prompt_beg = PromptTemplate.from_template(
    """
ä½ æ˜¯ä¸€ä½è²¡ç¶“ç§‘æ™®è¬›è§£å“¡ã€‚å…ˆè¼¸å‡ºã€ŒğŸ”° åè©å°è¾­å…¸ã€ï¼šå¾ä¸‹åˆ—ä»£è¡¨æ®µè½æŒ‘é¸ 3â€“7 å€‹å°ˆæœ‰åè©ï¼Œ
ä»¥ã€Œ- è©ï¼š10â€“25 å­—ç™½è©±è§£é‡‹ã€å®šç¾©ã€‚
æ¥è‘—è¼¸å‡ºã€ŒğŸ—“ï¸ æœˆä»½ç¸½çµã€â€”â€”**é€ç¯‡**æ•´ç† 1â€“2 å¥é‡é»ï¼ˆä¸å¯è·¨æ–‡æ··å¯«ï¼‰ï¼Œæ¯é»æœ«å°¾é™„ä¾†æºæ¨™ç±¤èˆ‡æ—¥æœŸï¼ˆæ ¼å¼ï¼šã€S#ï½œYYYY-MM-DDã€‘ï¼‰ï¼Œä»¥ç¹é«”ä¸­æ–‡è¼¸å‡ºã€‚
---------
{context}
---------
å•é¡Œï¼š{question}
"""
)

month_digest_prompt_pro = PromptTemplate.from_template(
    """
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„è²¡ç¶“èˆ‡ç”¢æ¥­åˆ†æé¡§å•ã€‚ä»¥ä¸‹æ˜¯ã€ŒæŸæœˆä»½å…§çš„å¤šç¯‡æ–‡ç« ä»£è¡¨æ®µè½ã€ï¼Œ
è«‹**é€ç¯‡**æ•´ç† 1â€“2 å¥é‡é»ï¼ˆä¸å¯è·¨æ–‡æ··å¯«ï¼‰ï¼Œæ¯é»**æœ«å°¾é™„ä¾†æºæ¨™ç±¤èˆ‡æ—¥æœŸ**ï¼ˆæ ¼å¼ï¼šã€S#ï½œYYYY-MM-DDã€‘ï¼‰ï¼Œ**ç¹é«”ä¸­æ–‡**è¼¸å‡ºã€‚
ï¼ˆè‹¥å¯èƒ½ï¼Œå¼·èª¿çµè«–èˆ‡ä¾æ“šçš„å°æ‡‰é—œä¿‚ï¼‰
---------
{context}
---------
å•é¡Œï¼š{question}
"""
)


def _clip_text(s: str, n: int) -> str:
    s = (s or "").strip()
    return s if len(s) <= n else (s[:n] + " â€¦")

def _batch_by_char_limit(blocks: list[str], limit: int) -> list[list[str]]:
    batches, cur, cur_len = [], [], 0
    sep = "\n\n---\n\n"
    for b in blocks:
        bb = b if len(b) <= limit else b[: int(limit * 0.9)]
        extra = (len(sep) if cur else 0) + len(bb)
        if cur and (cur_len + extra > limit):
            batches.append(cur); cur, cur_len = [], 0
        cur.append(bb); cur_len += (len(sep) if cur_len > 0 else 0) + len(bb)
    if cur:
        batches.append(cur)
    return batches

def _renumber_bullets(text: str) -> str:
    out_lines, idx = [], 1
    for line in (text or "").splitlines():
        l = line.strip()
        if re.match(r"^\s*\d+\.\s+", l):
            l = re.sub(r"^\s*\d+\.\s+", "", l)
            out_lines.append(f"{idx}. {l}")
            idx += 1
        else:
            out_lines.append(line)
    return "\n".join(out_lines)


def run_month_digest(
    user_input: str,
    start: datetime,
    end: datetime,
    *,
    per_doc_k: int = 1,
    max_docs: int = 12,
    candidates: Optional[List[Any]] = None,
    mode: str = "åˆå­¸è€…æ¨¡å¼",
    custom_prompt: str = "",
) -> Tuple[str, str]:
    """
    ç”¢ç”Ÿã€Œæ•´æœˆæ‘˜è¦ã€ã€‚å›å‚³ (answer_md, sources_md)
    - answer_mdï¼šæœ€çµ‚å›ç­”ï¼ˆå« ğŸ”°åè©å°è¾­å…¸ / ğŸ—“ï¸æœˆä»½ç¸½çµ å–æ±ºæ–¼ modeï¼‰
    - sources_mdï¼šå°æ‡‰ä¾†æºæ¸…å–®ï¼ˆS1/S2â€¦ï¼Œå«é ç¢¼å€é–“ï¼‰
    """
    CLIP_PER_BLOCK = 600          # æ¯å€‹ä»£è¡¨æ®µè½çš„æœ€å¤§å­—å…ƒæ•¸ï¼ˆé¿å… prompt éé•·ï¼‰
    BATCH_CHAR_LIMIT = 3600       # æ¯ä¸€æ‰¹é€ LLM çš„ä¸Šé™ï¼ˆç²—ä¼°ï¼Œå¯è¦–æ¨¡å‹èª¿æ•´ï¼‰
    SEP = "\n\n---\n\n"

    # 1) å–å¾—æœ¬æœˆå€™é¸ chunksï¼ˆè‹¥å¤–éƒ¨æ²’å‚³ candidates å°±æƒæ•´æœˆï¼‰
    if candidates is None:
        y, m = start.year, start.month
        month_chunks = scan_docs_by_month(y, m)
        if not month_chunks:
            return "åƒ…èƒ½éƒ¨åˆ†å›ç­”ï¼šè©²æœˆä»½å…§æŸ¥ç„¡æ–‡ç« ã€‚", ""
        used_chunks = month_chunks
    else:
        used_chunks = candidates or []
        if not used_chunks:
            y, m = start.year, start.month
            month_chunks = scan_docs_by_month(y, m)
            if not month_chunks:
                return "åƒ…èƒ½éƒ¨åˆ†å›ç­”ï¼šè©²æœˆä»½å…§æŸ¥ç„¡æ–‡ç« ã€‚", ""
            used_chunks = month_chunks

    # 2) ä¾æ–‡ä»¶åˆ†çµ„
    groups: Dict[Any, List[Any]] = {}
    for ch in used_chunks:
        md = ch.metadata or {}
        key = md.get("doc_id") or (md.get("source"), md.get("title"))
        groups.setdefault(key, []).append(ch)

    def _safe_dt(md):
        dt = get_doc_date_dt(md or {})
        # è‹¥ç„¡æ™‚å€ï¼Œè£œ UTCï¼Œç¢ºä¿å¯æ’åº
        if dt and not dt.tzinfo:
            dt = dt.replace(tzinfo=tz.tzutc())
        return dt or datetime.min.replace(tzinfo=tz.tzutc())

    # 3) å–æœ€æ–°çš„å‰ max_docs ä»½æ–‡ä»¶ï¼ˆæ¯ä»½æ–‡ä»¶å…§å†æŒ‘ per_doc_k å€‹ä»£è¡¨æ®µè½ï¼‰
    #    ä»¥ã€Œè©²ä»½æ–‡ä»¶ç¬¬ä¸€å€‹ chunk çš„æ—¥æœŸã€è¿‘â†’é æ’åºï¼ˆç²—ç•¥ä½†å¯¦ç”¨ï¼‰
    doc_keys_sorted = sorted(
        groups.keys(),
        key=lambda k: _safe_dt((groups[k][0].metadata or {})),
        reverse=True,
    )[:max_docs]

    # 4) ç‚ºæ¯ä»½æ–‡ä»¶æŒ‘ä»£è¡¨æ®µè½ï¼Œä¸¦å»ºç«‹ã€Œå¸¶ S æ¨™ç±¤ã€æ—¥æœŸã€é ç¢¼ã€çš„ context blocks
    picked_chunks: List[Any] = []
    for doc_key in doc_keys_sorted:
        reps = pick_representative_chunks_per_doc(groups.get(doc_key) or [], k=max(1, int(per_doc_k)))
        picked_chunks.extend(reps)

    if not picked_chunks:
        return "åƒ…èƒ½éƒ¨åˆ†å›ç­”ï¼šè©²æœˆä»½å…§æŸ¥ç„¡å¯ç”¨æ®µè½ã€‚", ""

    # å»ºç«‹ã€Œå¯è¢« LLM å¼•ç”¨ã€çš„ contextï¼ˆå« S#ï½œTitleï½œp. ç­‰é ­ï¼‰
    ctx_full, src_map = build_cited_context(picked_chunks)

    # å°‡ context ä¾é•·åº¦åˆ‡æ‰¹ï¼Œé¿å…è¶…éæ¨¡å‹é•·åº¦
    blocks = [b.strip() for b in ctx_full.split(SEP) if b.strip()]
    # clip éé•· block
    blocks = [_clip_text(b, CLIP_PER_BLOCK) for b in blocks]
    batches = _batch_by_char_limit(blocks, limit=BATCH_CHAR_LIMIT)

    # 5) ä¾ mode é¸ç”¨ promptï¼ˆå®¢è£½åŒ–æ¨¡å¼ä½¿ç”¨è‡ªè¨‚ promptï¼‰
    use_custom = bool(custom_prompt and ("å®¢è£½" in (mode or "")))
    if not use_custom:
        if "å°ˆå®¶" in (mode or ""):
            tmpl = month_digest_prompt_pro
        elif "åˆå­¸" in (mode or ""):
            tmpl = month_digest_prompt_beg
        else:
            tmpl = month_digest_prompt  # ä¸€èˆ¬æ¨¡å¼ï¼šåªæœ‰æœˆä»½ç¸½çµï¼ˆç„¡åè©å°è¾­å…¸ï¼‰

    # 6) åˆ†æ‰¹ä¸Ÿçµ¦ LLMï¼Œå†æŠŠçµæœæ¥èµ·ä¾†
    answers: List[str] = []
    for bs in batches:
        part_ctx = SEP.join(bs)
        try:
            if use_custom:
                # å®¢è£½åŒ–ï¼šè‹¥åŒ…å«å ä½ç¬¦å‰‡ formatï¼Œå¦å‰‡æŠŠ context èˆ‡ question æ¥åœ¨æŒ‡ä»¤å¾Œ
                cp = (custom_prompt or "").strip()
                if ("{context}" in cp) or ("{question}" in cp):
                    prompt_text = cp.format(context=part_ctx, question=user_input)
                else:
                    prompt_text = f"{cp}\n---------\n{part_ctx}\n---------\nå•é¡Œï¼š{user_input}"
                resp = llm.invoke(prompt_text).content.strip()
            else:
                resp = llm.invoke(tmpl.format(context=part_ctx, question=user_input)).content.strip()
        except Exception as e:
            resp = f"ï¼ˆæœ¬æ‰¹æ‘˜è¦å¤±æ•—ï¼š{e}ï¼‰"
        answers.append(resp)

    final_answer = "\n\n".join(answers)
    final_answer = _renumber_bullets(final_answer).strip()

    # 7) æ¸²æŸ“ä¾†æºæ¸…å–®ï¼ˆå«é ç¢¼ç¯„åœï¼‰ä¸¦åœ¨æ¯æ¢åˆ—æœ«å°¾è£œä¾†æºé€£çµ
    final_answer = annotate_bullet_sources(final_answer, src_map)
    sources_md = render_sources_md(src_map)
    return final_answer, sources_md

# =========================
# ä¸»æµç¨‹ï¼ˆå«æœˆä»½æ¨¡å¼ & é¡åˆ¥åå¥½ + æ–° Yahooï¼‰
# =========================
def handle_question(user_q: str, top_n: int, top_k: int,
                    blog_scale: float, pdf_scale: float, research_scale: float, summary_scale: float,
                    filing_scale: float,
                    mode: str, gen_model_choice: str,
                    custom_prompt: str = ""):
    # è¨ˆæ™‚å·¥å…·ï¼ˆæ¯«ç§’ï¼‰
    t0 = time.perf_counter()
    t_prev = t0
    times: dict[str, float] = {}
    def _lap(name: str):
        nonlocal t_prev
        now = time.perf_counter()
        times[name] = now - t_prev
        t_prev = now
    def _fmt_times(order: list[str]) -> str:
        parts = []
        for k in order:
            if k in times:
                parts.append(f"{k}:{int(times[k]*1000)}ms")
        parts.append(f"total:{int((time.perf_counter()-t0)*1000)}ms")
        return "[TIMES] " + " | ".join(parts)

    # é¡åˆ¥å€ç‡é¡¯ç¤ºå·¥å…·ï¼ˆå›ºå®šé †åºã€é¿å… KeyErrorï¼‰
    def _fmt_type_scales(ts: dict) -> str:
        order = ("blog", "pdf", "research", "transcript", "filing", "other")
        return ", ".join(f"{key}:{ts.get(key, 1.0):.2f}" for key in order)

    user_q = (user_q or "").strip()
    if not user_q:
        return gr.update(), gr.update(), gr.update(), gr.update(), gr.update(), "â€”", None

    effective_gen = set_gen_llm(gen_model_choice)
    _lap("set_llm")

    # === é¡åˆ¥å€ç‡ ===
    type_scales = {
        "blog":       float(blog_scale),
        "pdf":        float(pdf_scale),
        "research":   float(research_scale),
        "transcript": float(summary_scale),
        "filing":     float(filing_scale),
        "other":      1.0,
    }

    # === å•é¡Œè§£æ ===
    is_month, month_rng = is_whole_month_query(user_q)
    intents, companies = classify_intents_and_companies(user_q)
    intents_set = set(intents)
    requested_years = extract_report_year_targets(user_q) if ("financial_report" in intents) else set()
    wants_latest = bool(re.search(r"(æœ€æ–°|æœ€è¿‘|è¿‘æ³|æœªä¾†)", user_q))
    _lap("parse")

    if ("concept" in intents_set) or ("industry" in intents_set):
        type_scales = {**type_scales}
        type_scales["blog"] = max(type_scales.get("blog", 1.0), 1.5)
        type_scales["research"] = max(type_scales.get("research", 1.0), 1.0)
        type_scales["transcript"] = min(type_scales.get("transcript", 1.0), 0.6)
        type_scales["filing"] = min(type_scales.get("filing", 1.0), 0.3)

    # è‹¥æ˜¯è²¡å ±ä¸”æœ‰æŒ‡å®šå…¬å¸ï¼Œä½†ä½¿ç”¨è€…æŠŠ filing æ‹‰åˆ° 0ï¼Œé¿å… prefilter æ¸…ç©º
    if ("financial_report" in intents_set) and companies and type_scales.get("filing", 0.0) <= 0.0:
        type_scales["filing"] = 0.1

    # æº–å‚™ï¼šè‹¥åŒ…å«è‚¡åƒ¹/æ–°èæ„åœ–ï¼Œå…ˆç”¢ç”Ÿå¯å‰ç½®çš„å€å¡Š
    lead_parts = []
    stock_md_pref = ""
    news_md_pref = ""
    symbol_cache_local: Dict[str, Optional[str]] = {}
    quote_cache_local: Dict[str, Optional[List[dict]]] = {}
    chart_cache_local: Dict[str, Optional[dict]] = {}
    news_cache_local: Dict[Tuple[Optional[str], Optional[str]], List[dict]] = {}
    need_stock = "stock" in intents_set
    need_news = "news" in intents_set
    if need_stock or need_news:
        with ThreadPoolExecutor(max_workers=2) as pool:
            futures: Dict[str, Any] = {}
            if need_stock:
                futures["stock"] = pool.submit(
                    _prepare_stock_md,
                    user_q,
                    intents_set,
                    companies,
                    symbol_cache=symbol_cache_local,
                    quote_cache=quote_cache_local,
                    chart_cache=chart_cache_local,
                )
            if need_news:
                futures["news"] = pool.submit(
                    _prepare_news_md,
                    intents_set,
                    companies,
                    user_q,
                    symbol_cache=symbol_cache_local,
                    news_cache=news_cache_local,
                )
        if need_stock:
            stock_md_pref = futures["stock"].result()
        if need_news:
            news_md_pref = futures["news"].result()
    if not (need_stock or need_news):
        stock_md_pref = _prepare_stock_md(
            user_q,
            intents_set,
            companies,
            symbol_cache=symbol_cache_local,
            quote_cache=quote_cache_local,
            chart_cache=chart_cache_local,
        )
        news_md_pref = _prepare_news_md(
            intents_set,
            companies,
            user_q,
            symbol_cache=symbol_cache_local,
            news_cache=news_cache_local,
        )
    if stock_md_pref:
        lead_parts.append("ğŸ“ˆ **è‚¡ç¥¨æŸ¥è©¢**\n" + stock_md_pref)
    if news_md_pref:
        lead_parts.append("ğŸ“° **å³æ™‚æ–°è**\n" + news_md_pref)
    _lap("yahoo_blocks")

    # === Yahoo-only çŸ­è·¯ ===
    if (("stock" in intents) or ("news" in intents)) and lead_parts:
        answer_md = "\n\n".join(lead_parts).strip()
        queries_md = "ï¼ˆæ­¤å•é¡Œè¢«åˆ¤å®šç‚ºã€è‚¡åƒ¹/æ–°èã€ï¼Œå·²ç›´æ¥ä½¿ç”¨ Yahoo è³‡è¨Šï¼‰"
        debug_txt = "[SHORT-CIRCUIT] yahoo_only\n" + _fmt_times(["set_llm", "parse", "yahoo_blocks"])
        dd_choices = [FOLLOWUP_PLACEHOLDER]
        meta = {
            "type": "yahoo_only",
            "question": user_q,
            "intents": list(intents),
            "companies": companies,
            "picked": 0,
        }
        return (
            answer_md,
            "",
            queries_md,
            debug_txt,
            gr.update(choices=dd_choices, value=dd_choices[0]),
            "â€”",
            meta,
        )

    # === (A) æœˆä»½æ¨¡å¼ ===
    if is_month and month_rng:
        start, end = month_rng

        # æª¢ç´¢ + è©³ç´°æ‰“é»
        docs, cos_scores, zh_qs, en_qs = similarity_search_vectors(
            user_q,
            k=max(12, top_n),
            type_scales=type_scales,
            prefilter=True,
            intents=intents_set,
            year_targets=requested_years,
            strict_year=True,
            companies=companies,
            lap=_lap,  # â† å‚³å…¥è¨ˆæ™‚ callback
        )
        _lap("retrieval")  # å¤–å±¤æ”¶å°¾

        # è©²æœˆéæ¿¾
        month_docs, cos_masked = [], []
        for d, sc in zip(docs, cos_scores):
            dt = get_doc_date_dt(d.metadata or {})
            if dt:
                dt_naive = dt.replace(tzinfo=None) if dt.tzinfo else dt
                if start <= dt_naive < end:
                    month_docs.append(d)
                    cos_masked.append(sc)
        _lap("month_filter")

        if month_docs:
            # ç”¢å‡ºæœˆä»½ç¸½çµ
            answer_md, sources_md = run_month_digest(
                user_input=user_q,
                start=start, end=end,
                per_doc_k=1, max_docs=min(12, max(6, top_n)),
                candidates=month_docs,
                mode=mode or "ä¸€èˆ¬æ¨¡å¼",
                custom_prompt=custom_prompt,
            )
            if lead_parts:
                answer_md = ("\n\n".join(lead_parts + [answer_md])).strip()

            # æŸ¥è©¢é¡¯ç¤º
            def _fmt_queries(zh, en):
                z = "\n".join(f"- {q}" for q in (zh or [])[:8]) or "ï¼ˆç„¡ï¼‰"
                e = "\n".join(f"- {q}" for q in (en or [])[:8]) or "ï¼ˆç„¡ï¼‰"
                return f"**ä¸­æ–‡æŸ¥è©¢**\n{z}\n\n**è‹±æ–‡æŸ¥è©¢**\n{e}"
            queries_md = _fmt_queries(zh_qs, en_qs)
            _lap("fmt_queries")

            # Debug
            metric_counts = {"has_num": 0, "revenue": 0, "eps": 0, "gm": 0}
            for d in month_docs:
                f = _metric_flags(d.page_content or "")
                for k in metric_counts:
                    metric_counts[k] += int(bool(f.get(k)))

            debug_txt = (
                f"[MODE] month_digest {start.strftime('%Y-%m')} ~ {end.strftime('%Y-%m-%d')}\n"
                f"[INTENTS] {', '.join(intents) if intents else 'â€”'} | [COMPANIES] {', '.join(companies) if companies else 'â€”'}\n"
                "[TYPES] " + _fmt_type_scales(type_scales) + "\n"
                f"[CANDIDATES] {len(month_docs)} | num:{metric_counts['has_num']} rev:{metric_counts['revenue']} eps:{metric_counts['eps']} gm:{metric_counts['gm']}\n"
                + _fmt_times([
                    "set_llm", "parse", "yahoo_blocks",
                    # å…§éƒ¨æª¢ç´¢ç¯€é»
                    "retrieval:start",
                    "retrieval:mk_base_queries",
                    "retrieval:merged_pins_base",
                    "retrieval:before_alias_expand", "retrieval:after_alias_expand",
                    "retrieval:queries_ready",
                    "retrieval:zh_search_start", "retrieval:zh_search_done",
                    "retrieval:en_search_start", "retrieval:en_search_done",
                    "retrieval:score_norm_start", "retrieval:score_norm_done",
                    "retrieval:pooling_start", "retrieval:pooling_done",
                    "retrieval:metadata_fallback_start", "retrieval:metadata_fallback_done",
                    "retrieval:done_ok", "retrieval:done_empty",
                    # å¤–å±¤
                    "retrieval", "month_filter", "fmt_queries"
                ])
            )

            followups = make_followups(user_q, answer_md, n_min=3, n_max=5)
            dd_choices = [FOLLOWUP_PLACEHOLDER] + followups
            month_state_msg = f"ğŸ—“ï¸ å·²æ•´ç† {start.strftime('%Y-%m')}ï¼š{len(month_docs)} ä»½æ–‡ä»¶"
            meta = {
                "type": "month_digest",
                "question": user_q,
                "start": start.strftime("%Y-%m-%d"),
                "end": end.strftime("%Y-%m-%d"),
                "intents": intents,
                "companies": companies,
            }
            return (
                answer_md,
                sources_md or "ï¼ˆæœ¬æœˆä¾†æºæ•´ç†ï¼‰",
                queries_md,
                debug_txt,
                gr.update(choices=dd_choices, value=dd_choices[0]),
                month_state_msg,
                meta,
            )
        else:
            # æœˆä»½ç„¡å€™é¸ â†’ fallback
            parts = []
            stock_md = _prepare_stock_md(
                user_q,
                intents_set,
                companies,
                symbol_cache=symbol_cache_local,
                quote_cache=quote_cache_local,
                chart_cache=chart_cache_local,
            )
            if stock_md:
                parts.append("ğŸ“ˆ **è‚¡ç¥¨æŸ¥è©¢**\n" + stock_md)
            news_md = _prepare_news_md(
                intents_set,
                companies,
                user_q,
                symbol_cache=symbol_cache_local,
                news_cache=news_cache_local,
            )
            if news_md:
                parts.append("ğŸ“° **å³æ™‚æ–°è**\n" + news_md)
            parts.append(f"è©²æœˆä»½ï¼ˆ{start.strftime('%Y-%m')}ï¼‰æœªæ‰¾åˆ°**èˆ‡æ­¤ä¸»é¡Œ**ç›¸é—œçš„æ–‡ç« æ®µè½ã€‚")
            final_out = "\n\n".join(parts)

            ctx_md = "ï¼ˆè©²æœˆæ²’æœ‰å¯å¼•ç”¨çš„æ®µè½ï¼Œè‹¥éœ€è¦ä»å¯é»é¸å»¶ä¼¸å•é¡Œç”¢ç”Ÿè©²æœˆæ•´é«”æ¦‚è¦½ï¼‰"
            debug_txt = (
                "[DEBUG] month_pending | " + start.strftime("%Y-%m") +
                f"\n[INTENTS] {', '.join(intents) if intents else 'â€”'}"
                f" | [COMPANIES] {', '.join(companies) if companies else 'â€”'}"
                "\n[TYPES] " + _fmt_type_scales(type_scales) +
                "\n" + _fmt_times([
                    "set_llm", "parse", "yahoo_blocks",
                    "retrieval:start",
                    "retrieval:mk_base_queries",
                    "retrieval:merged_pins_base",
                    "retrieval:before_alias_expand", "retrieval:after_alias_expand",
                    "retrieval:queries_ready",
                    "retrieval:zh_search_start", "retrieval:zh_search_done",
                    "retrieval:en_search_start", "retrieval:en_search_done",
                    "retrieval:score_norm_start", "retrieval:score_norm_done",
                    "retrieval:pooling_start", "retrieval:pooling_done",
                    "retrieval:metadata_fallback_start", "retrieval:metadata_fallback_done",
                    "retrieval:done_ok", "retrieval:done_empty",
                    "retrieval"
                ])
            )
            followups = [SPECIAL_OVERVIEW_LABEL] + make_followups(user_q, final_out, n_min=3, n_max=5)
            dd_choices = [FOLLOWUP_PLACEHOLDER] + (followups or [])
            queries_md = "ï¼ˆæœˆä»½æ¨¡å¼ä¸‹ä¸å±•ç¤º multi-queriesï¼‰"
            month_state_msg = f"ğŸ—“ï¸ {start.strftime('%Y-%m')}ï¼šæœªæ‰¾åˆ°ä¸»é¡Œæ®µè½ï¼Œå¯é»ã€Œ{SPECIAL_OVERVIEW_LABEL}ã€ç”¢å‡ºæ•´æœˆç¸½çµ"
            meta = {
                "type": "month_pending",
                "question": user_q,
                "start": start.strftime("%Y-%m-%d"),
                "end": end.strftime("%Y-%m-%d"),
                "intents": intents,
                "companies": companies,
            }
            return (
                final_out,
                ctx_md,
                queries_md,
                debug_txt,
                gr.update(choices=dd_choices, value=dd_choices[0]),
                month_state_msg,
                meta,
            )

    # === (B) ä¸€èˆ¬æ¨¡å¼ï¼ˆRAGï¼‰ ===
    docs, cos_scores, zh_qs, en_qs = similarity_search_vectors(
        user_q,
        k=max(12, top_n),
        type_scales=type_scales,
        prefilter=True,
        intents=intents_set,
        year_targets=requested_years,
        strict_year=True,
        companies=companies,
        lap=_lap,  # â† å‚³å…¥è¨ˆæ™‚ callback
    )
    _lap("retrieval")

    if not docs:
        # æ²’æœ‰å€™é¸ï¼šçµ¦è‚¡ç¥¨/æ–°è fallback
        parts = []
        stock_md = _prepare_stock_md(
            user_q,
            intents_set,
            companies,
            symbol_cache=symbol_cache_local,
            quote_cache=quote_cache_local,
            chart_cache=chart_cache_local,
        )
        if stock_md:
            parts.append("ğŸ“ˆ **è‚¡ç¥¨æŸ¥è©¢**\n" + stock_md)
        news_md = _prepare_news_md(
            intents_set,
            companies,
            user_q,
            symbol_cache=symbol_cache_local,
            news_cache=news_cache_local,
        )
        if news_md:
            parts.append("ğŸ“° **å³æ™‚æ–°è**\n" + news_md)
        parts.append("ç›®å‰æ‰¾ä¸åˆ°å¯ç›´æ¥å›ç­”çš„å…§å®¹ã€‚å¯å˜—è©¦ï¼šæ”¾å¯¬æ¢ä»¶ã€æ”¹å¯«é—œéµå­—ã€æˆ–æé«˜é¡åˆ¥å€ç‡ã€‚")
        answer_md = "\n\n".join(parts)

        zh_view = "\n".join(f"- {q}" for q in (zh_qs or [])[:8]) or "ï¼ˆç„¡ï¼‰"
        en_view = "\n".join(f"- {q}" for q in (en_qs or [])[:8]) or "ï¼ˆç„¡ï¼‰"
        queries_md = f"**ä¸­æ–‡æŸ¥è©¢**\n{zh_view}\n\n**è‹±æ–‡æŸ¥è©¢**\n{en_view}"
        _lap("fmt_queries")

        debug_txt = (
            "[DEBUG] no_docs\n"
            f"[INTENTS] {', '.join(intents) if intents else 'â€”'} | [COMPANIES] {', '.join(companies) if companies else 'â€”'}\n"
            "[TYPES] " + _fmt_type_scales(type_scales) + "\n"
            + _fmt_times([
                "set_llm", "parse", "yahoo_blocks",
                "retrieval:start",
                "retrieval:mk_base_queries",
                "retrieval:merged_pins_base",
                "retrieval:before_alias_expand", "retrieval:after_alias_expand",
                "retrieval:queries_ready",
                "retrieval:zh_search_start", "retrieval:zh_search_done",
                "retrieval:en_search_start", "retrieval:en_search_done",
                "retrieval:score_norm_start", "retrieval:score_norm_done",
                "retrieval:pooling_start", "retrieval:pooling_done",
                "retrieval:metadata_fallback_start", "retrieval:metadata_fallback_done",
                "retrieval:done_ok", "retrieval:done_empty",
                "retrieval", "fmt_queries"
            ])
        )
        followups = make_followups(user_q, answer_md, n_min=3, n_max=5)
        dd_choices = [FOLLOWUP_PLACEHOLDER] + followups
        meta = {
            "type": "no_docs",
            "question": user_q,
            "intents": intents,
            "companies": companies,
        }
        return (
            answer_md,
            "ï¼ˆç„¡ context å¯é¡¯ç¤ºï¼‰",
            queries_md,
            debug_txt,
            gr.update(choices=dd_choices, value=dd_choices[0]),
            "â€”",
            meta,
        )

    picked_docs, final_scores, dbg = bge_rank_and_pick(
        user_q, docs, cos_scores,
        top_k=top_k,
        candidate_cap=max(36, top_n+12),   # å¯æŒ‰ç´¢å¼•è¦æ¨¡å¾®èª¿
        clip_chars=900,                     # ä¾åŸè¨­å®š
        batch_size=32,
        type_scales=type_scales,
        recency_boost=wants_latest,
        recency_half_life=45,
    )
    _lap("rerank")

    # ç”¢ç”Ÿç­”æ¡ˆ
    answer_md, ctx_full, src_map = synthesize_answer(
        question=user_q,
        picked_docs=picked_docs,
        mode=mode or "ä¸€èˆ¬æ¨¡å¼",
        intents=intents,
        custom_prompt=custom_prompt,
    )
    _lap("answer")

    if lead_parts:
        answer_md = ("\n\n".join(lead_parts + [answer_md])).strip()
    sources_md = render_sources_md(src_map)

    # æŸ¥è©¢é¡¯ç¤º
    zh_view = "\n".join(f"- {q}" for q in (zh_qs or [])[:8]) or "ï¼ˆç„¡ï¼‰"
    en_view = "\n".join(f"- {q}" for q in (en_qs or [])[:8]) or "ï¼ˆç„¡ï¼‰"
    queries_md = f"**ä¸­æ–‡æŸ¥è©¢**\n{zh_view}\n\n**è‹±æ–‡æŸ¥è©¢**\n{en_view}"
    _lap("fmt_queries")

    debug_txt = (
        "[MODE] normal_rag (BGE reranker)\n"
        f"[INTENTS] {', '.join(intents) if intents else 'â€”'} | [COMPANIES] {', '.join(companies) if companies else 'â€”'} | wants_latest={wants_latest}\n"
        "[TYPES] " + _fmt_type_scales(type_scales) + "\n"
        f"{dbg}\n" +
        _fmt_times([
            "set_llm", "parse", "yahoo_blocks",
            # å…§éƒ¨æª¢ç´¢ç¯€é»
            "retrieval:start",
            "retrieval:mk_base_queries",
            "retrieval:merged_pins_base",
            "retrieval:before_alias_expand", "retrieval:after_alias_expand",
            "retrieval:queries_ready",
            "retrieval:zh_search_start", "retrieval:zh_search_done",
            "retrieval:en_search_start", "retrieval:en_search_done",
            "retrieval:score_norm_start", "retrieval:score_norm_done",
            "retrieval:pooling_start", "retrieval:pooling_done",
            "retrieval:metadata_fallback_start", "retrieval:metadata_fallback_done",
            "retrieval:done_ok", "retrieval:done_empty",
            # å¤–å±¤
            "retrieval", "rerank", "answer", "fmt_queries"
        ])
    )

    followups = make_followups(user_q, answer_md, n_min=3, n_max=5)
    dd_choices = [FOLLOWUP_PLACEHOLDER] + followups

    meta = {
        "type": "normal_rag",
        "question": user_q,
        "intents": intents,
        "companies": companies,
        "picked": len(picked_docs),
    }
    return (
        answer_md,
        sources_md or ctx_full or "ï¼ˆä¾†æº/Contextï¼‰",
        queries_md,
        debug_txt,
        gr.update(choices=dd_choices, value=dd_choices[0]),
        "â€”",
        meta,
    )

def on_followup_change(v, n, k, sb, sp, sr, st, sf, mode, model_name, meta, custom_prompt=""):
    DEFAULT_OPT = FOLLOWUP_PLACEHOLDER

    # å¯èƒ½æ˜¯å­—ä¸²å°±å˜—è©¦ parse
    if isinstance(meta, str):
        try:
            meta = json.loads(meta)
        except Exception:
            meta = None

    # å¦‚æœæ²’é¸æˆ–é¸äº†é è¨­ï¼Œä»€éº¼éƒ½ä¸åš
    if not v or v == DEFAULT_OPT:
        return (
            gr.update(),   # questionï¼ˆä¸è®Šï¼‰
            gr.update(),   # answer
            gr.update(),   # context
            gr.update(),   # queries_md
            gr.update(),   # debug
            gr.update(),   # followup_ddï¼ˆä¸è®Šï¼‰
            gr.update(),   # month_state
            meta           # month_meta_state ä¿æŒåŸå€¼
        )

    # ç‰¹æ®Šï¼šé»äº†ã€Œæ•´æœˆæ¦‚è¦½ã€â†’ ç›´æ¥è·‘æœˆä»½ç¸½çµ
    if v == SPECIAL_OVERVIEW_LABEL and isinstance(meta, dict) and meta.get("type") == "month_pending":
        try:
            start = dtparse(meta["start"])
            end   = dtparse(meta["end"])
        except Exception:
            # meta å£æ‰å°±åªæŠŠé¸é …å¡«å›è¼¸å…¥æ¡†
            return (
                gr.update(value=v), gr.update(), gr.update(), gr.update(), gr.update(),
                gr.update(), gr.update(), meta
            )

        # ç”Ÿæˆæ•´æœˆæ‘˜è¦ï¼ˆä½¿ç”¨é¸å®šæ¨¡å¼ï¼‰
        try:
            set_gen_llm(model_name)  # â† å¯é¸ï¼šè®“æœˆä»½æ‘˜è¦ä¹Ÿç”¨ç•¶å‰çš„ç”Ÿæˆæ¨¡å‹
        except Exception:
            pass
        month_question = f"{start.strftime('%Y-%m')} æœˆä»½ç¸½çµ"
        ans_text, ctx_full = run_month_digest(
            month_question, start, end, per_doc_k=1, max_docs=12, candidates=None, mode=mode, custom_prompt=custom_prompt
        )

        # ç”¢ç”Ÿæ–°çš„å»¶ä¼¸å•é¡Œæ¸…å–®
        new_followups = make_followups(month_question, ans_text, n_min=3, n_max=5)
        dd_choices = [FOLLOWUP_PLACEHOLDER] + new_followups

        return (
            gr.update(value=month_question),           # questionï¼šæŠŠæœˆä»½ç¸½çµé¡Œç›®å¡«å›å»
            ans_text,                                  # answer
            ctx_full,                                  # contextï¼ˆæœˆä»½æ‘˜è¦çš„ contextï¼‰
            "ï¼ˆæœˆä»½æ¨¡å¼ä¸å±•ç¤º Multi-queriesï¼‰",          # queries_md
            "[DEBUG] month_digest generated",          # debug
            gr.update(choices=dd_choices, value=dd_choices[0]),  # followup_dd
            f"ğŸ—“ï¸ å·²ç”Ÿæˆæ•´æœˆï¼š{start.strftime('%Y-%m')}",   # month_state
            None                                       # month_meta_state ç”¨ä¸åˆ°äº†ï¼Œæ¸…æ‰
        )

    # ä¸€èˆ¬æƒ…æ³ï¼šæŠŠé¸åˆ°çš„å»¶ä¼¸å•é¡Œå›å¡«åˆ°è¼¸å…¥æ¡†ï¼Œç•™çµ¦ä½¿ç”¨è€…æŒ‰ Submit
    return (
        gr.update(value=v),  # question
        gr.update(),         # answer
        gr.update(),         # context
        gr.update(),         # queries_md
        gr.update(),         # debug
        gr.update(),         # followup_dd
        gr.update(),         # month_state
        meta                 # month_meta_state ä¿ç•™
    )

# =========================
# Gradio ä»‹é¢
# =========================
with gr.Blocks(title="RAG (zh/en) + LLM rerank + Yahoo æ–°è\ç•¶æ—¥è‚¡åƒ¹ + éƒ¨è½æ ¼æœˆä»½é‡é»æ•´ç†") as demo:
    gr.Markdown("## ğŸ” RAG ï¼‹ğŸ” BGE Rerankï¼‹ğŸ“ˆ/ğŸ“° Yahooï¼‹ğŸ—“ï¸ æœˆä»½è‡ªå‹•åµæ¸¬\n- **é¡åˆ¥åå¥½**ï¼ˆBlog / PDF / Research / Transcripts_summaryï¼‰")

    # è‡ªè¨‚å›ç­”æŒ‡ä»¤ï¼ˆåœ¨ã€Œå®¢è£½åŒ–æ¨¡å¼ã€ä¸‹ç”Ÿæ•ˆï¼›å…¶ä»–æ¨¡å¼å¯ç•™ç©ºï¼‰
    custom_prompt_tb = gr.Textbox(
        label="è‡ªè¨‚å›ç­”æŒ‡ä»¤ï¼ˆå®¢è£½åŒ–æ¨¡å¼ä½¿ç”¨ï¼‰",
        placeholder="ç¯„ä¾‹ï¼šè«‹ç”¨ç¹é«”ä¸­æ–‡ã€å°ˆæ¥­ä¸”æ¸…æ¥šçš„èªæ°£å›ç­”ï¼›å…ˆåˆ—å‡º 3â€“5 é»é‡é»ï¼ˆæ¯é»å«å…·é«”æ•¸å­—/æ—¥æœŸï¼‰ï¼Œä¹‹å¾Œç”¨ä¸€æ®µè©±çµ±æ•´å½±éŸ¿ï¼Œæœ€å¾Œçµ¦å‡º 2 å€‹å¯è¡Œå»ºè­°ã€‚",
        lines=6
    )

    with gr.Row():
        question = gr.Textbox(label="è«‹è¼¸å…¥å•é¡Œ / Ask a question",
                              placeholder="ä¾‹ï¼š2024 å¹´ 11 æœˆéƒ¨è½æ ¼é‡é»ï¼Ÿ æˆ– OPEC ä»Šå¹´æ¸›ç”¢å½±éŸ¿ï¼Ÿ æˆ– å°ç©é›»è‚¡åƒ¹ï¼Ÿ",
                              lines=2, autofocus=True)

    with gr.Row():
        top_n = gr.Slider(10, 20, value=15, step=1, label="æª¢ç´¢ Top N")
        top_k = gr.Slider(1, 10, value=5, step=1, label="æœ€çµ‚ Top K")

    mode_sel = gr.Radio(
        choices=["åˆå­¸è€…æ¨¡å¼", "å°ˆå®¶æ¨¡å¼", "å®¢è£½åŒ–æ¨¡å¼"],
        value="åˆå­¸è€…æ¨¡å¼",
        label="å›ç­”æ¨¡å¼ï¼ˆå–®é¸ï¼‰"
    )

    # å›ç­”æ¨¡å‹é¸æ“‡ï¼ˆç”±ä½¿ç”¨è€…æ±ºå®šæœ¬æ¬¡å›åˆä½¿ç”¨çš„æ¨¡å‹ï¼‰
    gen_model_dd = gr.Dropdown(
        label="ç”Ÿæˆæ¨¡å‹ï¼ˆåªå½±éŸ¿æœ€çµ‚å›ç­”ï¼‰",
        choices=GEN_MODEL_WHITELIST,
        value=GEN_MODEL_DEFAULT
    )

    with gr.Row():
        blog_scale       = gr.Slider(0, 2, value=1.0, step=0.1, label="Blog é¡åˆ¥åå¥½ (0â€“2)")
        pdf_scale        = gr.Slider(0, 2, value=1.0, step=0.1, label="PDF é¡åˆ¥åå¥½ (0â€“2)")
        research_scale   = gr.Slider(0, 2, value=1.0, step=0.1, label="Research é¡åˆ¥åå¥½ (0â€“2)")
        summary_scale    = gr.Slider(0, 2, value=1.0, step=0.1, label="Transcript Summary é¡åˆ¥åå¥½ (0â€“2)")
        filing_scale     = gr.Slider(0, 2, value=1.0, step=0.1, label="è²¡å ± é¡åˆ¥åå¥½ (0â€“2)") 
    
    submit_btn = gr.Button("Submit", variant="primary")

    followup_dd = gr.Dropdown(
        label="ğŸ’¡ å»¶ä¼¸å•é¡Œï¼ˆé»é¸è‡ªå‹•å¡«å›ï¼‰",
        choices=[FOLLOWUP_PLACEHOLDER],
        value=FOLLOWUP_PLACEHOLDER,
        interactive=True,
    )

    answer = gr.Markdown(label="ğŸ§  å›ç­” / Answer")
    with gr.Row():
        context = gr.Markdown(label="ğŸ“š å¼•ç”¨ Contextï¼ˆå«é¡åˆ¥èˆ‡å€ç‡ï¼‰")
        queries_md = gr.Markdown(label="ğŸ” æŸ¥è©¢ç´€éŒ„ï¼ˆMulti-queries / æœˆä»½æ¨¡å¼å‰‡éš±è—ï¼‰")
    debug = gr.Textbox(label="ğŸ›  Debugï¼ˆåˆ†æ•¸è¡¨ / æœˆä»½ç‹€æ…‹ï¼‰", lines=12)
    month_state = gr.Markdown("â€”")

    month_meta_state = gr.State(value=None)

    def _scales_all_zero(sb, sp, sr, st, sf) -> bool:
        try:
            return all(float(x) <= 0 for x in (sb, sp, sr, st, sf))
        except Exception:
            return False

    def wrapped(q, n, k, sb, sp, sr, st, sf, mode, model_name, custom_prompt):
        if _scales_all_zero(sb, sp, sr, st, sf):
            gr.Warning("è«‹è‡³å°‘é–‹å•Ÿä¸€ç¨®ä¾†æºï¼ˆBlog / PDF / Research / Transcript / Filingï¼‰ã€‚å…¨éƒ¨ç‚º 0 ç„¡æ³•æª¢ç´¢ã€‚")
            return (gr.update(), gr.update(), gr.update(), gr.update(), gr.update(), gr.update(), None)

        if not (q or "").strip():
            gr.Info("è«‹å…ˆè¼¸å…¥å•é¡Œ")
            return gr.update(), gr.update(), gr.update(), gr.update(), gr.update(), gr.update(), None

        final_out, ctx_md, qmd, dbg, dd, month_msg, meta = handle_question(
            q, n, k, sb, sp, sr, st, sf, mode, model_name, custom_prompt
        )
        return final_out, ctx_md, qmd, dbg, dd, month_msg, meta


    submit_btn.click(
        fn=wrapped,
        inputs=[question, top_n, top_k, blog_scale, pdf_scale, research_scale, summary_scale, filing_scale, mode_sel, gen_model_dd, custom_prompt_tb],
        outputs=[answer, context, queries_md, debug, followup_dd, month_state, month_meta_state],
    )
    question.submit(
        fn=wrapped,
        inputs=[question, top_n, top_k, blog_scale, pdf_scale, research_scale, summary_scale, filing_scale, mode_sel, gen_model_dd, custom_prompt_tb],
        outputs=[answer, context, queries_md, debug, followup_dd, month_state, month_meta_state],
    )

    # äº‹ä»¶ç¶å®šéœ€åœ¨ Blocks å…§å®¹ä¸­ï¼›ç”¨ wrapper å»¶å¾Œå¼•ç”¨æœ€çµ‚å®šç¾©çš„è™•ç†å‡½å¼
    def _on_followup_wrapper(v, n, k, sb, sp, sr, st, sf, mode, model_name, meta, custom_prompt):
        return on_followup_change(v, n, k, sb, sp, sr, st, sf, mode, model_name, meta, custom_prompt)

    followup_dd.change(
        fn=_on_followup_wrapper,
        inputs=[followup_dd, top_n, top_k, blog_scale, pdf_scale, research_scale, summary_scale, filing_scale, mode_sel, gen_model_dd, month_meta_state, custom_prompt_tb],
        outputs=[question, answer, context, queries_md, debug, followup_dd, month_state, month_meta_state],
    )

if __name__ == "__main__":
    if os.getenv("PURGE_SYMBOL_CACHE", "0") == "1":
        try:
            _SYM_KV["map"].clear()
            _symcache_flush()
            print("[symbol-cache] purged")
        except Exception as e:
            print(f"[symbol-cache] purge failed: {e}")
    _load_tw_lists(force=True)
    init_internal_llms()
    set_gen_llm(GEN_MODEL_DEFAULT)
    demo.queue()
    demo.launch()
    '''
    ç”¨ä¾†æŸ¥çœ‹æ¯å€‹åˆ†é¡æœ‰å¤šå°‘å€‹ chunk
    def _count_by_bucket(vs):
        d = getattr(vs, "docstore", None)
        if not d or not hasattr(d, "_dict"): return {}
        from collections import Counter
        c = Counter()
        for x in d._dict.values():
            c[get_bucket(x.metadata or {})] += 1
        return c
    print("ZH buckets:", _count_by_bucket(vectorstore_zh))
    print("EN buckets:", _count_by_bucket(vectorstore_en))
    '''
